---
title: NLP-PreExamB
toc: true
categories:
  - [AI, NLP]
tags: [笔记, AI, NLP, 考试]
date: 2024-11-16 18:22:06
---

~~论文加考试，要死了~~

<!-- more -->

# B1 注意力机制

## Seq2seq 基础

S2S 是基于 RNN 的，将一个任意长度的序列，变成另一个任意长度的序列。

它由 编码器 和 解码器 组成。

编码器输出一个固定长度的向量，其维度是隐藏层的大小。
一般是编码器最后的状态（单向 RNN），或者是平均或最大池化（双向 RNN）。

## 瓶颈问题

这会导致问题，如果信息量太大，而隐藏层向量太小，会导致信息丢失。

为了解决此问题，我们需要注意力机制。

## RNN 网络中的注意力

简单来说

解码器使用自身当前状态，与 编码器 的各个时间步的隐藏状态 进行比较，计算出每个输入的权重，它表示了每个输入对当前输出的重要性。

随后，用权重 对编码器所有隐藏状态进行加权求和，得到一个上下文向量。

解码器结合上下文与自身的隐藏状态，生成更好的输出。

## 注意力的属性

所以，注意力有

权重分配：通过计算 Query 和 Key 的相似度，得到每个 Key 的权重。

上下文向量：根据权重，对 Value 进行加权求和。

可反向传播：注意力机制是可微的。

其中

Query：当前输入 或 当前解码器状态。

Key：信息的摘要，编码器的隐藏状态。

Value：信息的实际内容，通常与 Key 相同。

# B2 注意力作为层和 Transformer 架构

## 点积注意力

前面提到了 “比较”，而使用点积是最简单有效计算权重的方法。

简单来说，将 $Q · K_i$ ，相似性越高，点积越大。

为了避免过大的点积，我们可以将结果除以 $\sqrt{d_k}$，其中 $d_k$ 是 Key 的维度。

然后对所有 Key 的点积进行 softmax，得到一个权重向量，表示了每个 Key 对当前 Query 的重要性。

最后，将 $Value_i$ 与权重相乘，然后求和，得到上下文向量。

## 缩放的作用

我们只需要将点积除以 $\sqrt{d_k}$，就能稳定计算结果，避免点积过大过小。

当 $Q$ 和 $K$ 的维度很大时，点积的值会很大，导致 softmax 的梯度很小，使得训练困难。

## 多头注意力

简而言之，让模型在多个视角下观察输入。
比如，对于翻译任务，一个头关注主语，一个关注宾语，一个关注动词。

首先，用线性变化生成多组 $Q$、$K$ 和 $V$，一组对应一个头，每个头都有自己的权重。

并行计算权重，然后将结果拼接，再次进行线性变换，得到最终结果。

## 自注意力

而自注意力，主要是捕捉元素之间的依赖关系。

简单来说，这次的 Query、Key 和 Value 都是出自同一个输入序列。

比如 I Love AI. 我们分别查询：

I 与 I, Love, AI 的关系，得到一个权重。
Love 与 I, Love, AI 的关系，得到一个权重。
AI 与 I, Love, AI 的关系，得到一个权重。

然后对所有权重进行加权求和，得到一个上下文向量。

## 交叉注意力

也就是前面说的给 编码器 和解码器 之间建立联系的注意力。

Query 是解码器当前状态，Key 和 Value 是编码器的隐藏状态。

解码器逐步生成每个单词，每次生成时，都会用交叉注意力参考编码器的隐藏状态。

# B3 使用 RNN 和 Transformer 的上下文嵌入

## Transformer 架构

它包含：

- 注意力机制
  - 自注意力 （双向）
  - 多头注意力 （可选）
- 位置编码 （可选）
- Feedforward
- 残差归一

### 编码器

> 输入序列 -> 输入嵌入-> 位置编码 -> 编码器 [6...12] -> 输出

> 每个编码器：上一个输出 -> 多头自注意 -> 残差归一 -> 前馈 -> 残差归一 -> 输出

编码器的输入是源序列（比如需要翻译的文本）

### 解码器

> 输出序列 -> 输出嵌入 -> 位置编码 -> 解码器 [6...12] -> 输出

> 每个解码器：上一个输出 -> **掩码**多头自注意 -> 残差归一 -> 多头**交叉**注意力 -> 残差归一 -> 前馈 -> 残差归一 -> 输出

解码器的输入：

- 训练时：教师强制（比如翻译 I Love AI. 输入为 "我爱"）
- 推理时：模型已经生成的部分

## 位置编码

由于输入的每个词都会被转换为向量表示，导致模型无法区分词的位置。
位置编码通过将位置信息添加到向量中，解决这个问题。

我们使用 sin 和 cos 函数，它们的周期性与它们不同频率的组合，
使得每个位置的编码都是唯一的，并且有助于模型理解单词之间的距离关系。

## 掩码

控制模型在处理数据时的可见性。

padding 掩码：在对较短句子进行填充后，将填充的数据位置标记为 0，使模型不会关注这些 [PAD]。

Look-ahead 掩码：在训练解码器时，确保模型不会看到未来的信息，使其只基于已经生成的单词进行预测。

## 推理和训练

训练时使用教师强制，计算损失，反向传播，梯度下降

推理则前向传播，逐步生成单词，直到生成结束标记。

## ELMo

Embeddings from Language Models，第一个上下文嵌入模型。
它与 Word2Vec 不同，能够判断多义词。

- 首先它使用 CNN 将单词转为向量
- 然后使用双向 LSTM，将单词的前后文结合起来
- 将多层 LSTM 的输出加权和

## GPT 训练目标

Generative Pre-Training 仅使用解码器，目标是预测下一个单词。
它不能像 BERT 一样考前后文，它只考虑前文。

与 ELMo 类似，它提供一个预训练的“特征提取”模块

## BERT

Bidirectional Encoder Representations，用于生成上下文嵌入。
它能够考虑单词的前后文，更好的理解单词的含义。

它使用了 Masked Language Model，它随机隐藏一些单词，然后让模型猜。

还使用了 Next Sentence Prediction，它随机给模型两个句子，让模型判断这两个句子是否相邻。

# B4 对话系统

## 对话系统的类型

## 一般对话需求

## 开放对话系统

## 任务导向对话系统

## 对话状态系统

## 对话状态系统组件和使用语言模型的实现

## 简化的任务导向对话系统

## 模式引导系统

## 对话系统的评估

# B5 大型语言模型的对齐

## 对齐在 AI 中的作用

## 指令跟随模型

## 合成指令数据集

## 监督微调

## 带有人类反馈的强化学习

## 聊天模型

# B6 提示和答案工程

## 大型语言模型提示的基础

## 提示挖掘和释义

## 基于梯度的提示优化

## 提示生成模型

## 前缀调优

## 答案工程

## 提示集成

## 基于推理结构的提示

# B7 嵌入模型和向量搜索

## 向量相似性搜索在增强型语言模型中的作用

## 近似最近邻搜索

## 局部敏感哈希

## [产品]量化

## KD 树 + 优先搜索

## 图索引

## 嵌入模型

# B8 检索和工具增强的语言模型

## 增强型语言模型概述

## 检索增强生成

## 假设文档嵌入

## RAG 微调模型

## 自我独白模型

## 工具微调的可能性

# B9 高效注意力机制

## 稀疏注意力

## 因子分解

## 位置嵌入类型

## ALiBi

## RoPe

## 位置插值

## 闪存注意力

# B10 大型语言模型的蒸馏和量化

## 蒸馏设置和训练目标

## 权重量化算法

## 模型尺寸增加对量化误差的影响

# B11 参数高效的微调方法

## 高效适应的优势

## 适配器

## 瓶颈适配器

## 低秩适应

## P* 调优

## 内在维度及其与模型尺寸的关系

# B12 数据集和自举

## 语言模型训练步骤所需的数据集类型

## 不同数据源的特征 [网络，艺术，专业等]

## 使用大型语言模型进行自举训练
