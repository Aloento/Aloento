---
title: NLP-PreExamB
toc: true
categories:
  - [AI, NLP]
tags: [笔记, AI, NLP, 考试]
date: 2024-11-16 00:13:57
---

~~论文加考试，要死了~~

<!-- more -->

# B1 注意力机制

## Seq2seq 基础

## 瓶颈问题

## RNN 网络中的注意力

## 注意力的属性

# B2 注意力作为层和 Transformer 架构

## 点积注意力

## 缩放的作用

## 多头注意力

## 自注意力

## 交叉注意力

# B3 使用 RNN 和 Transformer 的上下文嵌入

## Transformer 架构

## 位置编码

## 掩码

## 推理和训练

## BERT

## GPT 训练目标

## ELMo

# B4 对话系统

## 对话系统的类型

## 一般对话需求

## 开放对话系统

## 任务导向对话系统

## 对话状态系统

## 对话状态系统组件和使用语言模型的实现

## 简化的任务导向对话系统

## 模式引导系统

## 对话系统的评估

# B5 大型语言模型的对齐

## 对齐在 AI 中的作用

## 指令跟随模型

## 合成指令数据集

## 监督微调

## 带有人类反馈的强化学习

## 聊天模型

# B6 提示和答案工程

## 大型语言模型提示的基础

## 提示挖掘和释义

## 基于梯度的提示优化

## 提示生成模型

## 前缀调优

## 答案工程

## 提示集成

## 基于推理结构的提示

# B7 嵌入模型和向量搜索

## 向量相似性搜索在增强型语言模型中的作用

## 近似最近邻搜索

## 局部敏感哈希

## [产品]量化

## KD 树 + 优先搜索

## 图索引

## 嵌入模型

# B8 检索和工具增强的语言模型

## 增强型语言模型概述

## 检索增强生成

## 假设文档嵌入

## RAG 微调模型

## 自我独白模型

## 工具微调的可能性

# B9 高效注意力机制

## 稀疏注意力

## 因子分解

## 位置嵌入类型

## ALiBi

## RoPe

## 位置插值

## 闪存注意力

# B10 大型语言模型的蒸馏和量化

## 蒸馏设置和训练目标

## 权重量化算法

## 模型尺寸增加对量化误差的影响

# B11 参数高效的微调方法

## 高效适应的优势

## 适配器

## 瓶颈适配器

## 低秩适应

## P* 调优

## 内在维度及其与模型尺寸的关系

# B12 数据集和自举

## 语言模型训练步骤所需的数据集类型

## 不同数据源的特征 [网络，艺术，专业等]

## 使用大型语言模型进行自举训练
