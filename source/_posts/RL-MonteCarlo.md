---
title: RL-MonteCarlo
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-03-19 16:25:53
---

蒙特卡罗方法

<!-- more -->

# 入门

动态规划需要完整的环境模型且计算量大，而蒙特卡洛方法不需要环境模型，且适用于大规模问题。它只适用于 Episodic 回合任务，即任务有明确的结束点，如玩一局游戏。且只在回合结束后更新价值函数。

## 采样策略

MC 方法通过 经验数据（Estimated value function）来计算价值，有两种策略：

假设在同一个 episode 中，我们经历了状态：

$$
S_1, S_2, S_2, S_1
$$

- First-visit MC：每个状态在一个回合中 首次 出现时，它的回报被记录以用于估计

  也就是只会记录 $s_1$ 的第一次访问（第一步），这种策略适用于大部分任务，它减少了计算量，但可能浪费一些有用的信息。

- Every-visit MC：每个状态在一个回合中 所有访问 该状态的回报都被记录以用于估计

  则会在第一步和第四步都记录 $s_1$ 的回报，它利用所有数据，会更快的收敛到正确的估计函数，但可能导致过度计算。

## 经验采样

之前我们提到代理会产生一种叫 Trajectory 轨迹的数据，表示为

$$
(s_t, a_t, r_t, s_{t+1}), \cdots
$$

- $s_t$：状态
- $a_t$：动作
- $r_t$：回报
- $s_{t+1}$：下一个状态

这个过程也被称之为 episode 回合，是代理在环境中的一次完整尝试。经验采样利用这些数据，不需要知道环境的规则，代理通过多次交互摸索出最佳策略，记录经验并不断优化，且一定需要足够的探索。

## Bootstrap

自举是指利用已有的估值来更新当前估计。公式

$$
V(S_t) \leftarrow E_{\pi} [R_{t+1} + \gamma V(S_{t+1})]
$$

就体现了 DP 中的自举，它利用了下一个状态的估值来更新当前状态的估值。而 MC 方法不使用自举，它通过对实际经历的样本回报进行平均来估计状态的价值。这种特性使得 MC 方法对环境的依赖程度更低，它不需要知道状态之间的转移关系和其他估值。公式

$$
V(S_{t}) \leftarrow V(S_{t}) + \alpha (G_{t} - V(S_{t}))
$$

- $\alpha$：学习率，越大对新样本越重视
- $G_t$：是从 t 开始到一个 episode 结束的回报总和
- $V(S_t)$：表示当前状态的估计值
- $G_t - V(S_t)$：表示当前估计值与实际回报的误差

通过学习率来控制调整的幅度，然后将调整值加到估计值上，得到新的估计值。

# 21 点游戏
