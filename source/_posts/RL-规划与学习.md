---
title: RL-规划与学习
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-03-26 19:42:04
---

Planing and Learning
~~都学了一半了才来理概念，怎么想的~~

<!-- more -->

# RL 概念

![RL 关键点](key.png)

## Tabular methods

表格方法指的是基于有限状态和动作集合的方法，它们通常使用表格储存 State-Action Value Function。继续细分，可以分为 Model-Free （MC，TD，Learning） 和 Model-Based （DP，Heuristic，Planning） 方法。

![算法类型](type.png)

## Reflex / Planning

反射型代理的决策取决于当前感知（Perception），而规划型代理基于行动的后果。
反射型可以有环境模型，也可以没有，而规划型代理必须有环境模型。
反射型不考虑未来长期后果，适用于即时反应系统，而规划型代理会权衡短期和长期利益。

## Model of the Environment

环境模型，是代理用来 预测环境如何响应其行动 的任何机制。可以是随机的，意味着一个动作可能导致多个不同状态。环境模型分为

- 分布模型（Distribution Model），生成所有可能的下一个状态和对应分布，适用于 DP 迭代。和
- 采样模型（Sample Model），只提供一个可能的下一个状态，按照真实概率进行采样，如 MC 方法。

## Planning and Learning

规划的特点是基于模拟经验，使用环境模型生成的虚拟数据，不需要真实环境，适用于基于模型的方法。
学习的特点是基于真实经验，代理直接与环境交付，不需要环境模型，如 Sarsa。
规划和学习以可结合，如 Dyna-Q 方法。它们都是计算密集型任务，需要在规划和更新之间找到平衡。

## Direct / Indirect RL

- 间接强化学习是基于模型的，真实经验用于改进模型环境，再用模型环境进行规划，代理就不需要与真实环境频繁交互，可以模拟更多数据。但是如果模型不准确，会导致错误的规划。
- 直接强化学习是基于真实经验的，不需要构建和维护环境模型，还没有偏差，但是数据效率低。

# Dyna-Q

它结合了直接和间接强化学习，既从真实经验中学习，又从利用环境模型进行规划。

1. 它首先采用 Q-Learning 从真实经验中学习，更新 Q 表格。
2. 然后更新环境模型，将真实经验存储到模型中。
3. 最后用模型进行规划，随机采样之前储存的经验，模拟训练 Q 表。

Dyna-Q = Q-learning + 经验回放（Planning）
此算法比 Q-Learning 更快，但是需要更多计算资源，适合与真实场景交互成本较高的情况。
我们还可以在模型中增加 时间衰减因子，让代理更倾向尝试那些长期未访问的状态，这就叫 Dyna-Q+，避免局部最优解。
