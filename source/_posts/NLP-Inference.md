---
title: NLP-Inference
toc: true
categories:
 - [AI, NLP]
tags: [笔记, AI, NLP]
date: 2024-11-08 18:19:28
---

<!-- more -->

# 介绍

随着语言模型变得越来越大，关于如何有效地使用它们、如何使它们的答案更加多样化（“创造性”），以及如何防止它们生成有害内容的问题也随之而来。我们将讨论以下主题：

- 标准LLM推理方法和参数
- “边缘”和服务器推理
- 辅助推理和推测
- 引导文本生成
- 推理时模型“适应”
- 水印

# 标准LLM推理

## 回顾：采样

- **贪婪解码**：总是选择最可能的标记 $w_t = \arg\max_{w} P(w|w_{t-c}, \ldots, w_{t-1})$
- **随机采样**：从分布 $P(w|w_{t-c}, \ldots, w_{t-1})$ 中采样
- **束搜索**：在每一步保留 $b$ 个最可能的序列
- **随机束搜索**：从 $b$ 个最可能的序列中采样

## 概率分布

Softmax 用于将模型的 logits 转换为概率分布。为了从最可能的标记中提取概率质量（从而使模型更加“创造性”和不那么重复），我们可以使用温度缩放：

$$
P(w|w_{t-c}, \ldots, w_{t-1}) = \frac{\exp(\textit{logits}(w) / T)}{\sum_{w'} \exp(\textit{logits}(w') / T)}
$$

其中 $T$ 是温度参数。

### Top-k 采样

计算整个词汇表的 softmax 是昂贵的，低评分的标记通常不有趣，因此在每一步几乎限制词汇表到前 $k$ 个标记是可行的。这称为 top-$k$ 采样。

### Top-p 采样

核采样，或 top-$p$ 采样，以不同的方式限制词汇表：它保留最可能标记的最小集合，其组合概率质量达到（并超过）阈值 $p$。

给定以下标记和概率集：

**苹果** (0.3), **香蕉** (0.2), **樱桃** (0.15), **枣** (0.1), **接骨木** (0.1), **无花果** (0.1), **葡萄** (0.05)

一个 top-$p$ 采样，$p=0.6$ 将保留 **苹果**、**香蕉** 和 **樱桃**。

## Logit 偏置

我们还可以对模型的 logits 进行偏置，以偏向某些标记。这可以用来防止模型生成有害内容，或者使其生成更符合某种风格的内容。例如，意图分类将受益于对类别标签标记添加较大的正偏置，同时抑制其他标记的概率。

更复杂的情况包括“存在”和“频率”惩罚，其中前者对当前文本中出现的标记施加固定惩罚，而后者随着出现次数的增加逐步减少偏置。

应用的公式因实现而异，但惩罚通常使用指数形式。

## Beam 大小，最佳 N

束大小 $b$ 是束搜索的一个超参数。它是每一步保留的序列数量。较大的束大小将导致更多样化的输出，但也会显著减慢推理速度。束根据其累积概率对序列进行排序，并且可以在推理结束时选择最佳的 N 个序列。

更激进的方法包括完全重启，其中推理从头开始重复 N 次，并选择最佳序列。

# 高效和边缘推理

## CPU 推理

在 CPU 上进行推理通常速度较慢且内存有限。为了克服这些限制，当前的边缘计算库将模型的权重量化为 4-bit 整数（从 32 位浮点数）。这显著提高了速度并减少了内存占用。

llama.cpp 是一个流行的在 $C^{++}$ 上运行的 LLM CPU 推理库（也有 Rust 变体）。它优化了使用基于 CPU 的高级向量操作。显著成就包括在桌面 CPU、树莓派模型、Apple Silicon、安卓手机上运行 7B GPT 模型，并且还支持桌面混合 CPU-GPU 推理。

最新版本使用 `mmap` 兼容的内存映射来按需从磁盘加载和卸载权重。

## 高效 GPU 推理

使用 GPU 进行推理时，除了内存容量外，限制因素是内存带宽，因为逐个生成标记需要大量的内存访问操作。

- 为了克服这一点，可以进行**缓存**，我们将先前计算的键和值对保存在内存中，并在下一个标记生成时重用它们

- 这样，每个批次中的查询大小通常为 $1$，由于内存限制，批次大小通常较低（低于 GPU 中的处理单元数量）--> **低 GPU 利用率**

- **Flash 解码** 将 $QK$ 乘积计算并行化到序列长度上，softmax 和输出在并行处理完成后计算

### Flash 解码 vs 仅缓存

关于两者区别的：

![缓存和迭代解码](https://crfm.stanford.edu/static/img/posts/2023-10-13-flashdecoding/parallelization.gif)

![Flash 解码](https://crfm.stanford.edu/static/img/posts/2023-10-13-flashdecoding/parallelization_kv.gif)

## Softmax 问题

计算注意力分数中的 softmax 也可能成为瓶颈。LLM 通常使用“最大值”技巧来防止指数溢出（$\exp{x_i}\rightarrow\exp{x_i - \max{x}}$），但这包括一个最大值计算，这很难并行化。 Flashdecoding++ 使用了一个经验技巧。它使用基于 activation statistics 的 fixed global constant 来防止溢出，因此 softmax 的元素可以并行计算。如果方法遇到溢出，它将使用实际最大值重新计算 softmax，但这种情况的发生概率应小于 1%。

Flashdecoding++ 还通过双缓冲升级了通用矩阵乘法（General Matrix Multiplication，GEMM），以解决低批次大小下的内存延迟问题，并根据给定的 LLM 和批次大小启发式地选择最佳实现。

![Softmax 计算类型](./figures/stable_softmax.png)

### 最大注意力值

![最大注意力值](./figures/attn_score_distrib.png)

## 处理并发请求

给定一个集中式推理服务器，我们通常期望尽可能少的延迟并行处理大量请求。高性能推理包括两个阶段：

- **预填充**：处理用户提示，计算并缓存 K 和 V。这可以在单次传递中完成，并且可能比生成的输出序列更长。这还包括生成第一个输出标记

- **解码**：这是生成下一个标记并计算下一个 K 和 V 的迭代过程。这不能并行化，但可以重用缓存中的 K 和 V。我们只需要为每次传递计算一个 Q

### Flashdecoding++

![解码器推理从预填充和解码阶段需要完全不同的计算方法](./figures/flashdecpp.png)

## 并发请求的问题

- **Monolithic KV 缓存**：长序列的 KV 缓存可能导致内存碎片化，从而减慢推理速度并导致内存使用效率低下

- **短序列**：短序列无法利用变压器的输入大小，因此传统上填充它们是一种解决方案，但在内存和计算方面是浪费的

- **不同的预填充和解码时间**：我们可以估计给定请求的预填充时间，但解码时间难以预测。这可能导致处理队列中的阻塞和气泡效应

- **次优的 GPU 利用率**：使用正确的批次与序列长度比率对于高效的 GPU 利用率至关重要。我们无法控制传入请求的长度

### 解决缓存问题

缓存分页是一种高效的方法，受虚拟内存管理的启发，解决了各种缓存问题，例如**内存碎片化**、**未知的解码长度**、**共享序列前缀**。

Pages（小的固定大小内存块）用于存储 KV 缓存，以使逻辑上连续的序列存储在非连续的物理内存中。然后利用 PagedAttention，这是一种基于页面的间接部分注意力（可以以类似于 flashdecoding++ 的方式并行化）。vLLM 是一个实现此方法的框架。

## 内存问题

![内存问题 预分配但未使用和碎片化的序列可能会占用 5-15% 的 GPU 内存](./figures/paged_attn_fragment.png)

### vLLM 虚拟化

![虚拟化缓存处理](./figures/vllm.png)

### 逻辑 vs 物理内存

![逻辑 vs 物理内存](./figures/paging.png)

## Solving cache problems

这种方法允许动态内存分配和解码长度变化的释放，以及在序列之间共享缓存，并消除具有相同提示或束搜索输入的重复项。

共享前缀在聊天模型中非常常见（通常每个用户与相同的系统提示交互）。Hydragen 提出了进一步的优化，不仅用于缓存，还用于 QK 乘积计算。通过分别计算前缀和序列其余部分的 QK 乘积（可能在单独的传递中），节省了计算，并且前缀在 GPU 工作内存中从页面中读取一次。

### Hydragen

![Hydragen 的前缀和后缀单独计算](./figures/hydragen.png)

## Piggybacking 和连续批处理

可以将小的输入序列组合在一起，形成一个较长的序列，并使用掩码等方法在多个分区上计算注意力。这样，我们可以在一次传递中计算多个解码标记。这称为 continuous batching 或 piggybacking。

混合预填充和解码批处理也是可能的，其中一部分用于计算 KV 缓存，而另一部分用于生成标记。这对于消除解码期间的气泡效应非常有用（长序列处理必须完成后才能开始下一个任务，从而导致 GPU 利用率低下）。

## Microbatching

将长序列拆分为较小的部分并并行处理，同时将尽可能多的解码任务填充到连续批处理中（解码最大化微批处理）是解决不同序列长度引起的气泡问题的好方法。然而，不同请求的解码时间以及预填充和解码处理时间的总体差异可能会导致微批处理无法解决的气泡。

DeepSpeed-FastGen 还测量了最佳 GPU 吞吐量曲线，并使用此启发式方法找到适合给定 LLM、批处理大小和 GPU 的上下文长度。这通常只有几百个标记。

### Sarathi 解决的气泡效应

![Sarathi 的高效微批处理解决的气泡效应](./figures/sarathi.png)

### GPU 利用率

![不同上下文长度的 GPU 利用率曲线，超过约 400 个标记后吞吐量没有优势](./figures/splitfuse_saturation.png)

## 混合预填充和解码

每个任务还有不同的限制特征：

- 预填充是计算受限的
- 解码是内存受限且延迟关键的

对它们进行联合优化通常会导致干扰（你不能同时优化内存访问和计算）。解决方案：将它们解耦，通过另一个抽象层将逻辑预填充和解码请求映射到不同的物理资源（GPU）。

根据当前负载和预期的解码长度，将 GPU 分配给预填充或解码任务（像这样的解决方案开发了一个长度预测模型来实现这一点）。

## 解耦预填充和解码

![通过使用单独的资源进行预填充和解码，我们可以分别优化这两个任务](./figures/tetrinfer.png)
