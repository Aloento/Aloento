---
title: RL-Endterm
toc: true
categories:
  - [AI, RL]
tags: [AI, RL, 考试]
date: 2025-05-24 16:50:53
---

Quizes 和 Oral

<!-- more -->

# Quizes

1. 强化学习智能体的目标是什么？

   - 泛化知识
   - 学习数据集的隐藏结构
   - **最大化长期奖励 ✅**
   - 从经验中学习

   强化学习的核心思想是：智能体通过与环境的交互，从经验中学习策略，以最大化未来获得的总奖励。虽然“从经验中 学习”也是过程的一部分，但“最大化长期奖励”才是最终目标。

2. 马尔可夫性质表达的是什么？

   - 转移到某个特定状态不依赖于当前状态
   - **转移到某个特定状态只依赖于当前状态 ✅**
   - 转移到某个特定状态依赖于当前和之前的状态
   - 转移到某个特定状态只依赖于之前的状态

3. 马尔可夫决策过程（MDP）的主要元素是什么？

   - 环境、智能体、价值函数、策略、状态、动作、奖励
   - 模型、智能体、状态、奖励、动作、观察、策略
   - **环境、智能体、状态、动作、奖励、模型、策略 ✅**
   - 模型、环境动态、策略

4. 如果一个任务是“关联型”的（Associative），这意味着什么？

   - 不涉及在多个情境中学习行动
   - **在多个情境中采取行动 ✅**
   - 与采取的动作无关
   - 使用对动作进行评价的训练信息

   在下棋中，面对不同的棋局状态，玩家要做出不同的下法，这就是关联型任务。不是单纯学一个固定的动作，而是“观察状态 → 做出合适的动作”。

5. $q_* (a)$ 表示什么？

   - 动作 a 的估计动作价值
   - 动作 a 的计算动作价值
   - 动作 a 的期望动作价值
   - **动作 a 的真实动作价值 ✅**

6. 大数法则说明了什么？

   - 大量试验结果的平均值应接近均值
   - 大量试验结果的总和应接近累加总和
   - 大量试验结果的平均值应接近真实值
   - **大量试验结果的平均值应接近期望值 ✅**

7. 以下哪句话是正确的？

   - 任意时刻只有一个贪婪动作
   - **任意时刻至少有一个贪婪动作** ✅
   - 任意时刻总有多个贪婪动作
   - 任意时刻有时没有贪婪动作

   贪婪动作指的是在当前已知的信息中，具有最高价值的动作。
   在一个时刻，有可能多个动作的价值一样高，但至少总能选出一个或多个是“最优”的，这些都属于贪婪动作。

8. 以下哪句话是正确的？

   - **利用时总是选择贪婪动作** ✅
   - 探索时总是选择贪婪动作
   - 利用时总是选择非贪婪动作
   - 探索时总是选择非贪婪动作

   探索不一定总是非贪婪动作，它只是有概率尝试不同的动作。

9. 在这个随机网格世界中，以下哪一个值不可能是 p(14, 0 | 10, up)

   ![stochastic](9.png)

   - **1**✅
   - 0
   - 0.8
   - 0.1

   从状态 10 出发，执行 "up" 动作，到达状态 14 并获得奖励 0 的概率是多少？这是随机网格，所以一切都有可能，所以概率不可能是 100%。

10. 策略评估指的是

    - 根据当前价值函数，使策略变成贪婪的
    - **让价值函数与当前策略保持一致** ✅

    固定策略不动，然后去计算这个策略带来的价值函数，让它们一致。使策略变贪婪是策略改进

11. 关于动态规划，哪一项是正确的？

    - **使用 bootstrap 技术，且需要环境模型**✅
    - 不使用 bootstrap，但需要环境模型
    - 不使用 bootstrap，也不需要模型
    - 使用 bootstrap，但不需要模型

12. 图中的备份图表示的是哪种方法？

    ![backup](12.jpeg)

    - DP
    - **MC**✅
    - TD
    - Sarsa

13. 与 off-policy 预测相关的术语有哪些？

    - **目标策略 与 行为策略**✅
    - 更新策略 与 行为策略
    - 目标策略 与 控制策略
    - 更新策略 与 控制策略

    用一个策略学习另一个策略的价值

14. 以下是哪个算法的值函数更新公式

    $$
    V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
    $$

    - DP
    - MC
    - **TD**✅
    - Sarsa

15. 图中的备份图表示的是哪种方法？

    ```mermaid
    flowchart TD
    n1["Filled Circle"] --> n2["Small Circle"]
    n2 --> n3["Filled Circle"]

    n1@{ shape: f-circ}
    n2@{ shape: sm-circ}
    n3@{ shape: f-circ}
    ```

    - DP
    - MC
    - TD
    - **Sarsa**✅

16. 缺失的部分是什么？

    $$
    G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \text{?}
    $$

    - $V_{t+n-1}(S_{t+n})$ ✅
    - $V_{t+n}(S_{t+n})$
    - $V_{t+n}(S_{t+n-1})$
    - $V_{t+n-1}(S_{t+n-1})$

    有时间下标的值函数，n-step TD 的回报估计公式

17. 深度优先搜索算法在如下图中，从状态 S 出发，到达目标状态 G，会给出哪个解？

    ```mermaid
    flowchart LR
        n1(["a"]) --> n2(["G"]) & n3(["b"])
        n3 --> n1 & n2
        n4(["S"]) --> n1 & n3
    ```

    - SaG
    - SbG
    - SabG
    - **No solution** ✅

    图中有死循环，S -> b -> a -> b...

18. 在使用函数逼近的情况下，权重 $w$ 的维度（即参数数量 d）与状态数量 $S$ 之间的关系是什么？

    - $S \gg d$ ✅
    - $S > d$
    - $S < d$
    - $S \ll d$

    在函数逼近中，我们的目标是用一个相对低维的参数向量 $w$ 来逼近一个高维的状态空间。

19. 当 TD(λ) 中的 λ 参数设为 1 时，等价于哪种算法？

    - TD(0)
    - TD(1)
    - **MC** ✅
    - DP

20. λ 是什么？

    - 折扣因子 → 通常是 γ
    - 步长/学习率 → 通常是 α
    - 迹衰减参数 ✅
    - 权重向量 → 通常是 w

# Oral

## 强化学习的关键概念、要素与定义

智能体与环境交互，通过试错学习最优策略，以最大化长期累计奖励的过程。

状态，动作，奖励，策略，价值函数，环境模型 是核心要素

监督学习有标签，强化学习只有奖励反馈，没有标准答案

- 基于模型：有环境模型，如 DP
- 无模型：如 MC TD
- 基于值：如 Q-Learning
- 混合型：同时学习策略和价值函数，如 Dyna

- 回合式任务：任务有明确的开始和结束
- 持续任务：无限进行

## 马尔可夫决策过程的概念、要素与定义

在某个状态选择一个动作，得到奖励，并进入下一个状态。
用来描述代理和环境之间的交互过程，如何决策并最大化长期回报。

状态，动作，奖励，转移概率，折扣因子（0 - 1） 是 MDP 的核心要素
未来只和当前状态有关，和过去无关。

## 探索-利用困境，贪婪方法

exploration 与 exploitation 无法兼得。

贪婪方法在每一步都选择当前估计价值最大的动作，可能陷入局部最优。
而 $\epsilon$-贪婪方法增加了一点随机性：

- 以 $\epsilon$ 的概率随机选择一个动作（探索，包括最大价值的动作）
- 以 $1 - \epsilon$ 的概率选择当前估计价值最大的动作（利用）

## K 臂赌博机问题、非平稳问题与增量实现

最普通的情况，有 K 台老虎机，每台中奖概率不同，目标是在有限次数内最大化总奖励，特点是只有一个状态，每次只选择一个动作，只能观察结果，其他台的状态未知。

Non-stationary 的情况是老虎机的中奖概率会变化，我们使用学习率来更新估值，让新数据更重要。

Incremental implementation 是因为我们不想储存所有历史奖励，我们使用增量公式，基于之前的估计和当前的奖励再乘上学习率来更新当前的估计值，可以应对非平稳问题。

$Q_{n + 1}(a) = Q_n(a) + \alpha_n \left[ R_{n + 1} - Q_n(a) \right]$

## 状态价值函数、状态-动作价值函数、贝尔曼方程

State Value Function：在策略 π 下，从状态 $s$ 出发，未来累计奖励的期望值

$$
v_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]
$$

> 当前在状态 $s$，按照策略 π 走下去，能拿多少总奖励？

Action-Value Function：在策略 π 下，从状态 $s$ 执行动作 $a$ 后，未来累计奖励的期望值

$$
q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]
$$

> 在状态 $s$ 做了动作 $a$，然后按策略 π 走下去，总共能拿多少奖励？

Bellman Equation 是对上面两个价值函数的递归定义：

状态价值函数的贝尔曼方程：

$$
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

状态-动作价值函数的贝尔曼方程：

$$
q_\pi(s, a) = \sum_{s',r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')]
$$

> 当前价值 = 当前奖励 + 折扣 × 下个状态的价值
> 是“现在”和“未来”的平衡表达

## 动态规划

利用递归和记忆的方式，在已知环境模型的情况下，计算最优策略和价值函数。
空间必须是有限可枚举的，通过 Bellman 方程来迭代更新价值函数。

- 策略评估：迭代计算每个状态的价值
- 策略改进：对每个状态选择价值最高的动作，得到新的策略
- 策略迭代：交替进行评估和改进
- 价值迭代：评估和改进合成一步，每次只更新一次价值就立刻选择最优动作
- 异步更新，所有状态不一起更新，而是逐个状态更新

## 蒙特卡洛方法

不需要环境模型的强化学习方法，通过采样回合数据，直接估计价值函数。

- 模型未知
- 状态空间大
- Episodic

核心思想：玩很多次，算平均数

$$
V(s) \leftarrow V(s) + \alpha (G_t - V(s))
$$

其中 $G_t$ 是从状态 $s$ 开始到回合结束的累计奖励。

1. First-Visit MC
   只更新状态首次出现时的回报，稳定，适合大多数情况

2. Every-Visit MC
   每次状态出现都更新，收敛更快，但更新次数多

为了解决永远不访问某些状态-动作对的问题，MC 采用 Stochastic Policy
如 $\epsilon$-greedy，确保每个动作都有非零概率被选中

## 时序差分方法

TD 方法结合了：

- 蒙特卡洛的采样真实经验
- 动态规划的自举更新
- 不用等到回合结束，也不用环境模型，就能边学边更新

$$
V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
$$

- $R_{t+1}$：即时奖励
- $V(S_{t+1})$：下一个状态的估值
- 整体括号：预测误差

- Sarsa：On-policy TD，用自己策略估值更新
- Q-Learning：Off-policy TD，用最大动作估值更新
- Expected Sarsa：期望下一个动作的加权平均，折中

[算法对比](https://aloen.to/AI/RL/RL-Midterm/#%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94)

## 策略改进与广义策略改进

Policy Improvement：在已知策略 π 的状态价值函数 $v_\pi(s)$ 的基础上，找到更好的策略 π′ 来获取更高回报

改进原则：在每个状态选择当前最优动作，也就是：

$$
\pi'(s) = \arg\max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

Policy Iteration = (策略评估 + 策略改进) 交替进行：

1. 策略评估：计算 $v_\pi(s)$
2. 策略改进：根据 $v_\pi(s)$ 贪婪选最优动作，得到新策略

重复进行，直到策略不再变化。

Generalized Policy Iteration：现实中不需要等到完全评估完再改进策略，可以边评估边改进，只评估几步就开始改。

## On-policy 与 Off-policy，重要性采样

| 类型       | 核心区别                     | 示例算法   |
| ---------- | ---------------------------- | ---------- |
| On-policy  | 采样 & 学习用同一个策略      | Sarsa      |
| Off-policy | 采样用一个，学习用另一个策略 | Q-Learning |

On-policy：使用当前策略 π 来采样和学习

> 用 ε-greedy 策略采样，并用它来更新价值函数

优点：实现简单
缺点：探索效率可能低，收敛较慢

Off-policy：使用行为策略 b 来采样，但学习的是目标策略 π（通常是最优的贪婪策略）

优点：数据利用率高
缺点：要解决两个策略分布不同的问题

Importance Sampling 是指 Off-policy 要从行为策略 $b$ 学习目标策略 $\pi$，需要做“校正”，用重要性采样比率来重新加权采样数据：

一步重要性采样比率：

$$
\rho = \frac{\pi(a|s)}{b(a|s)}
$$

> 原本这个动作在目标策略 π 下的概率 / 行为策略 b 下的概率

多步回合重要性采样：

$$
\rho_t = \prod_{k=t}^{T} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
$$

| 方式              | 特点                     |
| ----------------- | ------------------------ |
| Ordinary Sampling | 无偏，但方差大           |
| Weighted Sampling | 有偏，但稳定性好、收敛快 |

## SARSA、Q-Learning 及其改进方法

SARSA 是 On-policy TD 控制算法，名字来自它更新用到的五元组：

$$
(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})
$$

更新公式：

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
$$

特点：使用自己当前策略采样并学习，更新比较稳定，但探索能力较弱，可能保守，不一定找到最优路径

Q-Learning 是一种 Off-policy TD 控制算法，学习的是最优策略，即使行为策略是 ε-greedy

更新公式：

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
$$

特点：学习目标是**最优动作值函数**，收敛快，适合找最短路径。缺点：容易高估（偏差）、不够稳定

Expected SARSA 是 SARSA 与 Q-Learning 的折中版本：

更新公式：

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right]
$$

- 比 SARSA 更高效
- 比 Q-Learning 更稳定
- 用策略的动作概率做加权平均

Q-Learning 有个问题叫 Maximization Bias

Double Q-Learning 的做法：维护两个 Q 表，一个选动作，一个估值，防止互相抬价

$$
Q_1(s,a) \leftarrow Q_1(s,a) + \alpha \left[ r + \gamma Q_2(s', \arg\max_a Q_1(s',a)) - Q_1(s,a) \right]
$$

好处：更稳更准，偏差小

## 自举（Bootstrapping）

用已有估计更新当前估计，也就是说，当前状态的价值，用下一个状态的估计值来更新

TD 和 DP 都是自举，MC 不是。

## 规划与学习智能体，Dyna 架构

- 学习：与真实环境交互
- 规划：用模拟环境学习

智能体可以有两种行为：

- 反射型：只看当前状态做决定，不考虑长期后果，可以没有模型
- 规划型：根据模型预测未来，做出更聪明的决策，需要环境模型

Dyna = 规划 + 学习 的融合方法

1. 与真实环境交互
2. 更新 Q 值
3. 把数据存入模型
4. 用模型生成虚拟经验

比较纯粹的 Q-Learning 更高效。

## 强化学习中的函数逼近

当状态空间太大，不能再用表格存 Q 和 V 值时，就需要用函数来近似。

典型的方法是线性逼近，可以估计没见过的状态，参数少，可用于在线学习，但是不够精确

- Coarse Coding：用多个圆覆盖状态空间，状态属于哪些圆，就激活哪些特征
- Tile Coding：用多个矩形网格编码状态，每个 tile 是一个特征，适合连续状态空间
- 神经网络：表达能力更强，但需要更多数据和计算资源

## 资格迹与 λ-回报

TD 更新快但信息少，MC 更新慢但信息多，我们需要一个折中方法，这就是 TD(λ)。
资格迹记录谁参与过，该被奖励，是一种短期记忆机制，随着时间衰减。
谁资格迹高，谁就更新的更频繁。

## 深度强化学习——价值与策略学习，SOTA 算法

深度强化学习 = 强化学习 + 深度神经网络

用神经网络代替表格或线性函数，逼近价值函数或策略函数

适合：

- 状态空间大或连续
- 高维输入（如图像、视频）

价值学习（Value-based）典型算法：DQN（Deep Q-Network）

思路：

- 用神经网络估计 Q 值：$Q(s, a)$
- 策略通过 $\arg\max Q(s, a)$ 决定动作（贪婪）

关键技术：

1. 经验回放（Replay Buffer）：打乱相关性，提升样本利用率
2. 目标网络（Target Network）：固定一段时间不更新，防止发散

策略学习（Policy-based）典型算法：Policy Gradient、A2C、PPO 等

思路：

- 直接用神经网络输出策略 $\pi(a|s)$
- 用梯度上升优化目标函数 $J(\theta)$
- 可学出随机策略，适合连续动作空间

Actor-Critic 架构结合策略和值函数：

- Actor：负责输出策略 π（选动作）
- Critic：估计值函数（判断动作好坏）

> 互相促进，提升性能和稳定性

SOTA（先进） 算法

| 算法 | 类型         | 特点                       |
| ---- | ------------ | -------------------------- |
| DQN  | 价值型       | 离散动作，Q-learning + CNN |
| A2C  | Actor-Critic | 同步多线程训练             |
| A3C  | Actor-Critic | 异步多智能体，更快收敛     |
| PPO  | Policy-based | 策略剪切，训练更稳定       |
| TRPO | Policy-based | 保守更新，防止退步         |
| DDPG | 连续控制     | Actor-Critic + 目标网络    |
| SAC  | 高效探索     | 最大熵策略，探索更强       |

## 事后经验回放、模仿学习

事后经验回放（Experience Replay）是什么？

把过去的交互经验存下来，在训练时重复利用旧数据，提高效率

适用于：如 DQN、DDPG、SAC 等深度强化学习算法

关键作用：

1. 打破数据相关性（防止连续样本导致训练不稳定）
2. 提高数据利用率（旧经验还能用）

重要变种：Hindsight Experience Replay（HER）

- 适用于稀疏奖励任务（如机器人抓东西）
- 把失败经验“重新解释成成功”，提高学习效率

思路：把没完成目标的经验，换个目标当作“完成了另一个任务”

模仿学习（Imitation Learning）是什么？

不是自己试错，而是模仿专家的行为来学习策略

适用于：专家容易演示，但奖励难以设计的任务
如自动驾驶、机械臂操作等

行为克隆（Behaviour Cloning）

- 把专家演示当成监督学习数据
- 输入状态 → 输出动作
- 学习一个策略网络模仿专家
- 简单高效
- 会积累错误，偏离专家轨迹
- 模仿学习 ≠ 监督学习，状态分布是非独立同分布（non-iid）的
- 要想提升鲁棒性，可结合数据增强、多任务训练
