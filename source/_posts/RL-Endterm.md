---
title: RL-Endterm
toc: true
categories:
  - [AI, RL]
tags: [AI, RL, 考试]
date: 2025-05-24 16:50:53
---

Quizes 和 Oral

<!-- more -->

# Quizes

1. 强化学习智能体的目标是什么？

   - 泛化知识
   - 学习数据集的隐藏结构
   - **最大化长期奖励 ✅**
   - 从经验中学习

   强化学习的核心思想是：智能体通过与环境的交互，从经验中学习策略，以最大化未来获得的总奖励。虽然“从经验中 学习”也是过程的一部分，但“最大化长期奖励”才是最终目标。

2. 马尔可夫性质表达的是什么？

   - 转移到某个特定状态不依赖于当前状态
   - **转移到某个特定状态只依赖于当前状态 ✅**
   - 转移到某个特定状态依赖于当前和之前的状态
   - 转移到某个特定状态只依赖于之前的状态

3. 马尔可夫决策过程（MDP）的主要元素是什么？

   - 环境、智能体、价值函数、策略、状态、动作、奖励
   - 模型、智能体、状态、奖励、动作、观察、策略
   - **环境、智能体、状态、动作、奖励、模型、策略 ✅**
   - 模型、环境动态、策略

4. 如果一个任务是“关联型”的（Associative），这意味着什么？

   - 不涉及在多个情境中学习行动
   - **在多个情境中采取行动 ✅**
   - 与采取的动作无关
   - 使用对动作进行评价的训练信息

   在下棋中，面对不同的棋局状态，玩家要做出不同的下法，这就是关联型任务。不是单纯学一个固定的动作，而是“观察状态 → 做出合适的动作”。

5. $q_* (a)$ 表示什么？

   - 动作 a 的估计动作价值
   - 动作 a 的计算动作价值
   - 动作 a 的期望动作价值
   - **动作 a 的真实动作价值 ✅**

6. 大数法则说明了什么？

   - 大量试验结果的平均值应接近均值
   - 大量试验结果的总和应接近累加总和
   - 大量试验结果的平均值应接近真实值
   - **大量试验结果的平均值应接近期望值 ✅**

7. 以下哪句话是正确的？

   - 任意时刻只有一个贪婪动作
   - **任意时刻至少有一个贪婪动作** ✅
   - 任意时刻总有多个贪婪动作
   - 任意时刻有时没有贪婪动作

   贪婪动作指的是在当前已知的信息中，具有最高价值的动作。
   在一个时刻，有可能多个动作的价值一样高，但至少总能选出一个或多个是“最优”的，这些都属于贪婪动作。

8. 以下哪句话是正确的？

   - **利用时总是选择贪婪动作** ✅
   - 探索时总是选择贪婪动作
   - 利用时总是选择非贪婪动作
   - 探索时总是选择非贪婪动作

   探索不一定总是非贪婪动作，它只是有概率尝试不同的动作。

9. 在这个随机网格世界中，以下哪一个值不可能是 p(14, 0 | 10, up)

   ![stochastic](9.png)

   - **1**✅
   - 0
   - 0.8
   - 0.1

   从状态 10 出发，执行 "up" 动作，到达状态 14 并获得奖励 0 的概率是多少？这是随机网格，所以一切都有可能，所以概率不可能是 100%。

10. 策略评估指的是

    - 根据当前价值函数，使策略变成贪婪的
    - **让价值函数与当前策略保持一致** ✅

    固定策略不动，然后去计算这个策略带来的价值函数，让它们一致。使策略变贪婪是策略改进

11. 关于动态规划，哪一项是正确的？

    - **使用 bootstrap 技术，且需要环境模型**✅
    - 不使用 bootstrap，但需要环境模型
    - 不使用 bootstrap，也不需要模型
    - 使用 bootstrap，但不需要模型

12. 图中的备份图表示的是哪种方法？

    ![backup](12.jpeg)

    - DP
    - **MC**✅
    - TD
    - Sarsa

13. 与 off-policy 预测相关的术语有哪些？

    - **目标策略 与 行为策略**✅
    - 更新策略 与 行为策略
    - 目标策略 与 控制策略
    - 更新策略 与 控制策略

    用一个策略学习另一个策略的价值

14. 以下是哪个算法的值函数更新公式

    $$
    V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
    $$

    - DP
    - MC
    - **TD**✅
    - Sarsa

15. 图中的备份图表示的是哪种方法？

    ```mermaid
    flowchart TD
    n1["Filled Circle"] --> n2["Small Circle"]
    n2 --> n3["Filled Circle"]

    n1@{ shape: f-circ}
    n2@{ shape: sm-circ}
    n3@{ shape: f-circ}
    ```

    - DP
    - MC
    - TD
    - **Sarsa**✅

16. 缺失的部分是什么？

    $$
    G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \text{?}
    $$

    - $V_{t+n-1}(S_{t+n})$ ✅
    - $V_{t+n}(S_{t+n})$
    - $V_{t+n}(S_{t+n-1})$
    - $V_{t+n-1}(S_{t+n-1})$

    有时间下标的值函数，n-step TD 的回报估计公式

17. 深度优先搜索算法在如下图中，从状态 S 出发，到达目标状态 G，会给出哪个解？

    ```mermaid
    flowchart LR
        n1(["a"]) --> n2(["G"]) & n3(["b"])
        n3 --> n1 & n2
        n4(["S"]) --> n1 & n3
    ```

    - SaG
    - SbG
    - SabG
    - **No solution** ✅

    图中有死循环，S -> b -> a -> b...

18. 在使用函数逼近的情况下，权重 $w$ 的维度（即参数数量 d）与状态数量 $S$ 之间的关系是什么？

    - $S \gg d$ ✅
    - $S > d$
    - $S < d$
    - $S \ll d$

    在函数逼近中，我们的目标是用一个相对低维的参数向量 $w$ 来逼近一个高维的状态空间。

19. 当 TD(λ) 中的 λ 参数设为 1 时，等价于哪种算法？

    - TD(0)
    - TD(1)
    - **MC** ✅
    - DP

20. λ 是什么？

    - 折扣因子 → 通常是 γ
    - 步长/学习率 → 通常是 α
    - 迹衰减参数 ✅
    - 权重向量 → 通常是 w

# Oral

## 强化学习的关键概念、要素与定义

## 马尔可夫决策过程的概念、要素与定义

## 探索-利用困境，贪婪方法

## K 臂赌博机问题、非平稳问题与增量实现

## 状态价值函数、状态-动作价值函数、贝尔曼方程

## 动态规划

## 蒙特卡洛方法

## 时序差分方法

## 策略改进与广义策略改进

## On-policy 与 Off-policy，重要性采样

## SARSA、Q-Learning 及其改进方法

## 自举（Bootstrapping）

## 规划与学习智能体，Dyna 架构

## 强化学习中的函数逼近

## 资格迹与 λ-回报

## 深度强化学习——价值与策略学习，SOTA 算法

## 事后经验回放、模仿学习
