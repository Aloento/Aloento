---
title: RL-马尔可夫决策过程
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-03-05 17:05:17
---

Markov Decision Processes
~~流感让我断更一周~~

<!-- more -->

# 基础概念

马尔可夫决策过程有点像自动机。简而言之，它：在某个状态下，选择一个动作，获得某种奖励，进入下一个状态。

- 状态（State）：$S$，环境状态，如棋盘上的位置，角色血量
- 动作（Action）：$A$，代理可选的动作，如走一步，攻击
- 奖励（Reward）：$R$，反馈，如吃到食物，受到伤害
- 转移概率（Transition Probability）：$P$，状态转移概率，如可能的下一步

决策是一个不断重复的过程，每次做出选择都会影响未来的状态和奖励。其本质是代理（Agent）与环境（Environment）之间的交互：

1. Agent 在状态 $S_t$ 选择动作 $A_t$
2. Environment 返回奖励 $R_{t+1}$ 和新状态 $S_{t+1}$
3. Agent 根据奖励和新状态更新策略
4. 重复

整个过程形成一个轨迹（Trajectory）$S_0, A_0, R_1, S_1, A_1, R_2, S_2, \cdots$，我们可以发现它的核心是“状态如何变化”，我们用状态转移概率（Transition Probability）来描述这个过程：

$$
P(s', r | s, a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
$$

其中：

- $s, s'$ 是状态
- $a$ 是动作
- $r$ 是奖励

意思是在状态 $s$ 下，选择动作 $a$，进入状态 $s'$，获得奖励 $r$ 的概率

且 $\sum_{s'} \sum_r P(s', r | s, a) = 1$ 也就是说一定会转移到某个状态并获得某个奖励，不会出现转移到虚空，或者没有奖励的情况，且所有可能的状态和奖励的概率和为 1。

如果状态，动作，奖励是有限个数的，就叫 Finite MDP，我们能够精确计算出最优策略。
