---
title: RL-马尔可夫决策过程
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-03-05 17:05:17
---

Markov Decision Processes
~~流感让我断更一周~~

<!-- more -->

# 基础概念

马尔可夫决策过程有点像自动机。简而言之，它：在某个状态下，选择一个动作，获得某种奖励，进入下一个状态。

- 状态（State）：$S$，环境状态，如棋盘上的位置，角色血量
- 动作（Action）：$A$，代理可选的动作，如走一步，攻击
- 奖励（Reward）：$R$，反馈，如吃到食物，受到伤害
- 转移概率（Transition Probability）：$P$，状态转移概率，如可能的下一步

决策是一个不断重复的过程，每次做出选择都会影响未来的状态和奖励。其本质是代理（Agent）与环境（Environment）之间的交互：

1. Agent 在状态 $S_t$ 选择动作 $A_t$
2. Environment 返回奖励 $R_{t+1}$ 和新状态 $S_{t+1}$
3. Agent 根据奖励和新状态更新策略
4. 重复

整个过程形成一个轨迹（Trajectory）$S_0, A_0, R_1, S_1, A_1, R_2, S_2, \cdots$，我们可以发现它的核心是“状态如何变化”，我们用

# 转移概率

（Transition Probability）来描述这个过程：

$$
p(s', r | s, a) \doteq Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
$$

其中：

- $s, s'$ 是状态
- $a$ 是动作
- $r$ 是奖励

意思是在状态 $s$ 下，选择动作 $a$，进入状态 $s'$，获得奖励 $r$ 的概率

且 $\sum_{s'} \sum_r P(s', r | s, a) = 1$ 也就是说一定会转移到某个状态并获得某个奖励，不会出现转移到虚空，或者没有奖励的情况，且所有可能的状态和奖励的概率和为 1。

如果状态，动作，奖励是有限个数的，就叫 Finite MDP，我们能够精确计算出最优策略。

此公式还有其他形式：

## 状态转移概率

$$
p(s' | s, a) \doteq Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_r p(s', r | s, a)
$$

它表示在状态 $s$ 下，选择动作 $a$，进入状态 $s'$ 的概率。由于状态转移可能伴随不同奖励，所以需要对所有可能的奖励求和。

## 期望即时奖励

$$
r(s, a) \doteq E[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_r r \sum_{s'} p(s', r | s, a)
$$

它表示在状态 $s$ 下，选择动作 $a$，期望获得的奖励 $R_t$。每种情况都有不同的概率和奖励，所以需要对所有可能的状态和奖励求和，以计算平均期望奖励。

## 期望奖励

$$
r(s, a, s') \doteq E[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_r r \frac{p(s', r | s, a)}{p(s' | s, a)}
$$

这里是在已经知道从 $s$ 执行 $a$ 后，转移到 $s'$ 的情况下，计算期望奖励。通过对所有可能的奖励加权求和，归一化到特定的状态转移概率 $s \to s'$。

## 确定与随机

如果一个系统是 Deterministic 的，那么在给定 $s$ 和 $a$ 后，$s'$ 是确定的，即 $p(s' | s, a) = 1$，每次执行相同的动作，结果都会一样，如棋类游戏。

如果是 Stochastic 的，那么 $p(s' | s, a)$ 是一个概率分布，且 $\sum_{s'} p(s' | s, a) = 1$，表示执行后可能转移到不同状态，如掷骰子，环境有随机变化等。

任何代理无法改变的事物都是环境的一部分，边界代表着代理能够影响的事物的范围极限。

## 例子

假设有环境，你当前处于 C：

| x    | 10, A | x    |
| ---- | ----- | ---- |
| 0, B | 0, C  | 0, D |
| x    | 0, E  | x    |

- State Space: $S = \{A, B, C, D, E\}$
- Action Space: $A = \{Up, Down, Left, Right\}$

### 确定性转移

代理在 C 选择 Up，进入 A 的概率是 1，奖励是 10，公式表示为

$$
p(A | C, Up) = 1
$$

奖励表述为

$$
r(C, Up) = 10 \times p(A, 10 | C, Up) = 10
$$

### 随机转移

设代理有 0.8 的概率向上移动成功，0.1 的概率被环境带到其他位置。

$$
p(A | C, Up) = 0.8, p(B | C, Up) = 0.1, p(C | C, Up) = 0.1
$$

奖励计算为：

$$
r(C, Up) = 10 \times p(A, 10 | C, Up) + 0 \times p(B, 0 | C, Up) + 0 \times p(C, 0 | C, Up) = 8
$$

# Exercise: Transition Graph
