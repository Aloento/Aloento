---
title: RL-多臂老虎机
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-02-23 23:09:25
---

~~强化学习~~
~~强化的不是机器模型~~
~~而是我的猪脑~~

<!-- more -->

# 老虎机

## 单臂

现在在你面前有一台老虎机，它有固定的中奖率，当你拉动它，你就会从固定的概率分布中得到奖励。而这个情况不能视作一个强化学习问题，因为它只有一个状态和一个动作，你不能通过任何方法来优化你的策略。

## 多臂

既然如此，我们就拿来多台老虎机。现在在你面前有数台单臂老虎机，它们各自有不同的中奖分布。现在给定有限的拉动次数 T，你如何拉动这些老虎机来最大化你的收益呢？

这个问题是一个 非联想的，评估性的反馈问题，及每次反馈只评估当前动作，也就是独立事件。

## 定义

- 非联想的：Non-Assocative，即每次动作的结果不会影响到接下来你对其他老虎机的预期收益。你只是单纯的记录下每个老虎机的平均奖励，然后选择最大的那个。

- 联想的：Associative，当前动作的结果会影响到未来动作的预期收益。你从这次拉动中获得的信息，会影响到你对其他老虎机的选择。在类似于上下文老虎机中，你的动作会影响其他老虎机的中奖概率。

- 评估性的：Evaluative，你得到的奖励，只是你当前动作的结果，你只能知道你刚刚得到了多少收益，而不能知道其他老虎机的中奖概率。

- 指导性的：Instructive，在这种情况下，你不光能得到当前的结果，还能得到其他可能的结果信息。比如你拉动了一个老虎机，你不仅得到奖励，还能得到其他老虎机的奖励信息，即使你没有选择它们。

# Formalization

需要数学的方式表达此问题才能应用到强化学习中。所以我们定义：

- t 时刻
- $A_t$ 在 t 时刻选择的动作
- $R_t$ 在 t 时刻得到的奖励
- $q_*(a)$ 动作 a 的真实值，是一个固定值，但通常未知
- $Q_t(a)$ 在 t 时刻对动作 a 的估计值，根据过去的经验得到

## 例子

假设你面前有三个老虎机 1, 2, 3，你有 4 个可用次数：

| 时刻 t | 选择 $A_t$ | 奖励 $R_t$ | 期望 $Q_t(1)$ | $Q_t(2)$ | $Q_t(3)$ |
| ------ | ---------- | ---------- | ------------- | -------- | -------- |
| 1      | 1          | 5          | 5             | 0        | 0        |
| 2      | 2          | 3          | 5             | 3        | 0        |
| 3      | 3          | 4          | 5             | 3        | 4        |
| 4      | **1**      | 4          | 4.5           | 3        | 4        |

每次选择一个动作并获得奖励后，需要更新对该动作的估计值。这里我们使用简单的平均值来估计：

<div>
$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
$$
</div>

$\mathbb{1}_{A_i = a}$ 是一个指示函数，当 $A_i = a$ 时为 1，否则为 0。确保了只有选择了动作 a 的时候才会更新估计值。

这就是 Action-value Methods，即通过动作的估计值来选择动作。

## 大数定律

简而言之，如果一个事情你做的次数足够多，那么它的平均结果（预估收益）就会接近于它的真实情况。比如抛硬币，正反面比例会越来越接近 0.5。

## 探索和利用

让我们来理解一下 探索 (Exploration) 和 利用 (Exploitation) 的概念。

- 在任何时间步，至少有一个动作的估计值是最大的。也就是说如果我们选择了一个最大的动作，这就是利用，或者贪婪动作。

- 如果我们选择非贪婪动作，那么就是探索，去试试看能否找到更好的动作。

像之前表中我们在 t=4 的时候选择了 1，这就是贪婪动作。但是我们会发现一个问题：

- 为了获得最大奖励，我们偏好选择已知的最大动作
- 但是如果我们不去探索其他动作，我们就无法知道其他动作的真实值
- 所以我们需要在探索和利用之间找到一个平衡

## $\epsilon$-greedy

它是平衡探索和利用的一个简单有效的方法。

- 以 $\epsilon$ 的概率进行探索，而不考虑价值估计
- 以 $1-\epsilon$ 的概率进行利用（贪心动作），选择已知的最大价值动作

### 练习：计算概率

当 $\epsilon = 0.5$ 时，计算有两个动作，且选择贪心动作的概率。

> two actions 不是说你选两次，而是说你在每次选择动作的时候，一共有两个可能，如选 A 或 B。

1. 情况1：确定选择 ($1-\epsilon$)：= 0.5 的概率直接选择贪心动作

2. 情况2：随机选择所有动作，也包括选中贪心动作的概率，此时：
    - 选择贪心动作的概率 = 非贪心动作的概率 = 0.5
    - 同时发生 情况2 & 情况2选中贪心动作 的概率 = 0.5 * 0.5 = 0.25

所以选择贪心动作的概率 = 情况1 + 情况2 = 0.5 + 0.25 = 0.75

```txt
           选择方式
          /      \
   情况1 (0.5)   情况2 (0.5)
      /             /   \
选贪心动作      选A(0.5) 选B(0.5)
```
