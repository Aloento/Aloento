---
title: RL-多臂老虎机
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-02-23 23:09:25
---

~~强化学习~~
~~强化的不是机器模型~~
~~而是我的猪脑~~

<!-- more -->

# 老虎机

## 单臂

现在在你面前有一台老虎机，它有固定的中奖率，当你拉动它，你就会从固定的概率分布中得到奖励。而这个情况不能视作一个强化学习问题，因为它只有一个状态和一个动作，你不能通过任何方法来优化你的策略。

## 多臂

既然如此，我们就拿来多台老虎机。现在在你面前有数台单臂老虎机，它们各自有不同的中奖分布。现在给定有限的拉动次数 T，你如何拉动这些老虎机来最大化你的收益呢？

这个问题是一个 非联想的，评估性的反馈问题，及每次反馈只评估当前动作，也就是独立事件。

## 定义

- 非联想的：Non-Assocative，即每次动作的结果不会影响到接下来你对其他老虎机的预期收益。你只是单纯的记录下每个老虎机的平均奖励，然后选择最大的那个。

- 联想的：Associative，当前动作的结果会影响到未来动作的预期收益。你从这次拉动中获得的信息，会影响到你对其他老虎机的选择。在类似于上下文老虎机中，你的动作会影响其他老虎机的中奖概率。

- 评估性的：Evaluative，你得到的奖励，只是你当前动作的结果，你只能知道你刚刚得到了多少收益，而不能知道其他老虎机的中奖概率。

- 指导性的：Instructive，在这种情况下，你不光能得到当前的结果，还能得到其他可能的结果信息。比如你拉动了一个老虎机，你不仅得到奖励，还能得到其他老虎机的奖励信息，即使你没有选择它们。

## Formalization

需要数学的方式表达此问题才能应用到强化学习中。所以我们定义：

- t 时刻
- $A_t$ 在 t 时刻选择的动作
- $R_t$ 在 t 时刻得到的奖励
- $q_*(a)$ 动作 a 的真实值，是一个固定值，但通常未知
- $Q_t(a)$ 在 t 时刻对动作 a 的估计值，根据过去的经验得到

### 例子

假设你面前有三个老虎机 1, 2, 3，你有 4 个可用次数：

| 时刻 t | 选择 $A_t$ | 奖励 $R_t$ | 期望 $Q_t(1) | $Q_t(2) | $Q_t(3) |
| ------ | ---------- | ---------- | ------------ | ------- | ------- |
| 1      | 1          | 5          | 5            | 0       | 0       |
| 2      | 2          | 3          | 5            | 3       | 0       |
| 3      | 3          | 4          | 5            | 3       | 4       |
| 4      | 1          | 4          | 4.5          | 3       | 4       |

每次选择一个动作并获得奖励后，需要更新对该动作的估计值。这里我们使用简单的平均值来估计：

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
$$

$\mathbb{1}_{A_i = a}$ 是一个指示函数，当 $A_i = a$ 时为 1，否则为 0。确保了只有选择了动作 a 的时候才会更新估计值。
