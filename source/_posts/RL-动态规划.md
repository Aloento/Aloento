---
title: RL-动态规划
toc: true
categories:
  - [AI, RL]
tags: [笔记, AI, RL]
date: 2025-03-12 00:18:12
---

Dynamic Programming
~~学不动了，真的~~

<!-- more -->

# 动态规划基础

我们都知道斐波那契数列计算

```ts
function fib(n: number): number {
  if (n === 0) return 0;
  if (n === 1) return 1;
  return fib(n - 1) + fib(n - 2);
}
```

这种朴素的递归算法，时间复杂度是指数级别的，$O(2^\frac{n}{2})$，我们可以用动态规划来优化。

```ts
const memo = [];

function fib(n: number): number {
  if (n === 0) return 0;
  if (n === 1) return 1;

  if (memo[n] !== undefined) return memo[n];

  memo[n] = fib(n - 1) + fib(n - 2);
  return memo[n];
}
```

它简单的将之前的计算结果保存下来，避免了重复计算，时间复杂度降到了$O(n)$。

这就引出了动态规划的两个重要概念：

DP = Recursion + Memoization

在强化学习中，动态规划是一组用来计算最优策略的算法集合。使用动态规划时需要给定环境的完美模型，我们使用 MDP 来描述此环境模型。就算使用动态规划，也需要消耗大量计算资源。

# Policy Evaluation

前面我们讲到 State-value 函数用于评估某个状态的价值

$$
v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
$$

$$
= \sum_{a} \pi(a|s) \sum_{s', r} p[s', r|s, a](r + \gamma v_{\pi}(s'))
$$

由于此方程一般没有解析解，通常采用迭代的方式来求解，这就为我们后续进行策略改进提供了基础。

## Grid World

我们在之前的 例：值迭代 中已经见过，这次让我们来看一道新的类型题目。

有一个已经计算好的策略如下：

| x   | 1   | 2   | 3   |
| --- | --- | --- | --- |
| 4   | 5   | 6   | 7   |
| 8   | 9   | 10  | 11  |
| 12  | 13  | 14  | x   |

| 0.0 | -14 | -20 | -22 |
| --- | --- | --- | --- |
| -14 | -18 | -20 | -20 |
| -20 | -20 | -18 | -14 |
| -22 | -20 | -14 | 0.0 |

奖励函数：-1，表示每一步都会受到 -1 的惩罚。

现在，让我们添加一个新的状态 15：

| x   | 1   | 2   | 3   |
| --- | --- | --- | --- |
| 4   | 5   | 6   | 7   |
| 8   | 9   | 10  | 11  |
| 12  | 13  | 14  | x   |
| -   | 15  | -   | -   |

并且规定在 15 处执行：

- 左：15 -> 12
- 右：15 -> 14
- 上：15 -> 13
- 下：15 -> 15

问 1，设原状态转移的情况保持不变，在等概率随机策略的情况下，状态 15 的价值是多少？

问 2，现在假设从 13 执行向下会到 15，状态 15 的价值是多少？

### 问 1

要注意，原状态转移情况不变的意思是，在 13 处向下不会进入到新状态 15。

我们知道 $\pi(a|s) = 0.25$，$p(s' | s, a) = 1$，$r = -1$，$\gamma = 1$。

所以：

$v_{\pi}(15) = \sum_{4} 0.25 \sum_{s', r} 1 \times [-1 + 1 \times v_{\pi}(s')] =$  
$0.25 \times [-1 + -22] +$  
$0.25 \times [-1 + -20] +$  
$0.25 \times [-1 + -14] +$  
$0.25 \times [-1 + v_{\pi}(15)] =$  
$-15 + 0.25 \times v_{\pi}(15)$

将 $v_{\pi}(15)$ 移到左边，得到：

$0.75 \times v_{\pi}(15) = -15$

$v_{\pi}(15) = -20$

### 问 2

由于 15 的移动特性，导致它实际上是 13 的别名，所以不会导致 13 或 15 的价值发生变化。

让我们来计算验证一下：

$v_{\pi}(15) =$  
$0.25 \times [-1 + -22] +$  
$0.25 \times [-1 + v_{\pi}(13)] +$  
$0.25 \times [-1 + -14] +$  
$0.25 \times [-1 + v_{\pi}(15)] =$  
$-10 + 0.25 \times v_{\pi}(13) + 0.25 \times v_{\pi}(15)$

$v_{\pi}(13) =$  
$0.25 \times [-1 + -22] +$  
$0.25 \times [-1 + -20] +$  
$0.25 \times [-1 + -14] +$  
$0.25 \times [-1 + v_{\pi}(15)] =$  
$-15 + 0.25 \times v_{\pi}(15)$

联立求解：

$0.75v_{\pi}(15) = -10 + 0.25 \times (-15 + 0.25v_{\pi}(15))$

$v_{\pi}(15) = -20$

$v_{\pi}(13) = -15 + 0.25 \times (-20) = -15 - 5 = -20$

因此，状态13和状态15的价值都是-20。

# Policy Improvement
