---
title: NLP-Introduction
toc: true
categories:
  - [AI, NLP]
tags: [笔记, AI, NLP]
date: 2024-09-18 15:53:24
---

本章主要讲述了 NLP 的基本概念，和关键主题

接下来我将把 PPT 和整理过后的笔记穿插在一起成文

<!-- more -->

# 什么是自然语言处理？

**自然语言处理（NLP）** 是一个跨学科领域，旨在使自然语言对计算机可访问。

- **自然语言** 在此上下文中指的是人类用于交流的普通语言，如英语、中文、西班牙语等。
- 能够**访问**自然语言涵盖了广泛的能力，重要领域包括：

> 自然语言是普通人类使用的语言。C++ 就不是自然语言。  
> 海军陆战队使用的语言来传递操作代码，这是一种非自然语言。  
> 微分方程或其符号表示也可以被视为非自然语言。

- **交流**：接受输入和生成自然语言输出的能力；
- **理解**：能够访问和利用信息和情感内容；
- **语言协助**：帮助人类在语言表达上的能力。

> “理解”和“语义内容”这些概念仍然有些模糊，难以明确定义。  
> 我们已经在使用一些语言辅助工具，如 Grammarly 和 Google Translate，这些工具是 NLP 的实际应用。

# 相关领域

## 计算语言学

使用计算方法对语言进行科学研究。

- 也许是最接近 NLP 的领域，但重点不同：NLP 并不关心自然语言的理论见解，而只关注对计算语言处理有用的方法的设计和分析。

- 它通常不是直接实现理论思想，而是为 NLP 系统提供**架构灵感**。

> NLP 是一种应用导向的构建，旨在实现实际功能，而计算语言学则是使用计算方法研究语言的理论学科。  
> NLP 从计算语言学的理论中获得灵感，但通常不会直接实现这些理论。一个著名的例子是 Transformer 架构

## 人工智能 (AI)

显然，NLP 目标与 AI 构建智能系统的目标有很大的重叠：

- 语言使用与成为智能所需的概念、表示和推理能力密切相关，
- 实际上，如果没有从自然语言输入中提取信息的能力，大规模知识获取也是不可能的。

上述特征使得 AI 子领域 **知识表示** 和 **推理** 对 NLP 尤为重要。

> AI 是一个更广泛的概念，而 GPT、NLP 和 ML 都是 AI 的子领域或应用。  
> 人类与其他动物的主要区别在于人类使用抽象语言的能力。这种能力被认为是人类智能的一个关键特征。

## 机器学习

现代 NLP 在很大程度上依赖于机器学习技术，事实上，近年来通用 ML 方法的语言学应用主导了该领域。

- 主要使用监督或半监督方法，但强化学习的使用也在增加。
- 文本是离散符号的序列，因此需要能够处理这种类型输入（以及生成时的输出）的 ML 模型。

> 强化学习在 NLP 中的应用变得非常重要  
> 文本基本上是离散符号的序列  
> 即使是非深度学习的统计模型也在 NLP 中广泛使用。
> 虽然可以将其归结为普通的统计模型，但这些模型在机器学习中仍然很重要。  
> 逻辑回归不是深度学习方法。它不使用神经网络，而是依赖统计建模和优化。  
> 提到一个名为 Aliza 的聊天机器人，它被认为是第一个通过图灵测试的聊天机器人。
> Aliza 是一个基于规则的系统，而不是基于机器学习的系统。  
> 使用机器学习是因为手动编写成千上万的规则既繁琐又不够灵活。

## 语音处理

语音信号的处理和生成传统上不被认为是 NLP 的一部分，NLP 主要关注文本，但显然与其密切相关：

- 语音转文本为 NLP 应用提供输入，
- NLP 应用为语音合成提供输入；
- 处理和合成语音都需要语言学知识，这对于 NLP 也很重要：尤其是语言建模在这两个领域中都起着核心作用。

# 应用

## 应用示例

- 机器翻译，
- 文档检索：检索与用户查询匹配的自由文本文档，
- 问答系统，例如，智能手机助手回答问题的能力，
- 文本分类，例如检测电子邮件垃圾邮件，
- 聊天机器人，例如，用于购买火车票的聊天机器人，
- 拼写检查和语法检查，
- 自由文本输入的自动补全，
- 文档摘要，
- 从结构化数据生成文本（从股票交易新闻到错误消息）。

# 中心主题

## 管道与端到端架构

一种有影响力的 NLP 观点认为其核心任务是提供一个**模块管道**，该管道依次生成通用的语言分析，每个模块基于前一个模块的输出构建：

![来自 [spaCy NLP 库文档](https://spacy.io/usage/processing-pipelines) 的图示。](https://spacy.io/images/pipeline.svg)

然后，专门的 NLP 应用程序作为这个通用管道元素之上的相对简单的附加组件构建。

相反的观点集中在构建 NLP 应用程序作为**端到端**机器学习模型，这些模型学习将原始输入转换为所需的输出，而无需专门的语言分析模块。

最先进的 NLP 应用程序通常介于这两个极端之间：它们使用一些通用的分析模块，例如用于分词或词干提取，并且还依赖于跳过某些传统管道步骤的机器学习模型来生成所需的输出。

> 流水线（或称为过滤器和管道）通常由不同的处理单元组成。  
> 每个处理单元的输入是前一个单元的输出，这种因果关系在流水线中保持不变。  
> 但是，NLP 中的流水线可能会出现循环（loops），这会导致一些架构问题。  
> 由于流水线方法的局限性（如递归处理问题），人们开始考虑端到端（end-to-end）架构。  
> 端到端架构直接从原始输入生成所需输出，中间没有任何分析器。  
> 有时，端到端模型中也会包含一些预处理或后处理的管道元素。  
> 端到端模型在某些情况下可能不够通用，无法适应所有任务。

## 迁移学习

一个有趣且相对较新的发展是出现了在非常大的文本集合上进行无监督任务预训练的端到端神经模型，这些模型可以替代传统的处理管道：

- 可以通过在架构中添加一些非常浅层的层来构建专门的模型，同时保留预训练的权重，可能只需进行一些微调。

- 似乎传统管道的某些组件在这些模型中有神经类比：某些层似乎学习（更多）形态学，其他层则学习语义等。

> 基础模型（Foundation Models，FM）  
> 这些基础模型通常是端到端预训练的，并且大多数情况下是无监督或半监督的。  
> 通过迁移学习，可以对这些预训练模型进行微调，创建专门的模型。  
> 迁移学习包括改变一些权重、调整它们、添加新权重，甚至可能是组合权重层。  
> 神经网络中的神经激活可以视为一种软流水线元素。  
> 基础模型旨在自动化特征工程的过程，从而减少对人工特征工程的依赖。

## 学习与搜索

我们将遇到的大量监督 NLP 任务可以表述为形式为 $$\hat y = \mathop{\mathrm{argmax}}_{y\in Y(x)}\Psi_\theta(x, y)$$ 的优化问题，其中

- $x\in X$ 和 $Y(x)$ 是任务的输入和潜在输出，

- $\Psi_\theta: X\times Y \rightarrow \mathbb R$ 是一个评分函数或模型，它为输入-输出对 $\langle x, y \rangle$ 分配分数，并由向量 $\theta$ 参数化，

- $\hat y$ 是预测输出。

> 特征工程的目的是减少数据的维度。通过特征生成模型（即基础模型）来实现这一点。  
> 基础模型需要足够大才能发挥作用。小模型通常不被称为基础模型。  
> 基础模型生成的向量空间需要足够大。  
> 提到如果 P 等于 NP，那么模型可能不需要太大。

例如，

- $X$ 可以包含电影评论，$Y$ 可以包含情感标签 Positive、Negative 和 Neutral，$\Psi_\theta$ 可以是一个函数，为评论的可能情感标注分配概率。

- 同样，$X$ 可以是德语文本集，$Y$ 可以是其潜在的英语翻译，$\Psi_\theta$ 为候选翻译分配翻译质量分数。
  这种表述使得可以将问题分解为两个由两个不同模块解决的优化子问题：

- **学习**：找到最优的 $\theta$ 参数。这通常通过在一个大型监督数据集 $\{\langle x_i, y_i \rangle\}_{i=1}^N$ 上优化 $\theta$ 来完成，使用数值优化方法。

- **搜索**：为特定的 $x$ 找到得分最高的 $y$，即计算公式中的 $\mathop{\mathrm{argmax}}$ 的值。由于搜索空间 $Y(x)$ 通常很大，因为潜在的 $y$ 具有复杂的结构（例如，考虑解析树），这个问题经常需要组合优化。

> 目标是将问题形式化为一个优化问题，并寻找一个好的解决方案。  
> 通过最大化某个函数来实现这一点，这个过程称为推理。  
> 通过预测输出和实际测量值之间的差异来进行优化。  
> 目标是最大化这个误差的逆，这就是所谓的学习。  
> 是否有不使用梯度的学习算法，提到零阶优化算法作为一个例子。  
> 提到消除排序（elimination-based sorting）和
> 消除搜索（elimination-based searching）作为学习算法的配对。
> 这种方法与修剪神经网络（pruning neural networks）相关联。

## 语义视角：关系

考虑以下话语

> *我叔叔买了一只猫。它可能是我见过的最讨厌的动物。*

我们如何知道“动物”是指提到的那只猫？一个因素是我们知道*猫*是*动物*的一个子类别：它们通过 is_a 关系连接。

> 计算机需要某种形式的知识来理解语义。  
> 这种知识通常以关系的形式存在，例如“猫是动物”。

**关系视角** 关注表达的意义之间的这些语义/概念链接，它们共同构成语义网络：

词汇语义本体如[WordNet](https://wordnet.princeton.edu/)和[FrameNet](http://framenet.icsi.berkeley.edu/)试图枚举大量词义之间的语义关系。

![语义网络片段 ([维基百科: 语义网络](https://en.wikipedia.org/wiki/Semantic_network)).](https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Semantic_Net.svg/1920px-Semantic_Net.svg.png)

> 关系信息通常以三元组的形式表示，包括源、目标和它们之间的谓词（关系）。  
> 通过关系数据库和语义网络来表示语义信息。  
> 如果你想使用已经形式化的本体论（如 WordNet），那么 WordNet 有一套封闭的关系集可以使用。  
> 过去的语言模型构建原则（如基于规则的翻译模型或对话系统）已经不再是主流。  
> 现代语言模型更注重表示底层信息，而不是依赖于预定义的规则。

## 语义视角：组合性

关系视角将词义视为网络中的原子节点。相反，**组合视角**根据表达式的内部组成来分析其意义。

例如，分解

$$un\vert bear \vert able \vert s$$

使我们能够看到*unbearables*的意义是由其部分*un*与*bear*、*able*和*s*的意义组成的。

> 分解视角，即分析语言中不同符号的内部组成，这种方法关注语言的内部结构和组成部分。

组合性原则：

> *复杂表达式的意义由其组成表达式的意义和用于组合它们的规则决定。*

该原则可以应用于比单词更大的语言单位：句子甚至段落等。

一种（传统的）方法是用逻辑公式表示意义，并将语法组合规则与语义/逻辑规则关联起来：

```prolog
John visits Julie (S)
├── John (NP)
└── visits Julie (VP)
    ├── visits (VT)
    └── Julie (NP)

visits(john, julie)
├── john
└── λx.visits(x, julie)
    ├── λy.λx.visits(x, y)
    └── julie
```

> 如果你了解有限自动机，你会发现组合性原理在数学公式中也有类似的应用。  
> 如果能将一个句子转化为一阶逻辑（first order logic），那就是一种组合语义表示。  
> 即使有很好的表示方法来反映单词之间的联系，仍然存在一个问题：这些表示与文本内容之间的关系是什么。  
> 可以使用 GPT 模型生成摘要，然后将摘要表示为向量。

## 语义视角：分布式

“bardiwac”是什么意思？

- 他递给她一杯**bardiwac**。
- 牛肉菜肴是为了搭配**bardiwacs**而制作的。
- 饮料很美味：血红色的**bardiwac**以及清淡甜美的莱茵酒。
- 奈杰尔的脸因为喝了太多**bardiwac**而变红。
- 马尔贝克是较不知名的**bardiwac**葡萄之一。
- 我吃了面包、奶酪和这款极好的**bardiwac**。

$\Rightarrow$ Bardiwac 是一种由葡萄制成的浓烈红色酒精饮料。

即使我们不知道“bardiwac”在语义网络中的位置，也不知道其部分的含义，但它出现的*上下文*提供了大量关于其含义的信息。

分布假设：

- “你将通过它所处的环境了解一个词。”

- “具有相似分布的语言项目具有相似的含义。”

分布式方法在实际应用中的一个重要优势是，它使得可以从大型但未标注的文本集合中自动学习单词的语义，而不需要专家知识和注释。

当然，这种方法也不是没有局限性：

- 对于罕见词汇存在问题；以及

- 学习相似性而不提供任何解释*为什么*这些分布是相似的。

> 由于缺乏分布式学习材料，这些罕见词语无法被模型有效地表示。  
> 提到“Bank”这个词，过去在语言模型中难以区分“河岸”（riverbank）和
> “金融机构”（monetary institution）的不同含义。  
> 如果模型只能访问分布数据而没有其他信息来源（如上下文或其他模态），
> 那么它将无法正确理解罕见词语的含义。
