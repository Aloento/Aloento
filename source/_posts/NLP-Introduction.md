---
title: NLP-Introduction
toc: true
categories:
  - [AI, NLP]
tags: [笔记, AI, NLP]
date: 2024-09-18 15:53:24
---

本章主要讲述了 NLP 的基本概念，和关键主题

接下来我将把 PPT 和整理过后的笔记穿插在一起成文

<!-- more -->

# 什么是自然语言处理？

**自然语言处理（NLP）** 是一个跨学科领域，旨在使自然语言对计算机可访问。

- **自然语言** 在此上下文中指的是人类用于交流的普通语言，如英语、中文、西班牙语等。
- 能够**访问**自然语言涵盖了广泛的能力，重要领域包括：

- **交流**：接受输入和生成自然语言输出的能力；
- **理解**：能够访问和利用信息和情感内容；
- **语言协助**：帮助人类在语言表达上的能力。

# 相关领域

## 计算语言学

使用计算方法对语言进行科学研究。

- 也许是最接近 NLP 的领域，但重点不同：NLP 并不关心自然语言的理论见解，而只关注对计算语言处理有用的方法的设计和分析。

- 它通常不是直接实现理论思想，而是为 NLP 系统提供**架构灵感**。

## 人工智能 (AI)

显然，NLP 目标与 AI 构建智能系统的目标有很大的重叠：

- 语言使用与成为智能所需的概念、表示和推理能力密切相关，
- 实际上，如果没有从自然语言输入中提取信息的能力，大规模知识获取也是不可能的。

上述特征使得 AI 子领域 **知识表示** 和 **推理** 对 NLP 尤为重要。

## 机器学习

现代 NLP 在很大程度上依赖于机器学习技术，事实上，近年来通用 ML 方法的语言学应用主导了该领域。

- 主要使用监督或半监督方法，但强化学习的使用也在增加。
- 文本是离散符号的序列，因此需要能够处理这种类型输入（以及生成时的输出）的 ML 模型。

## 语音处理

语音信号的处理和生成传统上不被认为是 NLP 的一部分，NLP 主要关注文本，但显然与其密切相关：

- 语音转文本为 NLP 应用提供输入，
- NLP 应用为语音合成提供输入；
- 处理和合成语音都需要语言学知识，这对于 NLP 也很重要：尤其是语言建模在这两个领域中都起着核心作用。

# 应用

## 应用示例

- 机器翻译，
- 文档检索：检索与用户查询匹配的自由文本文档，
- 问答系统，例如，智能手机助手回答问题的能力，
- 文本分类，例如检测电子邮件垃圾邮件，
- 聊天机器人，例如，用于购买火车票的聊天机器人，
- 拼写检查和语法检查，
- 自由文本输入的自动补全，
- 文档摘要，
- 从结构化数据生成文本（从股票交易新闻到错误消息）。

# 中心主题

## 管道与端到端架构

一种有影响力的 NLP 观点认为其核心任务是提供一个**模块管道**，该管道依次生成通用的语言分析，每个模块基于前一个模块的输出构建：

![来自 [spaCy NLP 库文档](https://spacy.io/usage/processing-pipelines) 的图示。](https://spacy.io/images/pipeline.svg)

然后，专门的 NLP 应用程序作为这个通用管道元素之上的相对简单的附加组件构建。

相反的观点集中在构建 NLP 应用程序作为**端到端**机器学习模型，这些模型学习将原始输入转换为所需的输出，而无需专门的语言分析模块。

最先进的 NLP 应用程序通常介于这两个极端之间：它们使用一些通用的分析模块，例如用于分词或词干提取，并且还依赖于跳过某些传统管道步骤的机器学习模型来生成所需的输出。

## 迁移学习

一个有趣且相对较新的发展是出现了在非常大的文本集合上进行无监督任务预训练的端到端神经模型，这些模型可以替代传统的处理管道：

- 可以通过在架构中添加一些非常浅层的层来构建专门的模型，同时保留预训练的权重，可能只需进行一些微调。

- 似乎传统管道的某些组件在这些模型中有神经类比：某些层似乎学习（更多）形态学，其他层则学习语义等。

## 学习与搜索

我们将遇到的大量监督 NLP 任务可以表述为形式为 $$\hat y = \mathop{\mathrm{argmax}}_{y\in Y(x)}\Psi_\theta(x, y)$$ 的优化问题，其中

- $x\in X$ 和 $Y(x)$ 是任务的输入和潜在输出，

- $\Psi_\theta: X\times Y \rightarrow \mathbb R$ 是一个评分函数或模型，它为输入-输出对 $\langle x, y \rangle$ 分配分数，并由向量 $\theta$ 参数化，

- $\hat y$ 是预测输出。

例如，

- $X$ 可以包含电影评论，$Y$ 可以包含情感标签 Positive、Negative 和 Neutral，$\Psi_\theta$ 可以是一个函数，为评论的可能情感标注分配概率。

- 同样，$X$ 可以是德语文本集，$Y$ 可以是其潜在的英语翻译，$\Psi_\theta$ 为候选翻译分配翻译质量分数。
  这种表述使得可以将问题分解为两个由两个不同模块解决的优化子问题：

- **学习**：找到最优的 $\theta$ 参数。这通常通过在一个大型监督数据集 $\{\langle x_i, y_i \rangle\}_{i=1}^N$ 上优化 $\theta$ 来完成，使用数值优化方法。

- **搜索**：为特定的 $x$ 找到得分最高的 $y$，即计算公式中的 $\mathop{\mathrm{argmax}}$ 的值。由于搜索空间 $Y(x)$ 通常很大，因为潜在的 $y$ 具有复杂的结构（例如，考虑解析树），这个问题经常需要组合优化。

## 语义视角：关系

考虑以下话语

> _我叔叔买了一只猫。它可能是我见过的最讨厌的动物。_

我们如何知道“动物”是指提到的那只猫？一个因素是我们知道*猫*是*动物*的一个子类别：它们通过 is_a 关系连接。

**关系视角** 关注表达的意义之间的这些语义/概念链接，它们共同构成语义网络：

词汇语义本体如[WordNet](https://wordnet.princeton.edu/)和[FrameNet](http://framenet.icsi.berkeley.edu/)试图枚举大量词义之间的语义关系。

![语义网络片段 ([维基百科: 语义网络](https://en.wikipedia.org/wiki/Semantic_network)).](https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Semantic_Net.svg/1920px-Semantic_Net.svg.png)

## 语义视角：组合性

关系视角将词义视为网络中的原子节点。相反，**组合视角**根据表达式的内部组成来分析其意义。

例如，分解

$$un\vert bear \vert able \vert s$$

使我们能够看到*unbearables*的意义是由其部分*un*与*bear*、*able*和*s*的意义组成的。

组合性原则：

> _复杂表达式的意义由其组成表达式的意义和用于组合它们的规则决定。_

该原则可以应用于比单词更大的语言单位：句子甚至段落等。

一种（传统的）方法是用逻辑公式表示意义，并将语法组合规则与语义/逻辑规则关联起来：

```prolog
John visits Julie (S)
├── John (NP)
└── visits Julie (VP)
    ├── visits (VT)
    └── Julie (NP)

visits(john, julie)
├── john
└── λx.visits(x, julie)
    ├── λy.λx.visits(x, y)
    └── julie
```

## 语义视角：分布式

“bardiwac”是什么意思？

- 他递给她一杯**bardiwac**。
- 牛肉菜肴是为了搭配**bardiwacs**而制作的。
- 饮料很美味：血红色的**bardiwac**以及清淡甜美的莱茵酒。
- 奈杰尔的脸因为喝了太多**bardiwac**而变红。
- 马尔贝克是较不知名的**bardiwac**葡萄之一。
- 我吃了面包、奶酪和这款极好的**bardiwac**。

$\Rightarrow$ Bardiwac 是一种由葡萄制成的浓烈红色酒精饮料。

即使我们不知道“bardiwac”在语义网络中的位置，也不知道其部分的含义，但它出现的*上下文*提供了大量关于其含义的信息。

分布假设：

- “你将通过它所处的环境了解一个词。”

- “具有相似分布的语言项目具有相似的含义。”

分布式方法在实际应用中的一个重要优势是，它使得可以从大型但未标注的文本集合中自动学习单词的语义，而不需要专家知识和注释。

当然，这种方法也不是没有局限性：

- 对于罕见词汇存在问题；以及

- 学习相似性而不提供任何解释*为什么*这些分布是相似的。
