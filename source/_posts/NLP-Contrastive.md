---
title: NLP-Contrastive
toc: true
categories:
  - [AI, NLP]
tags: [笔记, AI, NLP]
date: 2024-12-09 15:59:00
---

对比表征学习

<!-- more -->

# 自监督学习

## 主要目标

自监督学习（Self-supervised learning）旨在从数据本身获取监督。

“从一切中预测一切。”
*Yann Lecun*

数据是部分已知，部分未知的。
利用数据的潜在结构（例如语言建模中的顺序性）。

![ssl_meme](ssl_meme.png)

为什么不是强化学习？
*试错法无效。*

## 优势

自监督学习：

- 降低标注的成本和复杂性
- 增加系统的额外泛化 generalization 能力
- 使得可以利用数据的内部结构
- 能够重建控制输入集的潜在 latent 变量

## 基于能量的建模

基于能量的建模（Energy-based modeling）是大多数 SSL 方法的统一原则。

EBM 解决了 $L_2$ 类损失的“平均问题”。

- 想象一个有多个可行输出的情况（例如 Skipgram 模型中的相邻词）
- 损失将对这些单个输出的“平均值”最小化
- 我们希望损失函数对每一个可行解都接近最小

## 能量函数

能量函数 $F(x, y)$ 在 $x \in X$ 输入空间和 $y \in Y$ 输出空间上设计来解决这个问题，其中低能量意味着可行解。

这种模型的推理可以通过：$\hat{y} = argmin_y F(x, y)$

*需要注意的是，多个 $\hat{y}$ 可能是可行的！*

能量函数 $F(x, y)$ 衡量 $x$ 和 $y$ 之间的兼容性。
