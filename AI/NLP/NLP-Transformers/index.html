<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>NLP-Transformers - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="Attention is all you need"><meta property="og:type" content="blog"><meta property="og:title" content="NLP-Transformers"><meta property="og:url" content="https://aloen.to/AI/NLP/NLP-Transformers/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="Attention is all you need"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/dotprodattn.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_attention_heads_qkv.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/mhead2.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/mha.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_resideual_layer_norm_2.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_encoder.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_dec.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_posenc.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/positional.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/posenc1.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/posenc2.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/transformer_full.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/diagram_bartpost_gpt2.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/elmo.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/flair.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/gpt.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/bert1.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/bert2.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-Transformers/bigbird.jpg"><meta property="article:published_time" content="2024-11-06T10:18:54.000Z"><meta property="article:modified_time" content="2024-12-16T20:06:53.971Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="笔记"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/AI/NLP/NLP-Transformers/dotprodattn.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/NLP/NLP-Transformers/"},"headline":"NLP-Transformers","image":["https://aloen.to/AI/NLP/NLP-Transformers/dotprodattn.png","https://aloen.to/AI/NLP/NLP-Transformers/transformer_attention_heads_qkv.jpg","https://aloen.to/AI/NLP/NLP-Transformers/mhead2.jpg","https://aloen.to/AI/NLP/NLP-Transformers/mha.png","https://aloen.to/AI/NLP/NLP-Transformers/transformer_resideual_layer_norm_2.jpg","https://aloen.to/AI/NLP/NLP-Transformers/transformer_encoder.png","https://aloen.to/AI/NLP/NLP-Transformers/transformer_dec.png","https://aloen.to/AI/NLP/NLP-Transformers/transformer_posenc.png","https://aloen.to/AI/NLP/NLP-Transformers/positional.png","https://aloen.to/AI/NLP/NLP-Transformers/posenc1.png","https://aloen.to/AI/NLP/NLP-Transformers/posenc2.png","https://aloen.to/AI/NLP/NLP-Transformers/transformer_full.jpg","https://aloen.to/AI/NLP/NLP-Transformers/diagram_bartpost_gpt2.jpg","https://aloen.to/AI/NLP/NLP-Transformers/elmo.jpg","https://aloen.to/AI/NLP/NLP-Transformers/flair.jpg","https://aloen.to/AI/NLP/NLP-Transformers/gpt.jpg","https://aloen.to/AI/NLP/NLP-Transformers/bert1.jpg","https://aloen.to/AI/NLP/NLP-Transformers/bert2.jpg","https://aloen.to/AI/NLP/NLP-Transformers/bigbird.jpg"],"datePublished":"2024-11-06T10:18:54.000Z","dateModified":"2024-12-16T20:06:53.971Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/NLP/NLP-Transformers/"}},"description":"Attention is all you need"}</script><link rel="canonical" href="https://aloen.to/AI/NLP/NLP-Transformers/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-06T10:18:54.000Z" title="11/6/2024, 10:18:54 AM">2024-11-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-12-16T20:06:53.971Z" title="12/16/2024, 8:06:53 PM">2024-12-17</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/NLP/">NLP</a></span><span class="level-item">28 minutes read (About 4265 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP-Transformers</h1><div class="content"><p><del>Attention is all you need</del></p>
<span id="more"></span>

<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>最早的有影响力的 seq2seq 模型是 RNN，但后来的发展是通过发明 <em>transformers</em> 实现的，这是一种使用注意力机制作为完整层（full-fledged layers）的新型架构，而不是作为辅助 RNN 组件。</p>
<p>新架构的主要构建块是：</p>
<ul>
<li>作为软字典查找的注意力</li>
<li>自注意力层，和</li>
<li>transformer 模块</li>
</ul>
<h2 id="直观感受"><a href="#直观感受" class="headerlink" title="直观感受"></a>直观感受</h2><p>经典数据库查询（宠物店）：</p>
<p>查询：Key <strong>&#x3D;</strong> “cat”</p>
<table>
<thead>
<tr>
<th>Key（动物）</th>
<th>Value（价格）</th>
</tr>
</thead>
<tbody><tr>
<td>cat</td>
<td>1</td>
</tr>
<tr>
<td>dog</td>
<td>2</td>
</tr>
<tr>
<td>cat</td>
<td>3</td>
</tr>
<tr>
<td>parrot</td>
<td>4</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Key（动物）</th>
<th>Value（价格）</th>
<th>选择权重</th>
</tr>
</thead>
<tbody><tr>
<td>cat</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>dog</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>cat</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>parrot</td>
<td>4</td>
<td>0</td>
</tr>
</tbody></table>
<p>Output &#x3D; $1 \cdot 1 + 2 \cdot 0 + 3 \cdot 1 + 4 \cdot 0 &#x3D; 4$</p>
<p><strong>软</strong>数据库查询（宠物店）：</p>
<p>查询：Key <strong>~</strong> “cat”</p>
<table>
<thead>
<tr>
<th>Key（动物）</th>
<th>Value（价格）</th>
<th>选择权重</th>
</tr>
</thead>
<tbody><tr>
<td>cat</td>
<td>1</td>
<td><strong>0.4</strong></td>
</tr>
<tr>
<td>dog</td>
<td>2</td>
<td><strong>0.15</strong></td>
</tr>
<tr>
<td>cat</td>
<td>3</td>
<td><strong>0.4</strong></td>
</tr>
<tr>
<td>parrot</td>
<td>4</td>
<td><strong>0.05</strong></td>
</tr>
</tbody></table>
<p>Output &#x3D; $1 \cdot 0.4 + 2 \cdot 0.15 + 3 \cdot 0.4 + 4 \cdot 0.05 &#x3D; 2.1$</p>
<h1 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h1><p>回顾一下，注意力机制提供了基于查询的 $\langle \mathbf{x}_1,\dots, \mathbf{x}_n\rangle$ 向量序列的聚合：给定一个 $\mathbf{x^*}$ 查询向量，它们计算一个相关性分数序列</p>
<p>$$\mathbf{s} &#x3D; \langle s(\mathbf{x}_1, \mathbf{x}^*),\dots, s(\mathbf{x}_n, \mathbf{x}^*) \rangle$$</p>
<p>并返回加权和</p>
<p>$$\mathop{\mathrm{softmax}}(\mathbf{s})\cdot \langle \mathbf{x}_1,\dots, \mathbf{x}_n\rangle$$</p>
<p>作为根据相关性分数对 $\mathbf{x}_i$ 进行总结或聚合的结果。</p>
<p>$s(\cdot, \cdot)$ 评分函数有所不同，我们看到一个选项是使用缩放点积：</p>
<p>$$s(\mathbf{x}_i, \mathbf{x}^*) &#x3D; \frac{\mathbf{x}_i\cdot \mathbf{x}^*}{\sqrt{d}}$$</p>
<p>其中 $d$ 是 $\mathbf{x_i}$ 和 $\mathbf{x^*}$ 的维数。</p>
<p>基于这个模式，transformer 注意力机制做了一个关键的改变：将 $\langle \mathbf{x}_1,\dots, \mathbf{x}_n\rangle$ 视为一个 <em>字典</em>，其中有 $\mathcal K(\cdot)$ 和 $\mathcal V(\cdot)$ 映射，将每个 $\mathbf{x}_i$ 映射到相应的 $\mathcal K(\mathbf{x}_i)$ 键和 $\mathcal V(\mathbf{x}_i)$ 值。</p>
<p>假设还有一个 $\mathcal Q(\cdot)$ <em>查询</em> 映射，它将 $\mathbf{x}^*$ 映射到 $\mathcal K$(.) 的范围（“key-space”），评分可以重新表述为计算查询和键之间的点积相似度分数</p>
<p>$$s(\mathbf{x}_i, \mathbf{x}^*) &#x3D; \frac{\mathcal K (\mathbf{x}_i)\cdot \mathcal Q (\mathbf{x}^*)}{\sqrt{d}}$$</p>
<p>（$d$ 现在是“key-space”的维数），检索到的值将是加权和</p>
<p>$$\mathop{\mathrm{softmax}}(\langle s(\mathbf{x}_1,\mathbf{x}^*),\dots,s(\mathbf{x}_n,\mathbf{x}^*) \rangle)\cdot \mathcal V(\langle \mathbf{x_1},\dots,\mathbf{x}_n)\rangle$$</p>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><p>然而，这种注意力机制存在一个问题。假设所有序列的每个向量元素都来自标准正态分布 $\mathcal{N}(0, 1)$。它们的点积 $\sum\limits_{d}\mathbf{x}_i\cdot\mathbf{x}_i^*$ 将具有 $\mathcal{N}(0, d)$ 的分布，其中 $d$ 是向量的维数。为了将输出缩放回标准正态分布，点积被缩放为 $\frac{1}{\sqrt{d}}$。</p>
<p><img src="/AI/NLP/NLP-Transformers/dotprodattn.png" alt="Dot-product attention"></p>
<h1 id="Attention-as-a-layer"><a href="#Attention-as-a-layer" class="headerlink" title="Attention as a layer"></a>Attention as a layer</h1><p>所述的注意力机制可以用作独立层来转换输入向量序列 $\mathbf{I} &#x3D; \langle \mathbf{i}_1,\dots, \mathbf{i}_n \rangle$：</p>
<p>给定另一个序列 $\mathbf{X} &#x3D; \langle \mathbf{x}_1,\dots, \mathbf{x}_m \rangle$ 和 $\mathcal K(\cdot),\mathcal V(\cdot),\mathcal Q(\cdot)$ 映射，对于每个输入 $\mathbf{i_k}$，我们可以计算相应的 $\mathcal Q(\mathbf{i}_k)$ 查询，并使用它与 $\mathcal K$ 和 $\mathcal V$ 来 <em>关注</em> $\mathbf{X}$ 并计算相应的注意力响应 $\mathbf{o}_k$。</p>
<p>结果是一个 $\mathbf{O}&#x3D;\langle \mathbf{o}_1,\dots,\mathbf{o}_n \rangle$ 输出序列，整体上是输入 $\mathbf{I}$ 的层输出。</p>
<h2 id="注意力层类型"><a href="#注意力层类型" class="headerlink" title="注意力层类型"></a>注意力层类型</h2><p>根据层关注的位置（$\mathbf{X}$ 的来源），我们可以区分自注意力层和交叉注意力层。</p>
<ul>
<li>在 <em><strong>自注意力</strong></em> 层中，从输入生成的查询用于查询输入本身：$\mathbf{X}&#x3D;\mathbf{I}$</li>
<li>在 <em><strong>交叉注意力</strong></em> 层中，查询的是外部向量序列，例如，在编码器-解码器 transformer 架构中，由编码器创建的序列</li>
</ul>
<p>至于映射 $\mathcal K(\cdot),\mathcal V(\cdot),\mathcal Q(\cdot)$，这三者通常都实现为<em>线性投影</em>，具有学习到的权重矩阵 $W_K, W_V, W_Q$。</p>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p>为了能够关注输入的多个方面，transformers 中的注意力层包含几个并行的注意力“头”，每个头都有不同的 $W_K, W_V, W_Q$ 三元组：</p>
<p><img src="/AI/NLP/NLP-Transformers/transformer_attention_heads_qkv.jpg" alt="transformer_attention_heads_qkv"></p>
<p>头输出被组合成一个层输出：</p>
<p><img src="/AI/NLP/NLP-Transformers/mhead2.jpg" alt="mhead2"></p>
<p><img src="/AI/NLP/NLP-Transformers/mha.png" alt="Multi-head attention layer"></p>
<h1 id="Transformer-模块"><a href="#Transformer-模块" class="headerlink" title="Transformer 模块"></a>Transformer 模块</h1><p>transformers 的构建块是由注意力和简单的分段前馈层组成的 <em>transformer 模块</em>。最简单的变体只包含一个自注意力层：</p>
<p><img src="/AI/NLP/NLP-Transformers/transformer_resideual_layer_norm_2.jpg" alt="transformer_resideual_layer_norm_2"></p>
<h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>编码器由 $N$ 个相同的层组成，这些层具有自注意力和逐元素（element-wise） FFN 模块，以及残差连接。编码序列（上下文）是最后一个编码器层的输出。每个自注意力都是双向的。</p>
<p><img src="/AI/NLP/NLP-Transformers/transformer_encoder.png" alt="Transformer Encoder"></p>
<h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>解码器由 $N$ 个相同的层组成，这些层具有自注意力、交叉注意力和 FFN 模块，以及残差连接。交叉注意力将编码序列作为键和值，而查询来自解码器。每个自注意力是单向的，交叉注意力是双向的。</p>
<p><img src="/AI/NLP/NLP-Transformers/transformer_dec.png" alt="Transformer Decoder"></p>
<h2 id="嵌入和位置编码"><a href="#嵌入和位置编码" class="headerlink" title="嵌入和位置编码"></a>嵌入和位置编码</h2><p>Transformers 是为符号序列（例如文本）发明的，因此使用嵌入层将输入标记转换为向量表示。然后将此嵌入添加到位置编码向量中，该向量用于向模型传达位置信息。</p>
<p><img src="/AI/NLP/NLP-Transformers/transformer_posenc.png" alt="Transformer input embeddings"></p>
<p><img src="/AI/NLP/NLP-Transformers/positional.png" alt="Sinusoid (正弦) positional encoding"></p>
<p><img src="/AI/NLP/NLP-Transformers/posenc1.png" alt="Sinusoid positional encoding"></p>
<p><img src="/AI/NLP/NLP-Transformers/posenc2.png" alt="Dot-product of positional encodings"></p>
<h2 id="Seq2seq-Transformer"><a href="#Seq2seq-Transformer" class="headerlink" title="Seq2seq Transformer"></a>Seq2seq Transformer</h2><p><img src="/AI/NLP/NLP-Transformers/transformer_full.jpg" alt="transformer_full"></p>
<p>原始的 <em>全 transformer 模型</em> 是一个完全由 transformer 块构建的 Seq2seq 编码器-解码器 模型。在推理过程中，解码器部分逐步预测，类似于 RNNs 消耗已经预测的输出，但在训练过程中，它只需要通过教师强制进行一次前向传递。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>通过向解码器添加分类头，可以在两个序列上训练模型。给定完整的输入序列，解码器被训练来预测输出序列中的下一个元素。</p>
<p>为了生成完整的序列，模型以自回归（auto-regressive）方式使用。模型的输出用作下一步的输入。然而，单个错误预测将导致错误的级联。为避免这种情况，模型通过教师强制进行训练。</p>
<h2 id="掩码"><a href="#掩码" class="headerlink" title="掩码"></a>掩码</h2><p>掩码用于防止模型关注某些元素。Transformers 中主要有两种类型的掩码：</p>
<ul>
<li>填充（Padding）掩码</li>
<li>前瞻（Look-ahead）掩码（因果 causal 掩码）</li>
</ul>
<p><img src="/AI/NLP/NLP-Transformers/diagram_bartpost_gpt2.jpg" alt="GPT-2 中的因果掩码"></p>
<h2 id="编码器风格和解码器风格的模型"><a href="#编码器风格和解码器风格的模型" class="headerlink" title="编码器风格和解码器风格的模型"></a>编码器风格和解码器风格的模型</h2><p>某些应用只需要模型处理单个序列（例如语言建模）。在这种情况下，不需要交叉注意力和两个模块。我们只使用编码器或解码器（没有交叉注意力）。当存在双向信息时，使用编码器风格的模型，而对于因果问题，则使用解码器风格的模型。因此，两者之间的唯一区别是因果掩码。</p>
<h1 id="上下文嵌入"><a href="#上下文嵌入" class="headerlink" title="上下文嵌入"></a>上下文嵌入</h1><h2 id="词嵌入的局限性"><a href="#词嵌入的局限性" class="headerlink" title="词嵌入的局限性"></a>词嵌入的局限性</h2><p>传统的基于共现矩阵的词向量和第一代神经词嵌入有几个重要的局限性：</p>
<ul>
<li><p><em>上下文独立性:</em> 一个表面形式只有一个表示。例如，<em>bank</em> 在以下句子中的嵌入是相同的：</p>
<p><em>I went to my bank to withdraw some money.</em> （我去银行取了一些钱）</p>
<p><em>We explored the river bank.</em> （我们探索了河岸）</p>
<p>尽管这两个意思显然是不同的</p>
</li>
<li><p><em>单词是黑箱:</em> 单词有内部结构：它们由字符组成，可以由几个词素组成，但 Word2vec、GloVe 等忽略了单词的内部结构</p>
</li>
<li><p><em>对未见过或罕见单词没有有用的表示:</em> 由于单词被视为黑箱，这些模型无法为训练语料库中未出现或非常罕见的单词生成有用的表示</p>
</li>
<li><p><em>良好的覆盖需要巨大的模型尺寸:</em> 一个单词只有在明确包含在模型的词汇表中时才会获得有意义的表示，但内存消耗通常是覆盖词汇表的线性函数</p>
</li>
</ul>
<p>利用内部单词结构、处理 OOV 单词和减少词汇量的问题已通过以下方法有效解决：</p>
<ul>
<li>fastText 嵌入，尤其是</li>
<li>子词嵌入</li>
</ul>
<p>但这些嵌入仍然是<em>静态的</em>，即将相同形式的标记映射到相同的嵌入向量。</p>
<p>NLP 领域最近最重要的发展之一是<em>上下文嵌入</em>的出现，与之相反，上下文嵌入可以根据上下文的不同来改变相同表面形式的嵌入，以反映语言差异。</p>
<h2 id="Contextual-embeddings"><a href="#Contextual-embeddings" class="headerlink" title="Contextual embeddings"></a>Contextual embeddings</h2><p><em>上下文嵌入</em> 是由深度网络（通常是基于 LSTM 或自注意力机制）生成的单词或子词表示，这些网络在自监督的、广泛的语言建模目标上进行（预）训练。</p>
<p>与静态嵌入不同，这些表示不能简单地以查找表的形式存储和部署，因为它们是根据每个标记的上下文<em>动态</em>计算的：对于一个 $\mathbf{w} &#x3D; \langle w_1,\dots ,w_n \rangle$ 输入标记序列，网络生成一个嵌入序列</p>
<p>$$E(\langle w_1,\dots ,w_n \rangle) &#x3D; \langle E_\mathbf{w}(w_1),\dots,E_\mathbf{w}(w_n)\rangle$$</p>
<p>由于这种动态特性，网络本身必须用作 <em>特征提取模块</em>。</p>
<p>在 NLP 中，生成上下文嵌入的网络的预期用途类似于传统 NLP 中处理管道的角色：它们应该生成对下游任务有用的特征向量，实际上，希望只需要少量的进一步处理（例如，以浅层神经网络的形式）就能构建有用的 NLP 模型。</p>
<p>巨大的区别在于，上下文嵌入可以通过<em>自监督方式</em>学习，而不需要昂贵的监督训练集。</p>
<h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo（来自语言模型的嵌入，Embeddings from Language Models），第一个历史上重要的上下文嵌入模型，通过两个标准的单向语言建模任务学习词表示。</p>
<p>该架构首先使用字符级卷积生成上下文无关的嵌入，然后使用前向和后向双向 LSTM 层（它们的数量 $n$ 是一个可变的超参数）通过权重共享的 softmax 层预测下一个&#x2F;上一个标记。</p>
<p><img src="/AI/NLP/NLP-Transformers/elmo.jpg" alt="elmo"></p>
<p>在第一近似（approximation）中，上下文相关的嵌入是模型生成的所有 $2n +1$ 个中间表示（$2n$ 个基于上下文的 LSTM 和一个静态字符的表示）。</p>
<p>尽管这些向量可以一起被视为“完整的”ELMo 表示，但对于实际的下游 NLP 任务，ELMo 的创建者实际上建议不要使用这种非常高维的表示，而是使用这些向量的低维组合。他们建议的解决方案是</p>
<ul>
<li>简单地连接顶层 LSTM 层（前向和后向）的输出</li>
<li>在监督任务上学习 ELMo 表示的任务特定线性组合</li>
</ul>
<h2 id="FLAIR"><a href="#FLAIR" class="headerlink" title="FLAIR"></a>FLAIR</h2><p>FLAIR 是一种与 ELMo 密切相关的上下文嵌入模型，但</p>
<ul>
<li>完全由循环<em>字符级</em>语言模型（一个前向和一个后向）组成</li>
<li>从 LSTM 隐藏状态在标记的第一个和最后一个字符（从后向 LM 的第一个字符和从前向 LM 的最后一个字符）生成标记级嵌入</li>
</ul>
<p>FLAIR 嵌入在序列标注任务中被证明非常有用，使用它们的浅层模型目前在命名实体识别（NER）和词性标注（POS-tagging）中排名第二。</p>
<p><img src="/AI/NLP/NLP-Transformers/flair.jpg" alt="flair"></p>
<h2 id="基于-Transformer-的上下文嵌入"><a href="#基于-Transformer-的上下文嵌入" class="headerlink" title="基于 Transformer 的上下文嵌入"></a>基于 Transformer 的上下文嵌入</h2><p>Transformer 架构最初用于翻译（2017 年），但从 2018 年开始，开发了一系列基于 Transformer 的模型来生成上下文嵌入。最重要的研究领域是：</p>
<ul>
<li>寻找有助于学习高质量表示的自监督任务</li>
<li>架构改进，特别是找到更高效的注意力变体</li>
<li>如何为下游任务 adapt&#x2F;fine-tune 预训练的 representation 网络</li>
</ul>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT（Generative Pre-Training）是一种基于 BPE 的，仅使用解码器的 transformer 模型，使用传统的“预测下一个标记”语言建模目标进行训练。上下文嵌入只是顶层 transformer 模块的输出。</p>
<p>与 ELMo 类似，GPT 的主要目标是提供一个有用的预训练“特征提取”模块，可以针对监督的 NLP 任务进行微调。微调意味着在监督下游任务上以端到端的方式更改预训练的 GPT 权重。</p>
<p><img src="/AI/NLP/NLP-Transformers/gpt.jpg" alt="gpt"></p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>下一个具有高度影响力的模型是谷歌的 BERT（Bidirectional Encoder Representations from Transformers），其主要创新是</p>
<ul>
<li><p>使用了两个新的自监督目标，而不是传统的语言建模</p>
<ul>
<li>掩码语言模型（masked language modeling）以及</li>
<li>下一句预测（next sentence prediction, NSP）和</li>
</ul>
</li>
<li><p>相应的架构变化：该模型基于 <em>transformer 编码器</em> 架构</p>
</li>
</ul>
<h3 id="掩码语言模型"><a href="#掩码语言模型" class="headerlink" title="掩码语言模型"></a>掩码语言模型</h3><p>目标是猜测随机掩码的标记：</p>
<p><img src="/AI/NLP/NLP-Transformers/bert1.jpg" alt="bert1"></p>
<h3 id="下一句预测"><a href="#下一句预测" class="headerlink" title="下一句预测"></a>下一句预测</h3><p>第二个目标是判断两句话在训练语料库中是否相互跟随或是随机抽取的：</p>
<p><img src="/AI/NLP/NLP-Transformers/bert2.jpg" alt="bert2"></p>
<h2 id="微调上下文嵌入"><a href="#微调上下文嵌入" class="headerlink" title="微调上下文嵌入"></a>微调上下文嵌入</h2><p>预训练语言模型生成的上下文嵌入不一定适用于具体的下游任务（分类、语义搜索等），因此可以通过<em>微调</em>预训练权重来提高性能。</p>
<p>微调可以通过以下方式进行：</p>
<ul>
<li>在更能代表目标领域的语料库上使用<em>无监督任务</em>（这些任务通常与预训练任务相同）</li>
<li>在与目标任务相同或相关的<em>监督任务</em>上进行，例如语义搜索的相似性排序</li>
</ul>
<h2 id="后续趋势"><a href="#后续趋势" class="headerlink" title="后续趋势"></a>后续趋势</h2><p>更新的模型在 NLP 任务中不断刷新最先进的技术，但通常伴随着<em>参数数量的增加</em>和<em>更大的数据集</em>：</p>
<p>虽然最初的 ELMo 模型有 9360 万个参数，但 GPT-3 有 1750 亿个参数，数据集的规模从 8 亿个标记增加到 3000 亿个标记。</p>
<h2 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h2><p>模型规模的巨大增加导致了对<em>知识蒸馏</em> （distillation）方法的深入研究，以便能够基于原始模型生成更小、更高效的模型，而不会显著损失性能。</p>
<p>一个很好的例子是<em>DistilBERT</em>，这是一个经过蒸馏的 BERT 版本，旨在模仿 BERT 的输出。DistilBERT 保留了 BERT 97% 的性能，但参数减少了 40%，推理速度提高了 39%。</p>
<h2 id="稀疏注意力变体"><a href="#稀疏注意力变体" class="headerlink" title="稀疏注意力变体"></a>稀疏注意力变体</h2><p>提高效率的另一种方法是减少自注意力层中的注意力范围，因为在全注意力中，计算点积的数量与输入标记的数量成平方关系。线性替代方案包括：</p>
<ul>
<li><em>全局注意力</em>: 一组全局标记关注整个序列；</li>
<li><em>随机注意力</em>: 对于每个查询，计算一组 $r$ 个随机键，该查询关注这些键；</li>
<li><em>窗口注意力</em>: 仅关注固定半径内的局部邻居。</li>
</ul>
<p>Big Bird 上下文嵌入模型结合了所有这些线性注意力类型，以显著增加输入标记的数量，而不会显著改变内存需求：</p>
<p><img src="/AI/NLP/NLP-Transformers/bigbird.jpg" alt="bigbird"></p>
<h2 id="少样本学习、单样本学习和零样本学习"><a href="#少样本学习、单样本学习和零样本学习" class="headerlink" title="少样本学习、单样本学习和零样本学习"></a>少样本学习、单样本学习和零样本学习</h2><p>一个有趣的方向是尝试直接使用模型，而不在下游任务上添加层和进行梯度更新。一些最近的模型，最重要的是 GPT-3，在各种下游任务中表现出令人惊讶的效果，这些任务在输入中进行了说明。有三种学习设置：</p>
<ul>
<li><em><strong>零样本学习</strong></em>：输入仅包含监督任务的简短描述和一个具体的输入实例提示，例如“将英语翻译成法语：cheese &#x3D;$&gt;$ ”</li>
<li><em><strong>单样本学习</strong></em> 和 <em><strong>少样本学习</strong></em>：除了简短的任务描述外，输入还包含一个或几个训练示例，然后是提示</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP-Transformers</p><p><a href="https://aloen.to/AI/NLP/NLP-Transformers/">https://aloen.to/AI/NLP/NLP-Transformers/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-11-06</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-12-17</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI/NLP/NLP-DialogSystems/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">NLP-DialogSystems</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/NLP/NLP-PreExamA/"><span class="level-item">NLP-PreExamA</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">52</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">34</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">30</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">41</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#介绍"><span class="level-left"><span class="level-item">1</span><span class="level-item">介绍</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#直观感受"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">直观感受</span></span></a></li></ul></li><li><a class="level is-mobile" href="#数学公式"><span class="level-left"><span class="level-item">2</span><span class="level-item">数学公式</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Scaling"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Scaling</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Attention-as-a-layer"><span class="level-left"><span class="level-item">3</span><span class="level-item">Attention as a layer</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#注意力层类型"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">注意力层类型</span></span></a></li><li><a class="level is-mobile" href="#多头注意力"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">多头注意力</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Transformer-模块"><span class="level-left"><span class="level-item">4</span><span class="level-item">Transformer 模块</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#编码器"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">编码器</span></span></a></li><li><a class="level is-mobile" href="#解码器"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">解码器</span></span></a></li><li><a class="level is-mobile" href="#嵌入和位置编码"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">嵌入和位置编码</span></span></a></li><li><a class="level is-mobile" href="#Seq2seq-Transformer"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">Seq2seq Transformer</span></span></a></li><li><a class="level is-mobile" href="#训练"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">训练</span></span></a></li><li><a class="level is-mobile" href="#掩码"><span class="level-left"><span class="level-item">4.6</span><span class="level-item">掩码</span></span></a></li><li><a class="level is-mobile" href="#编码器风格和解码器风格的模型"><span class="level-left"><span class="level-item">4.7</span><span class="level-item">编码器风格和解码器风格的模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#上下文嵌入"><span class="level-left"><span class="level-item">5</span><span class="level-item">上下文嵌入</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#词嵌入的局限性"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">词嵌入的局限性</span></span></a></li><li><a class="level is-mobile" href="#Contextual-embeddings"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Contextual embeddings</span></span></a></li><li><a class="level is-mobile" href="#ELMo"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">ELMo</span></span></a></li><li><a class="level is-mobile" href="#FLAIR"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">FLAIR</span></span></a></li><li><a class="level is-mobile" href="#基于-Transformer-的上下文嵌入"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">基于 Transformer 的上下文嵌入</span></span></a></li><li><a class="level is-mobile" href="#GPT"><span class="level-left"><span class="level-item">5.6</span><span class="level-item">GPT</span></span></a></li><li><a class="level is-mobile" href="#BERT"><span class="level-left"><span class="level-item">5.7</span><span class="level-item">BERT</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#掩码语言模型"><span class="level-left"><span class="level-item">5.7.1</span><span class="level-item">掩码语言模型</span></span></a></li><li><a class="level is-mobile" href="#下一句预测"><span class="level-left"><span class="level-item">5.7.2</span><span class="level-item">下一句预测</span></span></a></li></ul></li><li><a class="level is-mobile" href="#微调上下文嵌入"><span class="level-left"><span class="level-item">5.8</span><span class="level-item">微调上下文嵌入</span></span></a></li><li><a class="level is-mobile" href="#后续趋势"><span class="level-left"><span class="level-item">5.9</span><span class="level-item">后续趋势</span></span></a></li><li><a class="level is-mobile" href="#知识蒸馏"><span class="level-left"><span class="level-item">5.10</span><span class="level-item">知识蒸馏</span></span></a></li><li><a class="level is-mobile" href="#稀疏注意力变体"><span class="level-left"><span class="level-item">5.11</span><span class="level-item">稀疏注意力变体</span></span></a></li><li><a class="level is-mobile" href="#少样本学习、单样本学习和零样本学习"><span class="level-left"><span class="level-item">5.12</span><span class="level-item">少样本学习、单样本学习和零样本学习</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-15T08:01:40.000Z">2024-12-15</time></p><p class="title"><a href="/AI/NLP/NLP-PreExamC/">NLP-PreExamC</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-14T08:01:02.000Z">2024-12-14</time></p><p class="title"><a href="/AI/NLP/NLP-DeepSTT/">NLP-DeepSTT</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-13T08:00:44.000Z">2024-12-13</time></p><p class="title"><a href="/AI/NLP/NLP-STT/">NLP-STT</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-12T08:00:25.000Z">2024-12-12</time></p><p class="title"><a href="/AI/NLP/NLP-VisionActionModels/">NLP-VisionActionModels</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-11T07:59:59.000Z">2024-12-11</time></p><p class="title"><a href="/AI/NLP/NLP-TextImageModels/">NLP-TextImageModels</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2024 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>