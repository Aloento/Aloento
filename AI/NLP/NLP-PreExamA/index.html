<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>NLP-PreExamA - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="期中考试，人已死"><meta property="og:type" content="blog"><meta property="og:title" content="NLP-PreExamA"><meta property="og:url" content="https://aloen.to/AI/NLP/NLP-PreExamA/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="期中考试，人已死"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/img/og_image.png"><meta property="article:published_time" content="2024-10-11T12:41:56.000Z"><meta property="article:modified_time" content="2024-12-31T01:03:02.420Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="笔记"><meta property="article:tag" content="考试"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/NLP/NLP-PreExamA/"},"headline":"NLP-PreExamA","image":["https://aloen.to/img/og_image.png"],"datePublished":"2024-10-11T12:41:56.000Z","dateModified":"2024-12-31T01:03:02.420Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/NLP/NLP-PreExamA/"}},"description":"期中考试，人已死"}</script><link rel="canonical" href="https://aloen.to/AI/NLP/NLP-PreExamA/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-11T12:41:56.000Z" title="10/11/2024, 12:41:56 PM">2024-10-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-12-31T01:03:02.420Z" title="12/31/2024, 1:03:02 AM">2024-12-31</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/NLP/">NLP</a></span><span class="level-item">36 minutes read (About 5439 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP-PreExamA</h1><div class="content"><p>期中考试，人已死</p>
<span id="more"></span>

<h1 id="A1-语言结构和语法"><a href="#A1-语言结构和语法" class="headerlink" title="A1 语言结构和语法"></a>A1 语言结构和语法</h1><p>Linguistic structure and grammars (Linguistic structure, representation levels, grammars, parsing task, generation task, relation to NLP pipelines)</p>
<h2 id="语言结构"><a href="#语言结构" class="headerlink" title="语言结构"></a>语言结构</h2><h3 id="表示层次"><a href="#表示层次" class="headerlink" title="表示层次"></a>表示层次</h3><p>phonological 也就是类似于字母表，音素音标的最小单元，通常本身没有意义</p>
<p>随后它们可以构成 morphological，也就是词根，词缀，词尾等，这些构成了词，是有意义的最小单位</p>
<p>词可以构成句子，这就是 syntactic，但是光满足语法要求不能说明句子有意义</p>
<p>所以我们有 semantic，去解释句子的意义和指代的对象等</p>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p>grammar 指导你如何构建句子，如何解析句子</p>
<p>同时也包括 phonological，也就是发音的规则，声调，哪些音可以拼在一起等</p>
<p>当然也包括 morphological，比如进行时过去时之类的</p>
<h2 id="解析任务"><a href="#解析任务" class="headerlink" title="解析任务"></a>解析任务</h2><p>基本上来说就是判断一个句子是否符合语法规则</p>
<p>然后再分析它们的结构，找出句子表达的意义</p>
<h2 id="生成任务"><a href="#生成任务" class="headerlink" title="生成任务"></a>生成任务</h2><p>有两种，一种是很傻的无条件生成，也就是不管句子是否有意义，直接按语法规则生成</p>
<p>另一种是有条件生成，生成满足一定需求的句子，也就是有意义的句子</p>
<h2 id="与-NLP-管道的关系"><a href="#与-NLP-管道的关系" class="headerlink" title="与 NLP 管道的关系"></a>与 NLP 管道的关系</h2><p>在那时我们还没有 transformer 和端到端模型，所以我们需要一步一步的去拆解和分析句子</p>
<p>也就是我们在管道中执行人为设计的算法，帮助计算机理解句子</p>
<h1 id="A2-传统-NLP-管道中的元素和任务"><a href="#A2-传统-NLP-管道中的元素和任务" class="headerlink" title="A2 传统 NLP 管道中的元素和任务"></a>A2 传统 NLP 管道中的元素和任务</h1><p>Elements and tasks in the traditional NLP pipeline (Structure&#x2F;order of the pipeline, tokenization, sentence splitting, morphology, POS tagging, syntactic parsing, NER, coreference resolution, entity linking, WSD, semantic role labeling, semantic parsing)</p>
<h2 id="管道的结构-顺序"><a href="#管道的结构-顺序" class="headerlink" title="管道的结构&#x2F;顺序"></a>管道的结构&#x2F;顺序</h2><p>实际上 PPT 中有一个重要的内容没有提到，那就是管道的第一步：预处理</p>
<p>预处理基本就是删除特殊字符，然后统一大小写，去除停用词 the a an 等</p>
<p>随后遵循以下步骤：</p>
<p>先分词，然后拆句子，标词性（POS），解析句法，找命名实体，合并相同指代，链接实体，消除歧义，标语义角色，最后解析语义</p>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>简而言之就是将句子拆成一个个词，具体怎么拆看需求，同时还可以有规范化的过程</p>
<p>对于如何处理缩写，数字，特殊表达，还有多个词组成的词等，都有不同的处理方式</p>
<p>更进一步的细节看下面专门的章节</p>
<h2 id="句子切分"><a href="#句子切分" class="headerlink" title="句子切分"></a>句子切分</h2><p>有的时候我们的输入是一大段话，所以我们需要将其拆成一个个句子</p>
<p>这个过程也不是那么简单，比如句号后面的 Mr. Mrs. Dr. 等不应该拆开</p>
<h2 id="形态学分析"><a href="#形态学分析" class="headerlink" title="形态学分析"></a>形态学分析</h2><p>其实也就是把词再拆开，比如动词的时态，名词的复数等</p>
<p>我们可以通过这个方式确定一个词的格式是否正确</p>
<p>然后我们就能进行时态标注一类的任务了</p>
<p>stem 通常不完整，而 lemma 是完整的，比如 produc 和 produce</p>
<h2 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h2><p>Part-of-speech tagging，简称 POS，就是给每个词标标记角色</p>
<p>none (名词)，verb (动词)，adj (形容词), adv (副词), pron (代词), prep (介词), conj (连词), interj (感叹词), det (限定词), num (数词), art (冠词), aux (助动词), modal (情态动词), cop (系动词), part (分词), punct (标点符号), sym (符号)</p>
<h2 id="句法解析"><a href="#句法解析" class="headerlink" title="句法解析"></a>句法解析</h2><p>其实也就是通常意义上的语法了，基于 constituent 的解析就是找出句子中谁是 NP(noun phrase)，VP(verb phease)，VT (transitive verb 及物动词)等</p>
<p>更重要的是 dependency parsing，找出句子中的依赖关系，比如主谓宾关系</p>
<p>具体的后面会有专门的章节</p>
<h2 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h2><p>就是找出句子中的专有名词，比如人名，地名，机构名等，还有时间，日期等</p>
<h2 id="指代消解"><a href="#指代消解" class="headerlink" title="指代消解"></a>指代消解</h2><p>就是把指向相同对象的指代词标记出来</p>
<p>比如，我的姐姐，她…，这里的她就是指代消解的对象</p>
<h2 id="实体链接"><a href="#实体链接" class="headerlink" title="实体链接"></a>实体链接</h2><p>就是把命名实体链接到知识库中的实体，比如把北京这个词指向百科中的北京</p>
<h2 id="词义消歧"><a href="#词义消歧" class="headerlink" title="词义消歧"></a>词义消歧</h2><p>就是找出一个词在句子中的具体含义，比如 bank 是银行还是河岸</p>
<h2 id="语义角色标注"><a href="#语义角色标注" class="headerlink" title="语义角色标注"></a>语义角色标注</h2><p>也就是谁做了什么，对谁做了什么，在哪里，什么时候</p>
<p>句法解析主要关注语法，而角色标注关注的是含义</p>
<h2 id="语义解析"><a href="#语义解析" class="headerlink" title="语义解析"></a>语义解析</h2><p>这就是一个合并任务了，把前面的步骤整合起来，找出句子的含义</p>
<h1 id="A3-经典（全词）分词"><a href="#A3-经典（全词）分词" class="headerlink" title="A3 经典（全词）分词"></a>A3 经典（全词）分词</h1><p>Classical (whole-word) tokenization (Tokenization task definition, whitespace splitting, regular expressions, and regex cascades, lexers)</p>
<h2 id="分词任务定义"><a href="#分词任务定义" class="headerlink" title="分词任务定义"></a>分词任务定义</h2><p>前面也说到，把文本切分成合适的小块，这个小块就是 token</p>
<h2 id="空白分割"><a href="#空白分割" class="headerlink" title="空白分割"></a>空白分割</h2><p>这还用说吗，就是按空格分</p>
<p>这种粗暴的方式问题很多，比如中文，或者缩写就分不了</p>
<h2 id="正则表达式和级联"><a href="#正则表达式和级联" class="headerlink" title="正则表达式和级联"></a>正则表达式和级联</h2><p>regex 其实是一个 regular language 的 finite acceptor</p>
<p>这里就需要提到形式语言，正则语言，上下文无关，上下文有关，递归可枚举</p>
<p>级联就是多个正则表达式串联起来，比如先找出数字，再找出字母</p>
<p>在执行替换前，把有问题的部分先替换掉</p>
<h2 id="词法分析器"><a href="#词法分析器" class="headerlink" title="词法分析器"></a>词法分析器</h2><p>也就是用正则表达式来匹配文本，找出 token 的工具</p>
<p>SpaCy 其实就跟 flex 一样</p>
<h1 id="A4-编辑距离和子词分词"><a href="#A4-编辑距离和子词分词" class="headerlink" title="A4 编辑距离和子词分词"></a>A4 编辑距离和子词分词</h1><p>Edit distance and subword tokenization (Edit distance, subword tokenization, Byte Pair Encoding, WordPiece, SentencePiece)</p>
<h2 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h2><p>就跟汉明距离很像，Levenshtein 就是在计算要把一个字符串变成另一个字符串需要多少步，然后每个操作还可以有权重</p>
<h2 id="子词分词"><a href="#子词分词" class="headerlink" title="子词分词"></a>子词分词</h2><p>一种可以无需人工预分词的，基于大数据的办法，文本会分割为最常见的组合</p>
<h2 id="字节对编码"><a href="#字节对编码" class="headerlink" title="字节对编码"></a>字节对编码</h2><p>先把单词拆散成字符，然后根据常出现的组合合并字符，比如 th 是一个常见的组合</p>
<p>贪婪的它只关心当前最常见的组合，然后合并，直到达到预设的词表大小，可能会生成不常见的组合</p>
<p>优化可以用 dropout，也可以使用 unigram</p>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>更智能的 BPE，从最高频的子词开始合并，尝试用更少的子词表示一个单词</p>
<p>但是它会考虑到生成的子词组合的概率，也就是尽可能增大召回率，更注重质量，生成的子词更有意义</p>
<h2 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h2><p>对不需要空格分隔的语言，直接对整段文本进行分词，甚至包含标点符号</p>
<p>它将文本视为连续的序列，然后利用类似于 BPE 等的方法来构建词表</p>
<h1 id="A5-一般语言建模"><a href="#A5-一般语言建模" class="headerlink" title="A5 一般语言建模"></a>A5 一般语言建模</h1><p>Language modeling in general (Language model, continuation probabilities, role of start and end symbols, text generation, LM evaluation)</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>定义就是有一个 L，还有一堆 w 属于 L，然后 sum(P(w)) &#x3D; 1</p>
<h2 id="连续概率"><a href="#连续概率" class="headerlink" title="连续概率"></a>连续概率</h2><p>P(Wn | W1, …, Wn-1)，我下一个词的概率只跟我前面的词有关</p>
<h2 id="起始和结束符号的作用"><a href="#起始和结束符号的作用" class="headerlink" title="起始和结束符号的作用"></a>起始和结束符号的作用</h2><p>因为链式法则需要一个确定的开始和结束符号才能计算概率</p>
<h2 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h2><p>使用概率模型来生成文本，可以有贪婪搜索，也可以是 beam 搜索，去找一个置信度最高的组合</p>
<h2 id="语言模型评估"><a href="#语言模型评估" class="headerlink" title="语言模型评估"></a>语言模型评估</h2><p>外部评估也就是看拼写，语法之类的</p>
<p>而内部评估是把每个词的概率相乘，然后变换来看整体置信度，或者 Perplexity</p>
<h1 id="A6-基于-N-gram-的语言建模"><a href="#A6-基于-N-gram-的语言建模" class="headerlink" title="A6 基于 N-gram 的语言建模"></a>A6 基于 N-gram 的语言建模</h1><p>N-gram-based language modeling (Estimating sequence and word probabilities, N-gram models, markov models, Smoothing)</p>
<h2 id="序列和词概率估计"><a href="#序列和词概率估计" class="headerlink" title="序列和词概率估计"></a>序列和词概率估计</h2><p>我们想估计某个单词在语料库中出现的概率，我们可以直接计数</p>
<p>但是对于句子（序列），我们不光要考虑词的独立概率，还要考虑每个单词跟前面的词一起出现的概率</p>
<p>也就是上面说的连续概率，但是有可能我们的语料库中没有这个序列，直接用的话会导致数据 sparse</p>
<p>那为了解决 0 概率问题，我们可以采取 n-gram 模型</p>
<h2 id="N-gram-模型"><a href="#N-gram-模型" class="headerlink" title="N-gram 模型"></a>N-gram 模型</h2><p>其实就是把连续改成离散，比如 bigram 就是只考虑前一个词，trigram 就是考虑前两个词</p>
<h2 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h2><p>就是一个有限状态自动机，二元以上才能马尔可夫</p>
<h2 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h2><p>但是就算 N-gram 也会有数据稀疏的问题</p>
<p>所以我们给每个词的计数都加一，但是加完了以后可能会导致一个情况</p>
<p>如果说 W1, W2 的计数是 0，那么 P(W1, W2) &#x3D; 0</p>
<p>W1, W3 的计数是 0，那么 P(W1, W3) &#x3D; 0</p>
<p>但是 W2 比 W3 常见，那应该有 P(W1, W2) &gt; P(W1, W3)</p>
<p>这就和我们之前得到的结果不符合，所以我们需要插值</p>
<p>比如在二元模型下，把一元模型的频率加进去</p>
<h1 id="A7-使用经典方法进行文本分类"><a href="#A7-使用经典方法进行文本分类" class="headerlink" title="A7 使用经典方法进行文本分类"></a>A7 使用经典方法进行文本分类</h1><p>Text classification with classical methods (Classification tasks, bag of words, TF-IDF, naive Bayes, discriminative methods)</p>
<h2 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h2><p>跟图像分类差不多，判断是不是垃圾邮件之类的</p>
<h2 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h2><p>就是把文本中的词拿出来记录词频，然后用这个向量来表示文本</p>
<p>通常可以省略 stopword，也可以用 TF-IDF 来加权</p>
<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>也就是 (全部文档 &#x2F; 包含 w 的文档)，然后再取对数</p>
<p>本质上是假设一个词出现的频率越低，它的重要性越高</p>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>NB 的假设和 unigram 一样，就是每个词都是独立的</p>
<p>那就很适合 BOW，因为它不关心词的顺序，纯粹看词频</p>
<p>我们本质上在推测文本属于不同 c 的概率，然后取最大的那个</p>
<p>也就是 P(x, c)，然后由于我们是连续概率，所以会有个累乘</p>
<h2 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h2><p>Logistic Regression，Random Forest 和 Gradient Boosting，它们都是用于分类任务的算法</p>
<h1 id="A8-使用经典方法进行序列标注"><a href="#A8-使用经典方法进行序列标注" class="headerlink" title="A8 使用经典方法进行序列标注"></a>A8 使用经典方法进行序列标注</h1><p>Sequence tagging with classical methods (Sequence tagging tasks, IOB tagging, supervised methods, HMM, Viterbi algorithm, MEMM, CRF, optimization and inference, generative and discriminative models)</p>
<h2 id="序列标注任务"><a href="#序列标注任务" class="headerlink" title="序列标注任务"></a>序列标注任务</h2><p>就是给句子中的每个词打标签，比如词性标注，命名实体识别等</p>
<h2 id="IOB-标注"><a href="#IOB-标注" class="headerlink" title="IOB 标注"></a>IOB 标注</h2><p>名词短语不是有多个词么，整个标记为 NP，然后用 IOB 把这个 NP 拆开</p>
<p>某个实体的开头标 B，里面的标 I，不属于任何实体的标 O</p>
<p>然后我们就能明确的区分实体边界，处理连续或者重叠的实体时就不会混乱</p>
<h2 id="监督方法"><a href="#监督方法" class="headerlink" title="监督方法"></a>监督方法</h2><p>无非就是生成模型（P(X,Y) 联合）和判别模型（P(Y|X) 条件）</p>
<h2 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h2><p>本质上就是有一个不可见状态 Y 集，那可见状态 X 就是每个 token，不可见状态就是词性</p>
<p>Y 指向 X，一一对应</p>
<p>Y 是连续的，然后我们就可以预测下一个词的词性</p>
<p>它还假设生成（发射）概率跟位置无关，随后通过最大似然估计来学习概率</p>
<p>转移概率就是词性转移的概率，发射概率在已知词性的情况下，生成某个词的概率</p>
<h2 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h2><p>HMM 通过 Viterbi 来对新的输入进行标注</p>
<p>它的目标是在给定观测序列 O 的情况下，找到最可能的隐藏序列</p>
<p>Q max &#x3D; argmax P(Q|O) Q 是隐藏序列</p>
<p>实际就是一个动态规划，每一步都留存了上一步的距离</p>
<p>在一个节点同时接到多个来源的时候，只留最小（大）的那个</p>
<p>然后我们可以通过反向引用来恢复路径</p>
<h2 id="最大熵马尔可夫模型"><a href="#最大熵马尔可夫模型" class="headerlink" title="最大熵马尔可夫模型"></a>最大熵马尔可夫模型</h2><p>之前那玩意是个生成模型，但是在只需要打标签的时候，生成模型就显得有点多余</p>
<p>那我们就可以让整个 X 指向每一个 Y，一对多</p>
<p>随后 MEMM 就不用转移和发射概率了，而是使用最大熵</p>
<p>给定前一个状态 Y，来观测 X，预计下一个 Y’ 的条件概率为 P(Y’|Y, X)</p>
<p>然后由于我们是连续的 Y，按顺序一个一个判断，所以需要累乘一下</p>
<p>它有 label bias 问题，也就是它会陷入局部最优解</p>
<h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>那么我们就需要 CRF，直接根据整个句子来判断，考虑上下文</p>
<p>变成无向图，然后我们再用一个 potential 特征函数来建模</p>
<p>关键区别是它的归一化因子会保证 P(Y|X) 的合为 1</p>
<h2 id="优化和推理"><a href="#优化和推理" class="headerlink" title="优化和推理"></a>优化和推理</h2><p>比如梯度下降，然后用 Viterbi 来推理</p>
<h2 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h2><h1 id="A9-依存句法解析"><a href="#A9-依存句法解析" class="headerlink" title="A9 依存句法解析"></a>A9 依存句法解析</h1><p>Dependency parsing (Dependency grammar, projectivity, transition-based parser, graph-based parsers, non-projective parsing)</p>
<h2 id="依存语法"><a href="#依存语法" class="headerlink" title="依存语法"></a>依存语法</h2><p>前面提到过，找出谁依赖谁，谁支配谁，并且修饰词可以省略</p>
<p>依存语法对句子有个类似于 AST 一样的约束，也就是有一个根</p>
<h2 id="投射性"><a href="#投射性" class="headerlink" title="投射性"></a>投射性</h2><p>但是很显然人类的语言不可能随时满足 AST，所以我们需要判断 projectivity</p>
<p>比如，我喜欢吃苹果，有 我 -&gt; 喜欢 &lt;- 吃 &lt;- 苹果 的线性关系，这就是投射的</p>
<p>但是：苹果，我喜欢吃。这句话就会导致 苹果 -&gt; 吃 与 ROOT -&gt; 喜欢 的关系线交叉，这就是非投射的</p>
<p>注意，一定有一个 ROOT 节点去指向句子的动词</p>
<h2 id="基于转换的解析器"><a href="#基于转换的解析器" class="headerlink" title="基于转换的解析器"></a>基于转换的解析器</h2><table>
<thead>
<tr>
<th>操作</th>
<th>Stack</th>
<th>Buffer</th>
<th>Arcs</th>
</tr>
</thead>
<tbody><tr>
<td>INIT</td>
<td>[ROOT]</td>
<td>[I, like, apples]</td>
<td>[]</td>
</tr>
<tr>
<td>SHIFT</td>
<td>[ROOT, I]</td>
<td>[like, apples]</td>
<td>[]</td>
</tr>
<tr>
<td>SHIFT</td>
<td>[ROOT, I, like]</td>
<td>[apples]</td>
<td>[]</td>
</tr>
<tr>
<td>LEFT-ARC</td>
<td>[ROOT, like]</td>
<td>[apples]</td>
<td>[(like, I)]</td>
</tr>
<tr>
<td>SHIFT</td>
<td>[ROOT, like, apples]</td>
<td>[]</td>
<td>[(like, I)]</td>
</tr>
<tr>
<td>RIGHT-ARC</td>
<td>[ROOT, like]</td>
<td>[]</td>
<td>[(like, I), (like, apples)]</td>
</tr>
<tr>
<td>RIGHT-ARC</td>
<td>[ROOT]</td>
<td>[]</td>
<td>[(ROOT, like), (like, I), (like, apples)]</td>
</tr>
</tbody></table>
<h2 id="非投射性解析"><a href="#非投射性解析" class="headerlink" title="非投射性解析"></a>非投射性解析</h2><p>转换解析器只能生成投射树，这肯定是不够高的</p>
<p>那我们可以使用 pseudo-projective parsing，它将非投射关系进行变形</p>
<p>比如重新标注，插入额外标记等，等解析完成后，再将预处理的部分还原</p>
<h2 id="基于图的解析器"><a href="#基于图的解析器" class="headerlink" title="基于图的解析器"></a>基于图的解析器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    ROOT --&gt; I</span><br><span class="line">    ROOT --&gt; like</span><br><span class="line">    ROOT --&gt; apples</span><br><span class="line">    like &lt;--&gt; I</span><br><span class="line">    like &lt;--&gt; apples</span><br><span class="line">    I &lt;--&gt; apples</span><br></pre></td></tr></table></figure>

<p>使用 ChuLiu 等评分算法对每一条边打分</p>
<p>然后通过 maximum spanning tree 来找出最可能的依存关系</p>
<h1 id="A10-基于词汇资源和潜在语义分析的词汇语义学"><a href="#A10-基于词汇资源和潜在语义分析的词汇语义学" class="headerlink" title="A10 基于词汇资源和潜在语义分析的词汇语义学"></a>A10 基于词汇资源和潜在语义分析的词汇语义学</h1><p>Lexical semantics based on lexical resources and LSA (Word senses and dictionaries, lexical relations, word vectors, latent semantic analysis)</p>
<h2 id="词义和词典"><a href="#词义和词典" class="headerlink" title="词义和词典"></a>词义和词典</h2><p>从词典中找出一个词的含义，查字典哥们</p>
<h2 id="词汇关系"><a href="#词汇关系" class="headerlink" title="词汇关系"></a>词汇关系</h2><p>synonymy 和 antonimy 同义词反义词</p>
<p>还有动物（上）和老鼠（下）的关系，hypo-hypernymy</p>
<p>还有 finger 和 hand 的 meronymy</p>
<p>可以使用 WordNet 来查找这些关系并进行消歧</p>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>WN 是手动标注的，这不好，我们可以从语料库中学习词向量</p>
<p>也就是根据词的上下文，和相似分布，来猜测词的含义</p>
<p>使用共现矩阵</p>
<table>
<thead>
<tr>
<th>词-文档</th>
<th>文档 1</th>
<th>文档 2</th>
</tr>
</thead>
<tbody><tr>
<td>词 1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>词 2</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>是统计词在文档中出现的次数</p>
<p>和</p>
<table>
<thead>
<tr>
<th>词-词</th>
<th>词 1</th>
<th>词 2</th>
</tr>
</thead>
<tbody><tr>
<td>词 1</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>词 2</td>
<td>1</td>
<td></td>
</tr>
</tbody></table>
<p>是统计词在同一个文档中一起出现的次数</p>
<h2 id="潜在语义分析"><a href="#潜在语义分析" class="headerlink" title="潜在语义分析"></a>潜在语义分析</h2><p>然后我们使用 SVD 来降维，找出词的潜在语义 LSA</p>
<p>C &#x3D; USV，U 和 V 是 orthonormal，S 是 diagonal</p>
<p>U：词矩阵，V：文档矩阵，S：不同维度的重要性</p>
<h1 id="A11-Word2vec-和-GloVe"><a href="#A11-Word2vec-和-GloVe" class="headerlink" title="A11 Word2vec 和 GloVe"></a>A11 Word2vec 和 GloVe</h1><p>Word2vec and GloVe (CBOW and Skipgram tasks, neural embeddings, training architectures, negative sampling, GloVe algorithm)</p>
<h2 id="CBOW-和-Skipgram-任务"><a href="#CBOW-和-Skipgram-任务" class="headerlink" title="CBOW 和 Skipgram 任务"></a>CBOW 和 Skipgram 任务</h2><p>首先，Word2vec 只能生成静态词向量，也就是说即使一个词有不同的含义，它的词向量仍然是一样的</p>
<p>所以不好处理多义词，比如 bank</p>
<p>Skipgram 通过中心词来预测它周围的词，它是 n-gram 的扩展，但可以跳过一些非中心词</p>
<p>反过来，Continuous BOW 通过上下文来预测中心词，输入是 One-hot 编码的上下文</p>
<h2 id="神经嵌入"><a href="#神经嵌入" class="headerlink" title="神经嵌入"></a>神经嵌入</h2><p>嵌入指的是把高维的数据映射到低维的空间，比如把词映射到一个向量</p>
<p>而神经嵌入就是通过神经网络来学习这个映射</p>
<h2 id="训练架构"><a href="#训练架构" class="headerlink" title="训练架构"></a>训练架构</h2><p>对于 Skipgram，假设我们有单词序列 W，那么给定中心词 Wt，模型最大化</p>
<p>P(W(t-i), W(t-i+1), …, W(t+i) | Wt), i 是窗口大小</p>
<p>那其实也就是当有 Wt 的时候，把窗口大小内的词的概率最大化</p>
<p>对于 CBOW，给定上下文 W(t-i), W(t-i+1), …, W(t+i)，模型最大化</p>
<p>P(Wt | W(t-i), W(t-i+1), …, W(t+i))</p>
<p>当有上下文的时候，最大化中心词的概率</p>
<p>传统的 softmax 需要计算词汇表中所有单词的概率，这不好</p>
<p>我们将词汇表变成一颗 Huffman 树，然后使用 hierarchical softmax 来计算</p>
<p>简单来说，我们将单词预测变成二叉决策</p>
<h2 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h2><p>实际上，语料库中的大部分词都是不应该和目标词一起出现的，与其把所有词都计算一遍，不如只计算一部分</p>
<p>这种随机抽取一些不应该一起出现的词来配对的方法就是负采样，随后每次模型只会更新一小部分参数</p>
<p>而正样本，就是上下文中出现的词</p>
<h2 id="GloVe-算法"><a href="#GloVe-算法" class="headerlink" title="GloVe 算法"></a>GloVe 算法</h2><p>Word2Vec 主要基于局部上下文窗口进行训练，而 GloVe 则是基于全局的共现矩阵</p>
<p>它更好的保留了词之间的语义关系</p>
<h1 id="A12-评估词嵌入和基于内部词结构的嵌入"><a href="#A12-评估词嵌入和基于内部词结构的嵌入" class="headerlink" title="A12 评估词嵌入和基于内部词结构的嵌入"></a>A12 评估词嵌入和基于内部词结构的嵌入</h1><p>Evaluating word embeddings and embeddings based on internal word structure (Intrinsic evaluations, extrinsic evaluations, subword [fastText] embeddings)</p>
<h2 id="内在评估"><a href="#内在评估" class="headerlink" title="内在评估"></a>内在评估</h2><p>我们可以通过词类比 Analogy，词相似度，词类别等</p>
<p>对于词类比，我们可以测试类似于</p>
<p>Wking - Wman + Wwoman</p>
<p>&#x3D; Wqueen 的概率，概率越大，表现越好</p>
<p>而对于词相似度，我们可以比作看词联想游戏</p>
<p>比如看到猫，可以想到狗，我们使用余弦相似度来衡量</p>
<p>在合理的模型中，sim(猫, 狗) &gt; sim(猫, 汽车)</p>
<h2 id="外在评估"><a href="#外在评估" class="headerlink" title="外在评估"></a>外在评估</h2><p>可以使用 NER 等实际任务来评估词向量的性能</p>
<h2 id="子词-fastText-嵌入"><a href="#子词-fastText-嵌入" class="headerlink" title="子词[fastText]嵌入"></a>子词[fastText]嵌入</h2><p>传统方法对没见过的词无能为力，但是 fastText 可以通过子词来生成词向量，类似于 n-gram</p>
<p>它跟 W2V 很像，只不过用了子词</p>
<h1 id="A13-使用-RNN-进行语言建模和序列处理"><a href="#A13-使用-RNN-进行语言建模和序列处理" class="headerlink" title="A13 使用 RNN 进行语言建模和序列处理"></a>A13 使用 RNN 进行语言建模和序列处理</h1><p>Language modeling and sequence processing with RNNs (Four types of sequence processing, sequence tagging, bidirectional RNNs, sequence encoding, sequence generation, seq2seq tasks, LSTM architecture)</p>
<h2 id="四种类型的序列处理"><a href="#四种类型的序列处理" class="headerlink" title="四种类型的序列处理"></a>四种类型的序列处理</h2><ol>
<li>单输入单输出，如图像分类</li>
<li>单输入多输出，如图像描述</li>
<li>多输入单输出，如情感分析</li>
<li>多输入多输出，如机器翻译</li>
</ol>
<h2 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h2><p>之前就提到过，比如 POS NER 等，可以使用一组 embedding -&gt; LSTM -&gt; softmax 来实现</p>
<h2 id="双向-RNN"><a href="#双向-RNN" class="headerlink" title="双向 RNN"></a>双向 RNN</h2><p>也就是用两个 RNN，一个正向一个反向，然后把结果合并，这样能更好的捕捉上下文</p>
<h2 id="序列编码"><a href="#序列编码" class="headerlink" title="序列编码"></a>序列编码</h2><p>首先我们要知道 Seq2Vec 是把序列编码成一个固定维度的向量</p>
<p>那我们就可以使用 LSTM 的最后一层输出来表示整个序列</p>
<p>双向 RNN 的话就用最大池化</p>
<p>RNN 就是 <code>h_t = tanh(W_hx * x_t + W_hh * h_t-1 + b_h)</code></p>
<p>h_t 输出，W_hx 输入到隐藏权重，x_t 输入</p>
<p>W_hh 隐藏到隐藏权重，h_t-1 前一刻的隐藏状态 ，b_h 隐藏偏置</p>
<h2 id="序列生成"><a href="#序列生成" class="headerlink" title="序列生成"></a>序列生成</h2><p>Seq2Vec 或其他模型，比如ConvNN，可以生成一个向量，然后交给 LSTM 来生成序列</p>
<p>那么 Vec2Seq 公式为 y_t &#x3D; softmax(W_hy * h_t + b_y)</p>
<p>y_t 输出，W_hy 隐藏到输出权重，h_t 隐藏状态，b_y 输出偏置</p>
<h2 id="seq2seq-任务"><a href="#seq2seq-任务" class="headerlink" title="seq2seq 任务"></a>seq2seq 任务</h2><p>把 Seq2Vec 和 Vec2Seq 结合起来，就叫 seq2seq 任务</p>
<p>我们可以使用 教师强制 来训练，它是把真实的输出作为下一时刻的输入</p>
<p>这样可以帮助模型更快的收敛，但是会导致推理时的性能下降</p>
<h2 id="LSTM-架构"><a href="#LSTM-架构" class="headerlink" title="LSTM 架构"></a>LSTM 架构</h2><p>是 RNN 的一种增强，它有三个门，Input Forget Output</p>
<p>它可以更好的处理长期依赖，避免梯度 vanishing 和 exploding</p>
<p>流程如下</p>
<ol>
<li><p>遗忘门决定丢弃多少信息</p>
</li>
<li><p>输入门决定添加多少信息</p>
</li>
<li><p>将遗忘门的输出与记忆单元相乘，然后加上输入门的输出</p>
</li>
<li><p>输出门决定输出多少信息，然后生成新的隐藏状态</p>
</li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP-PreExamA</p><p><a href="https://aloen.to/AI/NLP/NLP-PreExamA/">https://aloen.to/AI/NLP/NLP-PreExamA/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-10-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-12-31</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="link-muted mr-2" rel="tag" href="/tags/%E8%80%83%E8%AF%95/">考试</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI/NLP/NLP-Transformers/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">NLP-Transformers</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/NLP/NLP-Seq2Seq/"><span class="level-item">NLP-Seq2Seq</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">53</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">34</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">30</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">42</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#A1-语言结构和语法"><span class="level-left"><span class="level-item">1</span><span class="level-item">A1 语言结构和语法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#语言结构"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">语言结构</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#表示层次"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">表示层次</span></span></a></li><li><a class="level is-mobile" href="#语法"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">语法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#解析任务"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">解析任务</span></span></a></li><li><a class="level is-mobile" href="#生成任务"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">生成任务</span></span></a></li><li><a class="level is-mobile" href="#与-NLP-管道的关系"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">与 NLP 管道的关系</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A2-传统-NLP-管道中的元素和任务"><span class="level-left"><span class="level-item">2</span><span class="level-item">A2 传统 NLP 管道中的元素和任务</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#管道的结构-顺序"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">管道的结构/顺序</span></span></a></li><li><a class="level is-mobile" href="#分词"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">分词</span></span></a></li><li><a class="level is-mobile" href="#句子切分"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">句子切分</span></span></a></li><li><a class="level is-mobile" href="#形态学分析"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">形态学分析</span></span></a></li><li><a class="level is-mobile" href="#词性标注"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">词性标注</span></span></a></li><li><a class="level is-mobile" href="#句法解析"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">句法解析</span></span></a></li><li><a class="level is-mobile" href="#命名实体识别"><span class="level-left"><span class="level-item">2.7</span><span class="level-item">命名实体识别</span></span></a></li><li><a class="level is-mobile" href="#指代消解"><span class="level-left"><span class="level-item">2.8</span><span class="level-item">指代消解</span></span></a></li><li><a class="level is-mobile" href="#实体链接"><span class="level-left"><span class="level-item">2.9</span><span class="level-item">实体链接</span></span></a></li><li><a class="level is-mobile" href="#词义消歧"><span class="level-left"><span class="level-item">2.10</span><span class="level-item">词义消歧</span></span></a></li><li><a class="level is-mobile" href="#语义角色标注"><span class="level-left"><span class="level-item">2.11</span><span class="level-item">语义角色标注</span></span></a></li><li><a class="level is-mobile" href="#语义解析"><span class="level-left"><span class="level-item">2.12</span><span class="level-item">语义解析</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A3-经典（全词）分词"><span class="level-left"><span class="level-item">3</span><span class="level-item">A3 经典（全词）分词</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#分词任务定义"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">分词任务定义</span></span></a></li><li><a class="level is-mobile" href="#空白分割"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">空白分割</span></span></a></li><li><a class="level is-mobile" href="#正则表达式和级联"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">正则表达式和级联</span></span></a></li><li><a class="level is-mobile" href="#词法分析器"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">词法分析器</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A4-编辑距离和子词分词"><span class="level-left"><span class="level-item">4</span><span class="level-item">A4 编辑距离和子词分词</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#编辑距离"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">编辑距离</span></span></a></li><li><a class="level is-mobile" href="#子词分词"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">子词分词</span></span></a></li><li><a class="level is-mobile" href="#字节对编码"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">字节对编码</span></span></a></li><li><a class="level is-mobile" href="#WordPiece"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">WordPiece</span></span></a></li><li><a class="level is-mobile" href="#SentencePiece"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">SentencePiece</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A5-一般语言建模"><span class="level-left"><span class="level-item">5</span><span class="level-item">A5 一般语言建模</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#语言模型"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">语言模型</span></span></a></li><li><a class="level is-mobile" href="#连续概率"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">连续概率</span></span></a></li><li><a class="level is-mobile" href="#起始和结束符号的作用"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">起始和结束符号的作用</span></span></a></li><li><a class="level is-mobile" href="#文本生成"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">文本生成</span></span></a></li><li><a class="level is-mobile" href="#语言模型评估"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">语言模型评估</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A6-基于-N-gram-的语言建模"><span class="level-left"><span class="level-item">6</span><span class="level-item">A6 基于 N-gram 的语言建模</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#序列和词概率估计"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">序列和词概率估计</span></span></a></li><li><a class="level is-mobile" href="#N-gram-模型"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">N-gram 模型</span></span></a></li><li><a class="level is-mobile" href="#马尔可夫模型"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">马尔可夫模型</span></span></a></li><li><a class="level is-mobile" href="#平滑"><span class="level-left"><span class="level-item">6.4</span><span class="level-item">平滑</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A7-使用经典方法进行文本分类"><span class="level-left"><span class="level-item">7</span><span class="level-item">A7 使用经典方法进行文本分类</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#分类任务"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">分类任务</span></span></a></li><li><a class="level is-mobile" href="#词袋模型"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">词袋模型</span></span></a></li><li><a class="level is-mobile" href="#TF-IDF"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">TF-IDF</span></span></a></li><li><a class="level is-mobile" href="#朴素贝叶斯"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">朴素贝叶斯</span></span></a></li><li><a class="level is-mobile" href="#判别方法"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">判别方法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A8-使用经典方法进行序列标注"><span class="level-left"><span class="level-item">8</span><span class="level-item">A8 使用经典方法进行序列标注</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#序列标注任务"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">序列标注任务</span></span></a></li><li><a class="level is-mobile" href="#IOB-标注"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">IOB 标注</span></span></a></li><li><a class="level is-mobile" href="#监督方法"><span class="level-left"><span class="level-item">8.3</span><span class="level-item">监督方法</span></span></a></li><li><a class="level is-mobile" href="#隐马尔可夫模型"><span class="level-left"><span class="level-item">8.4</span><span class="level-item">隐马尔可夫模型</span></span></a></li><li><a class="level is-mobile" href="#维特比算法"><span class="level-left"><span class="level-item">8.5</span><span class="level-item">维特比算法</span></span></a></li><li><a class="level is-mobile" href="#最大熵马尔可夫模型"><span class="level-left"><span class="level-item">8.6</span><span class="level-item">最大熵马尔可夫模型</span></span></a></li><li><a class="level is-mobile" href="#条件随机场"><span class="level-left"><span class="level-item">8.7</span><span class="level-item">条件随机场</span></span></a></li><li><a class="level is-mobile" href="#优化和推理"><span class="level-left"><span class="level-item">8.8</span><span class="level-item">优化和推理</span></span></a></li><li><a class="level is-mobile" href="#生成模型和判别模型"><span class="level-left"><span class="level-item">8.9</span><span class="level-item">生成模型和判别模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A9-依存句法解析"><span class="level-left"><span class="level-item">9</span><span class="level-item">A9 依存句法解析</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#依存语法"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">依存语法</span></span></a></li><li><a class="level is-mobile" href="#投射性"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">投射性</span></span></a></li><li><a class="level is-mobile" href="#基于转换的解析器"><span class="level-left"><span class="level-item">9.3</span><span class="level-item">基于转换的解析器</span></span></a></li><li><a class="level is-mobile" href="#非投射性解析"><span class="level-left"><span class="level-item">9.4</span><span class="level-item">非投射性解析</span></span></a></li><li><a class="level is-mobile" href="#基于图的解析器"><span class="level-left"><span class="level-item">9.5</span><span class="level-item">基于图的解析器</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A10-基于词汇资源和潜在语义分析的词汇语义学"><span class="level-left"><span class="level-item">10</span><span class="level-item">A10 基于词汇资源和潜在语义分析的词汇语义学</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#词义和词典"><span class="level-left"><span class="level-item">10.1</span><span class="level-item">词义和词典</span></span></a></li><li><a class="level is-mobile" href="#词汇关系"><span class="level-left"><span class="level-item">10.2</span><span class="level-item">词汇关系</span></span></a></li><li><a class="level is-mobile" href="#词向量"><span class="level-left"><span class="level-item">10.3</span><span class="level-item">词向量</span></span></a></li><li><a class="level is-mobile" href="#潜在语义分析"><span class="level-left"><span class="level-item">10.4</span><span class="level-item">潜在语义分析</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A11-Word2vec-和-GloVe"><span class="level-left"><span class="level-item">11</span><span class="level-item">A11 Word2vec 和 GloVe</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#CBOW-和-Skipgram-任务"><span class="level-left"><span class="level-item">11.1</span><span class="level-item">CBOW 和 Skipgram 任务</span></span></a></li><li><a class="level is-mobile" href="#神经嵌入"><span class="level-left"><span class="level-item">11.2</span><span class="level-item">神经嵌入</span></span></a></li><li><a class="level is-mobile" href="#训练架构"><span class="level-left"><span class="level-item">11.3</span><span class="level-item">训练架构</span></span></a></li><li><a class="level is-mobile" href="#负采样"><span class="level-left"><span class="level-item">11.4</span><span class="level-item">负采样</span></span></a></li><li><a class="level is-mobile" href="#GloVe-算法"><span class="level-left"><span class="level-item">11.5</span><span class="level-item">GloVe 算法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A12-评估词嵌入和基于内部词结构的嵌入"><span class="level-left"><span class="level-item">12</span><span class="level-item">A12 评估词嵌入和基于内部词结构的嵌入</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#内在评估"><span class="level-left"><span class="level-item">12.1</span><span class="level-item">内在评估</span></span></a></li><li><a class="level is-mobile" href="#外在评估"><span class="level-left"><span class="level-item">12.2</span><span class="level-item">外在评估</span></span></a></li><li><a class="level is-mobile" href="#子词-fastText-嵌入"><span class="level-left"><span class="level-item">12.3</span><span class="level-item">子词[fastText]嵌入</span></span></a></li></ul></li><li><a class="level is-mobile" href="#A13-使用-RNN-进行语言建模和序列处理"><span class="level-left"><span class="level-item">13</span><span class="level-item">A13 使用 RNN 进行语言建模和序列处理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#四种类型的序列处理"><span class="level-left"><span class="level-item">13.1</span><span class="level-item">四种类型的序列处理</span></span></a></li><li><a class="level is-mobile" href="#序列标注"><span class="level-left"><span class="level-item">13.2</span><span class="level-item">序列标注</span></span></a></li><li><a class="level is-mobile" href="#双向-RNN"><span class="level-left"><span class="level-item">13.3</span><span class="level-item">双向 RNN</span></span></a></li><li><a class="level is-mobile" href="#序列编码"><span class="level-left"><span class="level-item">13.4</span><span class="level-item">序列编码</span></span></a></li><li><a class="level is-mobile" href="#序列生成"><span class="level-left"><span class="level-item">13.5</span><span class="level-item">序列生成</span></span></a></li><li><a class="level is-mobile" href="#seq2seq-任务"><span class="level-left"><span class="level-item">13.6</span><span class="level-item">seq2seq 任务</span></span></a></li><li><a class="level is-mobile" href="#LSTM-架构"><span class="level-left"><span class="level-item">13.7</span><span class="level-item">LSTM 架构</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-29T07:30:00.000Z">2024-12-29</time></p><p class="title"><a href="/Algorithm/%E8%BF%B7%E5%AE%AB%E5%AF%BB%E8%B7%AF/">迷宫寻路</a></p><p class="categories"><a href="/categories/Algorithm/">Algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-15T08:01:40.000Z">2024-12-15</time></p><p class="title"><a href="/AI/NLP/NLP-PreExamC/">NLP-PreExamC</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-14T08:01:02.000Z">2024-12-14</time></p><p class="title"><a href="/AI/NLP/NLP-DeepSTT/">NLP-DeepSTT</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-13T08:00:44.000Z">2024-12-13</time></p><p class="title"><a href="/AI/NLP/NLP-STT/">NLP-STT</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-12-12T08:00:25.000Z">2024-12-12</time></p><p class="title"><a href="/AI/NLP/NLP-VisionActionModels/">NLP-VisionActionModels</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/NLP/">NLP</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2024 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>