<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>NLP-TextImageModels - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="条件文本到图像模型"><meta property="og:type" content="blog"><meta property="og:title" content="NLP-TextImageModels"><meta property="og:url" content="https://aloen.to/AI/NLP/NLP-TextImageModels/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="条件文本到图像模型"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_output.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_read.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/gan.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_gan.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_gan_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_style_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_locality.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_wordlevel.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_vqvae.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_attn_mask.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_generated.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/diff_transform.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/diffprocess.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_chain.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_train.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_uncond.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_interpol.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_interpol_result.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_chain.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_accel.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/glide_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle2_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/stable_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/stable_vqgan.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_zero_shot_result.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_bsr_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_synthesis_result.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_layout_result.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dit.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/dalle3_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/imagen_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/parti_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/wurstchen_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/wurstchen_training.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/inpaint_example.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/textual_inversion.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/textual_inversion_arch.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/lora_fusion.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/lcm_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/lcm_lora_results.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/controlnet.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/t2i_adapter.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-TextImageModels/controlnet_results.png"><meta property="article:published_time" content="2024-12-11T07:59:59.000Z"><meta property="article:modified_time" content="2025-03-19T23:47:24.226Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="笔记"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_output.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/NLP/NLP-TextImageModels/"},"headline":"NLP-TextImageModels","image":["https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_output.png","https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/aligndraw_read.png","https://aloen.to/AI/NLP/NLP-TextImageModels/gan.png","https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_gan.png","https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_gan_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/textconditional_style_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_locality.png","https://aloen.to/AI/NLP/NLP-TextImageModels/controlgan_wordlevel.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_vqvae.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_attn_mask.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle_generated.png","https://aloen.to/AI/NLP/NLP-TextImageModels/diff_transform.png","https://aloen.to/AI/NLP/NLP-TextImageModels/diffprocess.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_chain.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_train.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_uncond.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_interpol.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddpm_interpol_result.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_chain.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_accel.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ddim_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/glide_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle2_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/stable_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/stable_vqgan.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_zero_shot_result.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_bsr_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_synthesis_result.png","https://aloen.to/AI/NLP/NLP-TextImageModels/ldm_layout_result.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dit.png","https://aloen.to/AI/NLP/NLP-TextImageModels/dalle3_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/imagen_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/parti_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/wurstchen_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/wurstchen_training.png","https://aloen.to/AI/NLP/NLP-TextImageModels/inpaint_example.png","https://aloen.to/AI/NLP/NLP-TextImageModels/textual_inversion.png","https://aloen.to/AI/NLP/NLP-TextImageModels/textual_inversion_arch.png","https://aloen.to/AI/NLP/NLP-TextImageModels/lora_fusion.png","https://aloen.to/AI/NLP/NLP-TextImageModels/lcm_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/lcm_lora_results.png","https://aloen.to/AI/NLP/NLP-TextImageModels/controlnet.png","https://aloen.to/AI/NLP/NLP-TextImageModels/t2i_adapter.png","https://aloen.to/AI/NLP/NLP-TextImageModels/controlnet_results.png"],"datePublished":"2024-12-11T07:59:59.000Z","dateModified":"2025-03-19T23:47:24.226Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/NLP/NLP-TextImageModels/"}},"description":"条件文本到图像模型"}</script><link rel="canonical" href="https://aloen.to/AI/NLP/NLP-TextImageModels/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-12-11T07:59:59.000Z" title="12/11/2024, 7:59:59 AM">2024-12-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-03-19T23:47:24.226Z" title="3/19/2025, 11:47:24 PM">2025-03-20</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/NLP/">NLP</a></span><span class="level-item">an hour read (About 8807 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP-TextImageModels</h1><div class="content"><p>条件文本到图像模型</p>
<span id="more"></span>

<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="条件文本到图像生成"><a href="#条件文本到图像生成" class="headerlink" title="条件文本到图像生成"></a>条件文本到图像生成</h2><p>条件文本到图像生成是一项任务，其中模型被训练为根据给定的文本描述生成图像。<br>该任务可以通过以下方法之一来制定：</p>
<ul>
<li>序列到序列 (seq2seq) 生成（与自回归 autoregression 的联系）</li>
<li>生成对抗网络 (GAN) 方法</li>
<li>自回归方法</li>
<li>扩散方法</li>
</ul>
<h2 id="序列到序列生成"><a href="#序列到序列生成" class="headerlink" title="序列到序列生成"></a>序列到序列生成</h2><p>2015年，DeepMind发布了DRAW，这是一个顺序的、无条件的图像生成模型。此类模型用于生成MNIST数据集等图像。</p>
<p>同年晚些时候，AlignDRAW发布了，这是DRAW的条件版本。它能够根据给定的文本描述生成图像。</p>
<h2 id="AlignDRAW"><a href="#AlignDRAW" class="headerlink" title="AlignDRAW"></a>AlignDRAW</h2><p>AlignDRAW首先通过双向LSTM (BiLSTM)编码器将文本描述编码为潜在向量。然后解码器是一组“绘图”操作，尝试根据潜在向量生成图像。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/aligndraw_output.png" alt="AlignDRAW早期结果"></p>
<p><img src="/AI/NLP/NLP-TextImageModels/aligndraw_arch.png" alt="AlignDRAW架构"></p>
<p>训练是变分下界 (ELBO) 优化（最大化）。</p>
<p>给定$x$，图像，$y$，文本描述，以及$Z$，一系列潜在向量，损失为：</p>
<p>$\mathcal{L} &#x3D; \sum\limits_{Z}Q(Z|x,y)logP(x|y,Z)-D_{KL}(Q(Z|x,y)||P(Z|y))$</p>
<p>这里$Q(Z|x,y)$是近似后验分布。$P(x|y,Z)$是给定文本描述和潜在向量的图像的似然。$P(Z|y)$是给定文本描述的潜在向量的先验分布。</p>
<h3 id="神经分布建模"><a href="#神经分布建模" class="headerlink" title="神经分布建模"></a>神经分布建模</h3><p>$Q(Z_t|x,y,Z_{1:t-1})$ 和 $P(Z_t|Z_{1:t-1})$ 由高斯分布建模，分别基于推理和生成网络参数化。潜在变量以顺序方式相互依赖。</p>
<p>$P(Z_t|Z_{1:t-1}) &#x3D; \mathcal{N}\left(\mu(h_{t-1}^{gen}),\sigma(h_{t-1}^{gen})\right)$</p>
<p>$Q(Z_t|x,y,Z_{1:t-1}) &#x3D; \mathcal{N}\left(\mu(h_t^{infer}),\sigma(h_t^{infer})\right)$</p>
<p>$\mu(h) &#x3D; tanh(W_{\mu}h)$</p>
<p>$\sigma(h) &#x3D; exp\left(tanh(W_{\sigma}h)\right)$</p>
<h3 id="读取和写入操作"><a href="#读取和写入操作" class="headerlink" title="读取和写入操作"></a>读取和写入操作</h3><p>读取和写入操作由不同大小、步幅和位置的固定高斯（模糊）滤波器组参数化。写入操作是滤波器组 ($F_x$, $F_y$) 和生成的 ($K$) 补丁的组合。</p>
<p>$write(h_t^{gen}) &#x3D; F_x(h_t^{gen})K(h_t^{gen})F_y(h_t^{gen})^T$</p>
<p>而读取操作是该操作的逆操作。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/aligndraw_read.png" alt="输入图像、补丁、结果和读取操作的高斯滤波器示例"></p>
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><h2 id="生成对抗网络架构"><a href="#生成对抗网络架构" class="headerlink" title="生成对抗网络架构"></a>生成对抗网络架构</h2><p><img src="/AI/NLP/NLP-TextImageModels/gan.png" alt="GAN架构"></p>
<h2 id="生成对抗网络回顾"><a href="#生成对抗网络回顾" class="headerlink" title="生成对抗网络回顾"></a>生成对抗网络回顾</h2><p>GAN是一种生成模型，由两个网络组成：生成器和判别器。生成器尝试生成与真实数据集相似的数据，而判别器尝试区分真实数据和生成的数据点。</p>
<p>对抗游戏的目标如下：</p>
<div>
$
min_G max_D V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\left[log(D(x))\right] + \mathbb{E}_{z\sim p(z)}\left[log(1-D(G(z)))\right]
$
</div>

<p>因此，生成器尝试最大化 $log(D(G(z)))$，而判别器尝试最大化 $log(D(x)) + log(1-D(G(z)))$。<br>这里 $z$ 是从随机先验分布 $p(z)$ 中采样的，$x$ 是从真实数据分布 $p_{data}(x)$ 中采样的。</p>
<h2 id="文本条件GAN架构"><a href="#文本条件GAN架构" class="headerlink" title="文本条件GAN架构"></a>文本条件GAN架构</h2><p>提出了一种文本条件GAN架构，该架构将文本编码作为潜在向量的一部分。该架构是一个反卷积-卷积网络。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/textconditional_gan.png" alt="文本条件GAN架构"></p>
<h2 id="文本条件GAN训练"><a href="#文本条件GAN训练" class="headerlink" title="文本条件GAN训练"></a>文本条件GAN训练</h2><p>朴素GAN判别器任务：区分真实和虚假图像。</p>
<p>通过文本条件，我们获得了额外的任务：</p>
<ul>
<li>拒绝任何文本描述的虚假图像。</li>
<li>拒绝不匹配的文本-图像对。</li>
</ul>
<p>后者通过在每个训练批次中提供一组不匹配的示例来实现。</p>
<div>
$
\mathcal{L}_{CLS} = log(D(I_{real}, T_{real})) + 0.5 log(1-D(I_{real}, T_{fake})) + 0.5 log(1-D(I_{fake}, T_{real}))
$
</div>

<h2 id="文本嵌入插值"><a href="#文本嵌入插值" class="headerlink" title="文本嵌入插值"></a>文本嵌入插值</h2><p>由于当时文本-图像数据集稀疏，作者使用了一种文本嵌入插值方法来为未见过的文本描述生成更多的训练示例。这样，输入的文本嵌入是两个文本嵌入的线性组合。一个是正确的文本嵌入，另一个是数据集中的随机文本嵌入。然后将其传递给生成器。即使判别器没有给定对的真实示例，它仍然可以学习何时拒绝生成的图像。这增强了生成器。<br>作者建议对于 $t_1$ 和 $t_2$ 文本嵌入使用 $\beta&#x3D;0.5$：</p>
<p>$D(G(z, t_1)) \rightarrow D(G(z, \beta t_1 + (1-\beta)t_2))$</p>
<h2 id="文本条件GAN结果"><a href="#文本条件GAN结果" class="headerlink" title="文本条件GAN结果"></a>文本条件GAN结果</h2><p>文本输入：灰色的鸟有一个浅灰色的头和灰色的蹼足</p>
<p><img src="/AI/NLP/NLP-TextImageModels/textconditional_gan_results.png" alt="文本条件GAN结果"></p>
<h2 id="风格编码"><a href="#风格编码" class="headerlink" title="风格编码"></a>风格编码</h2><p>怀疑潜在变量应该包含文本中缺失但可以从图像（或一组图像）中推断的信息。他们称之为图像的“风格”，并提出了一种将此信息编码到潜在变量中的方法。</p>
<p>即他们提出了一种反转生成器的方法。$S(x)$ 通过以下目标进行训练，其中 $x$ 是图像：<br>$\mathcal{L}_{style}||z - S(G(z,t_1))||_2^2$</p>
<p>风格迁移推理过程如下：</p>
<p>$s \leftarrow S(x), \hat{x} \leftarrow G(s, t_1)$</p>
<h2 id="风格编码结果"><a href="#风格编码结果" class="headerlink" title="风格编码结果"></a>风格编码结果</h2><p><img src="/AI/NLP/NLP-TextImageModels/textconditional_style_results.png" alt="风格编码结果"></p>
<h2 id="ControlGAN"><a href="#ControlGAN" class="headerlink" title="ControlGAN"></a>ControlGAN</h2><p>ControlGAN 提供了一种细粒度的多阶段生成过程，并通过额外的监督方法来实现更好的结果。</p>
<p>他们添加了：</p>
<ul>
<li>词和图像特征级别的判别器</li>
<li>多阶段生成和判别</li>
<li>无条件和条件损失、感知损失、词&#x2F;图像特征级别的相关损失、文本到图像的余弦损失</li>
<li>定制的类似注意力机制</li>
</ul>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/AI/NLP/NLP-TextImageModels/controlgan_arch.png" alt="ControlGAN架构"></p>
<h3 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h3><p><img src="/AI/NLP/NLP-TextImageModels/controlgan_locality.png" alt="注意力引导的局部特征生成"></p>
<h2 id="感知损失"><a href="#感知损失" class="headerlink" title="感知损失"></a>感知损失</h2><p>感知损失用于生成一致的图像，包括未直接由文本引导的部分。这是通过使用预训练网络（在 ImageNet 上进行分类训练）来完成的。该网络用于从生成的图像和真实图像中提取特征。损失是特征之间的 L2 距离。特征从网络的中间层提取，以获得抽象但不太低级的表示。</p>
<p>$\mathcal{L}_{perceptual}^i(I, I’) &#x3D; \frac{1}{C_iH_iW_i} ||\phi_i(I) - \phi_i(I’)||_2^2$</p>
<p>其中 $\phi_i$ 是特征提取器，$C_i$、$H_i$ 和 $W_i$ 分别是网络第 $i$ 层的通道数、高度和宽度。</p>
<h2 id="词级特征"><a href="#词级特征" class="headerlink" title="词级特征"></a>词级特征</h2><p><img src="/AI/NLP/NLP-TextImageModels/controlgan_wordlevel.png" alt="词级（类似注意力）判别器。提供词和图像特征级别的梯度对于局部生成器训练很重要。"></p>
<h1 id="自回归方法"><a href="#自回归方法" class="headerlink" title="自回归方法"></a>自回归方法</h1><p>如果潜在先验包括文本条件，变分自编码器 (VAE) 也可用于根据文本描述创建图像。包括文本条件的一种方法是使用自回归先验模型。</p>
<p>这种自回归先验类似于语言建模中使用的自回归模型。不同之处在于词汇不仅是文本标记，还有使用与 VAE 相同表示的潜在图像特征。</p>
<p>图像编码和解码通常由 VAE 处理，而文本编码是自回归模型的一部分。</p>
<h2 id="DALL-E-架构"><a href="#DALL-E-架构" class="headerlink" title="DALL-E 架构"></a>DALL-E 架构</h2><p><img src="/AI/NLP/NLP-TextImageModels/dalle_arch.png" alt="DALL-E 架构"></p>
<h2 id="DALL-E-训练"><a href="#DALL-E-训练" class="headerlink" title="DALL-E 训练"></a>DALL-E 训练</h2><p>第一个 DALL-E 模型是基于变压器的自回归模型，使用 dVAE 进行编码，并使用稀疏变压器进行自回归潜在条件。</p>
<p>训练包括两个阶段：</p>
<p>首先，带有中间瓶颈块的 ResNet 风格 VAE 使用 ELBO 损失进行训练。潜在特征使用 argmax（贪婪采样）进行量化。此阶段的目标是图像重建。<br>这一阶段学习了一个约 8k 的视觉代码本（因为 dVAE 利用 argmax 进行量化），该代码本稍后在自回归模型中使用。</p>
<h2 id="DALL-E-dVAE-重建"><a href="#DALL-E-dVAE-重建" class="headerlink" title="DALL-E dVAE 重建"></a>DALL-E dVAE 重建</h2><p><img src="/AI/NLP/NLP-TextImageModels/dalle_vqvae.png" alt="不同几何形状的 dVAE 重建"></p>
<h2 id="DALL-E-自回归先验"><a href="#DALL-E-自回归先验" class="headerlink" title="DALL-E 自回归先验"></a>DALL-E 自回归先验</h2><p>在第二阶段，dVAE 权重被固定，解码器风格的潜在变压器被训练以学习条件先验。模型使用交叉熵损失进行训练，其中图像标记的权重是文本标记的 7 倍。</p>
<p>BPE 标记化的文本与特殊的填充标记（如果需要）和特定的 [START OF IMAGE] 标记连接在一起。然后将图像标记序列化并添加到末尾。文本具有 1D 位置编码，而图像标记具有单独的列和行位置编码。模型使用稀疏注意力来预测文本和图像的下一个标记。</p>
<h2 id="DALL-E-注意力机制"><a href="#DALL-E-注意力机制" class="headerlink" title="DALL-E 注意力机制"></a>DALL-E 注意力机制</h2><p>文本标记具有因果注意力，而图像标记具有单独的行和列注意力，以及最后一层的 $3x3$ 局部注意力。列注意力被转置为行以减少计算量。行和列注意力以 (r, c, r, r) 的方式交替进行。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/dalle_attn_mask.png" alt="DALL-E 注意力机制"></p>
<h2 id="DALL-E-生成的图像"><a href="#DALL-E-生成的图像" class="headerlink" title="DALL-E 生成的图像"></a>DALL-E 生成的图像</h2><p><img src="/AI/NLP/NLP-TextImageModels/dalle_generated.png" alt="零样本联合图像-文本条件生成的图像"></p>
<h1 id="扩散方法"><a href="#扩散方法" class="headerlink" title="扩散方法"></a>扩散方法</h1><h2 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h2><p>扩散模型是依赖于马尔可夫过程生成图像的高效模型。扩散由迭代的加噪和去噪过程定义。加噪是一个高斯噪声注入过程。维度不变。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/diff_transform.png" alt="加噪对数据分布的影响，来源：[ayandas.me](https:&#x2F;&#x2F;ayandas.me&#x2F;blog-tut&#x2F;2021&#x2F;12&#x2F;04&#x2F;diffusion-prob-models.html)"></p>
<p>前向（编码）和反向（解码）扩散可以看作是 VAE 的等价物。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/diffprocess.png" alt="扩散过程，来源：[github](https:&#x2F;&#x2F;lilianweng.github.io&#x2F;)"></p>
<h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><h3 id="去噪扩散概率模型"><a href="#去噪扩散概率模型" class="headerlink" title="去噪扩散概率模型"></a>去噪扩散概率模型</h3><p><img src="/AI/NLP/NLP-TextImageModels/ddpm_chain.png" alt="DDPM 处理链"></p>
<p><strong>前向过程</strong></p>
<p>$x_0$ 是起始图像，而 $x_T$ 是扩散过程的最终步骤 $t\in[0,…, T]$。</p>
<p>每一步 $q(x_t|x_{t-1})&#x3D;\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_tI)$ 马尔可夫转移分布添加由固定 $\beta_t$ 调节的噪声。</p>
<p><strong>反向过程</strong></p>
<p>$x_T$ 从最终的 $p(x_T)&#x3D;\mathcal{N}(x_T;0,I)$ 分布中采样。</p>
<p>每个反向步骤学习一个 $p_\theta(x_{t-1}|x_{t})&#x3D;\mathcal{N}(x_{t-1};\mu_\theta, \Sigma_\theta)$ 高斯分布作为由 $\theta$ 参数化的转移。</p>
<p>两个马尔可夫过程在以下方程中建模：</p>
<p>$q(x_{1:T}|x_0)&#x3D;\prod\limits_{t&#x3D;1}^Tq(x_t|x_{t-1})$</p>
<p>$p_\theta(x_{0:T})&#x3D;p(x_T)\prod\limits_{t&#x3D;1}^Tp_\theta(x_{t-1}|x_t)$</p>
<p>给定 $\alpha_t &#x3D; 1-\beta_t$ 和 $\bar\alpha_t&#x3D;\prod\limits_{s&#x3D;1}^t\alpha_s$，可以直接计算 $q(x_t|x_0)&#x3D;\mathcal{N}(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)$。这加速了训练，因为不需要递归。</p>
<h3 id="学习反向过程"><a href="#学习反向过程" class="headerlink" title="学习反向过程"></a>学习反向过程</h3><p>我们必须再次优化（作者取负并最小化）变分下界 $L &#x3D; \sum\limits_{t&#x3D;0}^TL_t$（直接应用计算的项）：</p>
<div>
$
L_T = \mathbb{E}_q(D_{KL}(q(x_T|x_0)||p_\theta(x_T)))
$
</div>

<p>这确保了第一个反向步骤接近最后一个前向步骤。</p>
<div>
$
L_{1 \ldots t-1} = \mathbb{E}_q(D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)))
$
</div>

<p>这里</p>
<div>
$
q(x_{t-1}|x_t,x_0)
$
</div>

<p>是后验（贝叶斯后）分布，这是每一步的最优反向分布。</p>
<p>$L_0 &#x3D; \mathbb{E}<em>q(-logp</em>\theta(x_0|x_1))$ 数据对潜在链末端的对数似然。</p>
<h3 id="简化一切"><a href="#简化一切" class="headerlink" title="简化一切"></a>简化一切</h3><p>有关训练目标推导的更多详细信息，请参见附录A。</p>
<p>如果我们固定两个过程的标准差并在每一步将它们绑定在一起（在每个高斯中使用 $\beta_tI$），则 $L_{t-1}$ 项回归到两个均值的距离。</p>
<p>$L_{1 \ldots t-1} &#x3D; \mathbb{E}<em>q\left(\frac{1}{2\sigma_t^2}||\tilde\mu(x_t,x_0)-\mu</em>\theta(x_t, t)||^2\right) + C$</p>
<p>这里 $\tilde\mu(x_t,x_0)$ 是前向后验分布的均值（通过贝叶斯反转），而 $\mu_\theta(x_t, t)$ 是可学习的反向分布的均值。$C$ 是一个常数。</p>
<p>将重参数化技巧应用于 $q(x_t|x_0)&#x3D;\mathcal{N}(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)$ 我们得到：</p>
<p>$x_t &#x3D; \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon$ 其中 $\epsilon\sim\mathcal{N}(0,I)$</p>
<p>重新排列方程得到：$x_0 &#x3D; \frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}$</p>
<p>贝叶斯后：$\tilde\mu(x_t,x_0)&#x3D;\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}x_t+\frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{1-\bar\alpha_t}x_0$</p>
<p>替换 $x_0$ 我们得到：$\tilde\mu(x_t,\epsilon)&#x3D;\frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon\right)$</p>
<h3 id="噪声预测"><a href="#噪声预测" class="headerlink" title="噪声预测"></a>噪声预测</h3><p>最后我们得出结论，为了最小化 KL 散度（通过近似 $\tilde\mu(x_t,x_0)$ 和 $\mu_\theta(x_t,t)$ 来最小化），我们必须预测 $\tilde\mu(x_t,x_0)$ 的未知部分，即 $\epsilon$，因为 $x_t$，$\alpha$ 和 $\beta$ 是已知的。为此，我们创建了一个 $\epsilon$ 的近似器，即 $\epsilon_\theta(x_t, t)$，因此 $\mu_\theta(x_t, \epsilon_\theta(x_t, t))&#x3D;\frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta\right)$。这可以在推理过程中使用。</p>
<p>如果我们忽略 $C$，$L_0$ 和 $L_T$，我们得到以下优化函数：</p>
<div>
$
L_{simp}(\theta) = \mathbb{E}_{t, x_0, \epsilon} ||\epsilon - \epsilon_\theta(x_t, t) ||^2 = \mathbb{E}_{t, x_0, \epsilon} ||\epsilon - \epsilon_\theta\left(\sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon, t\right) ||^2
$
</div>

<p>，在训练过程中我们使用第二种形式，因为 $x_0$ 和 $t$ 是输入。</p>
<h3 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h3><p><img src="/AI/NLP/NLP-TextImageModels/ddpm_train.png" alt="DDPM 训练和推理。梯度基于图像和时间步长计算。推理通过预测的噪声项和独立采样的噪声项计算。"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>前向过程是一个马尔可夫过程，向图像添加噪声</li>
<li>我们可以直接计算前向过程的任意深度 $q(x_t|x_0)$</li>
<li>通过取前向过程的贝叶斯后验，我们有了反向过程的标签 $q(x_{t-1}|x_t,x_0)$</li>
<li>在推理过程中 $x_0$ 是未知的，因此我们需要一个不依赖于 $x_0$ 的反向后验近似器。这是 $p_\theta(x_{t-1}|x_t)$</li>
<li>优化目标是最小化（训练期间已知的）后验和近似后验之间的 KL 散度 $D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))$</li>
<li>我们固定方差，因此只需通过 $\mu_\theta$ 近似均值</li>
<li>使用重参数化技巧，我们可以将不确定性分解为标准高斯噪声项 $\epsilon$，在 $x_0 \rightarrow x_t$ 转换过程中使用</li>
<li>这样 $\mu_\theta$ 的唯一非确定性部分是我们必须预测的 $\epsilon_\theta$。因此训练目标变为 $||\epsilon - \epsilon_\theta(x_t, t) ||^2$。其中 $\epsilon$ 是在前向过程中添加到图像的噪声项（训练期间可用）</li>
</ol>
<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><p>DDPM 使用小步长，累积约 500-1000 步生成。$\beta$ 的时间调度是线性的。作者使用相对较小的 $256\times256$ 尺寸的 U-Net 架构进行 $\epsilon_\theta$ 预测。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/ddpm_uncond.png" alt="无条件生成"></p>
<h3 id="潜在插值"><a href="#潜在插值" class="headerlink" title="潜在插值"></a>潜在插值</h3><p><img src="/AI/NLP/NLP-TextImageModels/ddpm_interpol.png" alt="潜在插值。潜在插值是图像流形中的有效点，而不是像素空间插值。"></p>
<p><img src="/AI/NLP/NLP-TextImageModels/ddpm_interpol_result.png" alt="结果取决于图像插值的步骤。“更深”的混合导致高保真度，但原始信息丢失。"></p>
<h2 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h2><h3 id="去噪扩散隐式模型"><a href="#去噪扩散隐式模型" class="headerlink" title="去噪扩散隐式模型"></a>去噪扩散隐式模型</h3><p>DDPM 报告说，采样一批 $128$ 张 $256\times256$ 尺寸的图像大约需要 $300$ 秒。这是由于大量的小去噪步骤。</p>
<p>DDIM 通过放松马尔可夫约束并在前向和反向过程中制定隐式步骤来解决这个问题，从而使用更少的去噪步骤。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/ddim_chain.png" alt="DDIM 处理链"></p>
<h3 id="重新定义为非马尔可夫过程"><a href="#重新定义为非马尔可夫过程" class="headerlink" title="重新定义为非马尔可夫过程"></a>重新定义为非马尔可夫过程</h3><p>在 DDPM 中，我们可以直接计算每一步的前向分布 $q(x_t|x_0)$。利用这一特性，我们可以在任何给定步骤 $t$ 直接计算 $L$，因为 DDPM 损失仅依赖于预测误差 $\epsilon_\theta(x_0, t)$。</p>
<p>DDIM 通过使用一个正系数向量 $\gamma$ 来权衡每一步的损失（由于 $t$ 仅影响添加的噪声 $\epsilon$，我们用 $\epsilon^t$ 表示 $t$ 依赖性）：</p>
<div>
$
L_{\gamma}(\epsilon_\theta) = \mathbb{E}_{x_0, \epsilon^t, t} \sum\limits_{t=1}^T\gamma_t||\epsilon^t - \epsilon_\theta^t(x_0, \epsilon^t)||^2
$
</div>

<p>通过将所有步骤的 $\gamma$ 固定为 1，我们得到原始的 DDPM 损失。</p>
<p>如果我们选择一组 $Q$ 前向分布，使它们可以边缘化为 DDPM 使用的相同 $q(x_t|x_0)$ 分布，我们得到相同的训练目标，因此可以使用相同的 DDPM 训练模型进行前向和反向过程，即使这些过程由于我们选择的 $Q$ 分布而是非马尔可夫的。</p>
<p>对于给定的正 $\sigma$ 标准差向量，存在这样的非马尔可夫过程。首先可以通过生成过程定义它。</p>
<p>$q_\sigma(x_{1:T}|x_0) &#x3D; q_\sigma(x_T|x_0)\prod\limits_{t&#x3D;2}^Tq_\sigma(x_{t-1}|x_{t},x_0)$</p>
<p>通过取 $q_\sigma(x_{t-1}|x_{t},x_0)$ 的后验（贝叶斯），我们得到 $q_\sigma(x_{t}|x_{t-1},x_0)$。通过对 $x_{t-1}$ 进行边缘化，我们得到 $q_\sigma(x_{t}|x_{0})$，因此我们可以使用（$\gamma$）广义 DDPM 训练方案。</p>
<p>我们也可以重新定义反向过程 $p_\theta(x_{t-1}|x_t)$，它应该近似 $q_\sigma(x_{t-1}|x_{t},x_0)$。在生成过程中 $x_0$ 是未知的，但如果我们知道所有的误差 $\epsilon_t$，我们可以近似它。我们也有一个近似器 $\epsilon_\theta^t$。</p>
<p>让 $f_\theta(x_t)$ 通过从 $x_t$ 中减去适当缩放的误差近似 $\epsilon_\theta^t$ 来近似这个 $x_0$ 原始输入。</p>
<p>反向过程是 $p_\theta(x_{t-1}|x_t) &#x3D; q_\sigma(x_{t-1}|x_t, f_\theta(x_t))$ 对于 $t&gt;1$ 和 $\mathcal{N}(f_\theta(x_1), \sigma_1^2I)$ 对于 $t&#x3D;1$。</p>
<p>优化目标可以表述为 ELBO，其中 $\simeq$ 用于描述等于一个不依赖于 $\theta$ 的常数，对于 $t&gt;1$：</p>
<div>
$
\mathbb{E}_{x_{0:T}}(\sum\limits_{t=2}^TD_{KL}(q_\sigma(x_{t-1}|x_{t},x_0)||p_\theta(x_{t-1}|x_t)) - logp_\theta(x_1|x_0))
$
</div>

<div>
$
\simeq \mathbb{E}_{x_0, x_t}(\sum\limits_{t=2}^TD_{KL}(q_\sigma(x_{t-1}|x_{t},x_0)||q_\sigma(x_{t-1}|x_{t},f_\theta(x_t))))
$
</div>

<div>
$
\simeq \mathbb{E}_{x_0, x_t}(\sum\limits_{t=2}^T||x_0 - f_\theta(x_t)||^2) \simeq \mathbb{E}_{x_0, \epsilon, x_t}(\sum\limits_{t=2}^T||\epsilon - \epsilon_\theta^t(x_0, \epsilon)||^2)$
$=L_{\gamma}(\epsilon_\theta)+C
$
</div>

<p>鉴于此，如果我们选择正确的 $\gamma$ 和 $C$ 项，我们将得到原始的 DDPM 损失。</p>
<h3 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h3><p>如果我们定义 $p_\theta(x_{t-1}|x_t) &#x3D; q_\sigma(x_{t-1}|x_t, f_\theta(x_t))$ 并对其使用重参数化技巧（不包含详细信息），我们得到以下内容：</p>
<p>$x_{t-1} &#x3D; \sqrt{\alpha_{t-1}}f_\theta(x_t)+\sqrt{1-\bar\alpha_{t-1}-\sigma_t^2}\epsilon_\theta^t + \sqrt{\sigma_t}\epsilon$</p>
<p>其中第一项是缩放的预测 $x_0$ 输入（也包括 $\epsilon_\theta^t$ 误差项），第二项是指向 $x_t$ 的“方向”，第三项是 $x_{t-1}$ 和 $x_t$ 之间的独立噪声差异。</p>
<p>如果我们为所有 $t$ 选择 $\sigma_t &#x3D; \sqrt{(1-\bar\alpha_{t-1})&#x2F;(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t&#x2F;\bar\alpha_{t-1}}$，则前向过程变为马尔可夫过程，反向过程变为 DDPM。</p>
<p>如果 $\sigma_t &#x3D; 0$，则前向过程在已知 $x_0$ 和 $x_{t-1}$ 的情况下变为确定性。反向过程将不包含独立噪声项，因此可以使用固定程序进行预测。</p>
<p>我们还可以通过超参数 $\eta$ 在两种版本之间进行插值：</p>
<p>$\sigma_t &#x3D; \eta \sqrt{(1-\bar\alpha_{t-1})&#x2F;(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t&#x2F;\bar\alpha_{t-1}}$</p>
<h3 id="加速生成"><a href="#加速生成" class="headerlink" title="加速生成"></a>加速生成</h3><p><img src="/AI/NLP/NLP-TextImageModels/ddim_accel.png" alt="DDIM 加速生成"></p>
<p>由于我们现在可以使用固定的反向过程（除了我们必须从 $x_t$ 预测 $x_0$），我们可以直接从任何一组 $x_t$ 中采样，包括跳过步骤或子采样步骤，甚至考虑基于连续时间的采样。</p>
<p>只要 $q_\sigma(x_{\tau_i}|x_{\tau_{i-1}},x_0)$ 是已知的，加速生成适用于一组步骤 $\tau_i \in [0, T]$，因为反向过程可以通过预测 $\epsilon_\theta^{\tau_i}$ 来近似 $q_\sigma(x_{\tau_{i-1}}|x_{\tau_i},x_0)$，这也用于 $f_\theta(x_{\tau_i})$。</p>
<p>重要的是，仅通过近似 $\epsilon_\theta^t$ 才能实现这种采样步骤的重新调度（至少在 DDIM $\eta&#x3D;0$ 的情况下），因为我们可以在数学上解释这些变化。</p>
<p>这样，采样不再与训练（前向）步骤数绑定，可以实现大约 $10-50$ 倍的加速生成，从而促进在接近实时应用中使用更大的模型。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/AI/NLP/NLP-TextImageModels/ddim_results.png" alt="DDIM 结果。模型在 T&#x3D;1000 前向步骤上训练。DDIM 也略微受益于更长的采样（估计误差添加的噪声更少，因为估计的分布更接近我们用高斯近似的分布）。$\hat\sigma$ 代表为 1000 步设计的原始 DDPM 参数。"></p>
<h3 id="超越-DDIM"><a href="#超越-DDIM" class="headerlink" title="超越 DDIM"></a>超越 DDIM</h3><p>通过将 DDIM 重新表述为 ODE 求解器（常微分方程），得出它等效于 ODE 求解的欧拉方法。还可以使用其他扩展的 ODE 求解器，例如 DPM++。到 2023 年底，这些是最流行的扩散采样器。在更高阶 ODE 求解器领域也有一些研究，但尚未观察到广泛使用。</p>
<p>还提出了超越 DDPM 和 DDIM 使用的线性 $\beta$ 的新噪声调度方法。</p>
<h3 id="DDIM-总结"><a href="#DDIM-总结" class="headerlink" title="DDIM 总结"></a>DDIM 总结</h3><ol>
<li>DDIM 使用非马尔可夫过程进行扩散，使用跳到 $T$ 的想法，然后逐渐反转到 $t$。$q_\sigma(x_{1:T}|x_0) &#x3D; q_\sigma(x_T|x_0)\prod\limits_{t&#x3D;2}^Tq_\sigma(x_{t-1}|x_{t},x_0)$</li>
<li>然后反向过程可以直接使用 $q_\sigma(x_{t-1}|x_t,x_0)$。这里 $x_0$ 仅在训练期间已知，因此在推理过程中我们必须使用预测器来近似它，而不是像在 DDPM 中那样近似完整的后验</li>
<li>我们定义 $f_\theta(x_t)$ 从 $x_t$ 近似 $x_0$，并使用相同的表示技巧将问题转化为噪声 $\epsilon$ 预测问题</li>
<li>证明了 DDIM 和 DDPM 的训练目标是等效的，只是包围误差预测的数学结构不同</li>
<li>DDIM 推理过程实现了 $x_t \rightarrow f_\theta(x_t) \rightarrow x_0 \rightarrow x_{t-1}$ 的直觉。注意 $t-1$ 预测不显式依赖于 $t$，因此我们可以跳到任何先前的步骤而不是 $t-1$</li>
<li>然而，$f_\theta(x_t)$ 预测并不完美，因此反向过程也应该采取多个步骤的细化以确保一致的质量</li>
<li>现代实现将时间步长处理为连续变量，因此它们使用 ODE 求解器来指导反向过程</li>
</ol>
<h2 id="引导扩散"><a href="#引导扩散" class="headerlink" title="引导扩散"></a>引导扩散</h2><h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3><p>扩散引导也可以类似于文本依赖的GAN和VAE。在这种情况下，我们的扩散模型的估计器应该受到引导信号（例如文本嵌入）的扰动。</p>
<p>在反向过程中，给定噪声估计器 $\epsilon_\theta$，我们将条件偏移加到噪声项的估计中。</p>
<p>$\hat\epsilon_\theta(x_t,y) &#x3D; \epsilon_\theta(x_t,y) + s\sigma_t \nabla_{x_t}logp_\phi(y|x_t)$</p>
<p>其中$s$是引导尺度，$p_\phi(y|x_t)$ 是一个分类器，使用关于我们想生成的类别的对数似然的导数来引导扩散过程。</p>
<h3 id="无分类器"><a href="#无分类器" class="headerlink" title="无分类器"></a>无分类器</h3><p>这需要扩散模型与分类器一起训练，并且在生成过程中也需要分类器梯度。为了避免这种情况，无分类器网络在无条件估计器 $\epsilon_\theta(x_t)$ 和条件估计器 $\epsilon_\theta(x_t,y)$ 上操作，通过与无条件预测的差异来实现 $\hat\epsilon_\theta(x_t,y) &#x3D; \epsilon_\theta(x_t,y) + s(\epsilon_\theta(x_t,y)-\epsilon_\theta(x_t))$</p>
<p>这本质上是通过在训练期间向分类器添加$0$标签来完成的，因此无分类器估计器只是 $\epsilon_\theta(x_t,0)$。</p>
<h3 id="CLIP引导扩散-GLIDE"><a href="#CLIP引导扩散-GLIDE" class="headerlink" title="CLIP引导扩散: GLIDE"></a>CLIP引导扩散: GLIDE</h3><p>引入了一种方法，用CLIP点积相似度度量替换分类器。</p>
<p>$\hat\epsilon_\theta(x_t,y) &#x3D; \epsilon_\theta(x_t,y) + s\sigma_t \nabla_{x_t}(f(x_t)\cdot g(y))$</p>
<p>这里$f$是图像编码器，$g$是文本编码器。重要的是，这个CLIP版本是用噪声图像训练的，以匹配扩散模型的噪声水平。</p>
<p>还得出结论，无分类器引导在小众任务上似乎比CLIP引导更有效。这里文本由类似GPT的模型编码，利用最后的嵌入向量。</p>
<h3 id="GLIDE结果"><a href="#GLIDE结果" class="headerlink" title="GLIDE结果"></a>GLIDE结果</h3><p><img src="/AI/NLP/NLP-TextImageModels/glide_results.png" alt="GLIDE结果"></p>
<h2 id="潜在扩散"><a href="#潜在扩散" class="headerlink" title="潜在扩散"></a>潜在扩散</h2><p>潜在扩散是指在VAE或GAN的潜在空间中进行扩散的方法。DALL-E 2和Stable Diffusion是这种方法的两个流行示例。</p>
<p>虽然DALL-E 2使用类似CLIP的模型作为VAE，并具有基于Transformer的扩散先验，但Stable Diffusion利用VQ-GAN生成器和U-Net风格的先验。</p>
<h2 id="DALL-E-2"><a href="#DALL-E-2" class="headerlink" title="DALL-E 2"></a>DALL-E 2</h2><p><img src="/AI/NLP/NLP-TextImageModels/dalle2_arch.png" alt="DALL-E 2架构"></p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>DALL-E 2 包含一个由 CLIP 文本编码器构建的 VAE 和一个用作图像解码器的扩散模型（因此 OpenAI 也将 DALL-E 2 背后的模型命名为“unCLIP”）。该解码器以 CLIP 图像嵌入为条件，也可以选择以字幕文本嵌入为条件。它使用 GLIDE 的架构，并通过在解码器训练期间偶尔丢弃 CLIP 嵌入和字幕嵌入来包括一些无分类器的改进。<br>训练使用锐度感知最小化。</p>
<p>解码器在 DDIM 过程中使用 $\eta&gt;0$ 以允许生成图像的变化。</p>
<h3 id="先验"><a href="#先验" class="headerlink" title="先验"></a>先验</h3><p>他们比较的 VAE 先验是 DALL-E（1）风格的自回归先验和扩散先验，后者被证明更优越。扩散过程也使用 Transformer 解码器来预测下一个潜在标记版本。损失不是基于 $\epsilon$ 的，而是预测和真实潜在表示之间的 L2 损失。这被解释为扩散过程中由 $f_\theta(x_t, t, y)$ 参数化的预测 $x_0$ 的损失。</p>
<p>在生成过程中，先验生成两次，选择与文本嵌入点积较高的那个。</p>
<h2 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h2><h3 id="SD-架构"><a href="#SD-架构" class="headerlink" title="SD 架构"></a>SD 架构</h3><p><img src="/AI/NLP/NLP-TextImageModels/stable_arch.png" alt="Stable Diffusion 架构"></p>
<h3 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ-GAN"></a>VQ-GAN</h3><p>在 Stable Diffusion 中，为了将图像投影到潜在空间，使用 VQ-GAN 对图像进行编码。VQ-GAN 由 VQ-VAE 生成器和对抗性和感知损失的判别器组成。在 Stable Diffusion 的情况下，VQ-VAE 得到轻微的 KL 散度正则化，以生成更接近高斯分布的潜在表示。</p>
<p>此外，判别器后来被忽略，只使用生成器 VQ-VAE，以一种将量化与解码器合并的方式使用。使用 2D 表示的压缩因子 $4-8$ 被发现表现最佳。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/stable_vqgan.png" alt="VQ-GAN"></p>
<h3 id="SD-先验"><a href="#SD-先验" class="headerlink" title="SD 先验"></a>SD 先验</h3><p>先验本身是一个卷积 U-Net 架构，每一层都有交叉注意力块。这些交叉注意力关注于编码的条件模态。这个 U-Net 执行反向扩散过程（预测 $\epsilon_\theta$）。</p>
<p>作为条件编码的模态可以是文本（transformer-decoder 或 CLIP）、图像、分割掩码等……甚至低分辨率图像也可以用来构建潜在的超分辨率模型。无分类器引导也被探索并且有益。</p>
<p>这是一个潜在扩散，这意味着名称 LDM（潜在扩散模型），它是 Stable Diffusion 中使用的模型的泛化。</p>
<h3 id="零样本"><a href="#零样本" class="headerlink" title="零样本"></a>零样本</h3><p><img src="/AI/NLP/NLP-TextImageModels/ldm_zero_shot_result.png" alt="Stable Diffusion 零样本生成"></p>
<h3 id="放大"><a href="#放大" class="headerlink" title="放大"></a>放大</h3><p><img src="/AI/NLP/NLP-TextImageModels/ldm_bsr_results.png" alt="Stable Diffusion 放大"></p>
<h3 id="分割掩码合成"><a href="#分割掩码合成" class="headerlink" title="分割掩码合成"></a>分割掩码合成</h3><p><img src="/AI/NLP/NLP-TextImageModels/ldm_synthesis_result.png" alt="Stable Diffusion 分割掩码合成"></p>
<h3 id="布局合成"><a href="#布局合成" class="headerlink" title="布局合成"></a>布局合成</h3><p><img src="/AI/NLP/NLP-TextImageModels/ldm_layout_result.png" alt="Stable Diffusion 布局合成"></p>
<h2 id="DiT"><a href="#DiT" class="headerlink" title="DiT"></a>DiT</h2><p>在一些最近的模型中，UNet 被一种特殊的扩散变压器 (DiT) 架构所取代，该架构使用类似 ViT 的主干，并通过条件输入来预测误差图像（在某些情况下甚至是复杂扩散用例中使用的 $\Sigma$ 协方差）。</p>
<p>这样，DiT 的输出是两个预测的“图像”（如 UNet 的情况）——误差和协方差，它们通过线性层和补丁过程的逆过程从输出标记中组装而成。</p>
<p>作者观察到，与卷积 UNet 相比，该模型具有更好的扩展性。</p>
<h3 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h3><p>还探索了注入条件的最佳方式。他们发现自适应层归一化是最佳解决方案，具有零缩放初始化（因此在开始时只有残差是活跃的）。<br>这意味着条件信号通过 MLP 映射到潜在大小，MLP 发出输入缩放、输出缩放和我们应用于变压器数据流的移位值。</p>
<p>作者还指出，这优于上下文内条件、交叉注意力，甚至是具有非零缩放初始化的自适应层归一化。</p>
<h3 id="DiT-架构"><a href="#DiT-架构" class="headerlink" title="DiT 架构"></a>DiT 架构</h3><p><img src="/AI/NLP/NLP-TextImageModels/dit.png" alt="DiT 架构"></p>
<h1 id="扩散模型的扩展"><a href="#扩散模型的扩展" class="headerlink" title="扩散模型的扩展"></a>扩散模型的扩展</h1><h2 id="多阶段网络"><a href="#多阶段网络" class="headerlink" title="多阶段网络"></a>多阶段网络</h2><p>多阶段网络用于提高扩散和潜在扩散模型的效率。DALL-E 2 在像素空间中使用多阶段扩散，结合基于 CLIP + 字幕的文本条件超分辨率模型。DALL-E 3 论文还提到他们在不同分辨率上使用了三阶段扩散模型。细节未披露。</p>
<p>最新的发展还引入了精炼器，这些精炼器经过训练以提高生成图像的细节和一致性（整体质量）。这些精炼器在高质量图像上进行训练，并在生成原始潜在图像后使用。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/dalle3_results.png" alt="高分辨率 DALL-E 3 生成"></p>
<p>Imagen 在先验扩散上使用仅文本条件，然后在像素空间中进行两个文本条件超分辨率。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/imagen_arch.png" alt="Imagen 架构"></p>
<p>自回归模型也可以以多阶段方式使用。例如，Parti 由 ViT-VQ-GAN 标记器和解码器组成，用于编码和解码图像。自回归模型是一个全栈变压器，其中条件被编码并生成图像。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/parti_arch.png" alt="Parti 架构"></p>
<p>最近的升级，Würstchen 是一种潜在模型，使用类似于 Stable Diffusion 的 VQ-GAN 生成器（即 VQ-VAE）的编码器-解码器架构，但量化后来被放弃。先验是一个多阶段扩散模型，在高度压缩步骤上使用仅文本条件，然后将此压缩潜在先验和文本用作潜在上采样器的条件步骤，生成最终先验。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/wurstchen_arch.png" alt="Würstchen 架构"></p>
<p><img src="/AI/NLP/NLP-TextImageModels/wurstchen_training.png" alt="Würstchen 训练步骤"></p>
<h2 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h2><p>修复是通过用生成的部分替换原始图像的部分（无论是在像素空间还是潜在空间）来实现的。这是通过使用用户提供的掩码来完成的。在扩散过程中，在扩散过程的每一步中使用未掩盖部分的充分加噪版本。掩盖部分通过加噪的原始 $x_0 \rightarrow x_T$ 或生成的随机初始值进行初始化。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/inpaint_example.png" alt="修复"></p>
<h2 id="文本反演"><a href="#文本反演" class="headerlink" title="文本反演"></a>文本反演</h2><p>文本反演用于扩展文本到图像模型的功能。它们在一小组图像上进行训练，这些图像被输入到预训练的文本到图像模型中。模型被冻结，除了文本编码器的嵌入层。这个嵌入层通过一小组嵌入向量（可以不止一个）进行扩展，这些嵌入向量在一小组图像的扩散损失上进行训练。这样，模型就学会了生成具有新添加的嵌入向量中编码的相同对象或风格的图像。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/textual_inversion.png" alt="文本反演"></p>
<p><img src="/AI/NLP/NLP-TextImageModels/textual_inversion_arch.png" alt="文本反演"></p>
<h2 id="适配器-LoRA-s"><a href="#适配器-LoRA-s" class="headerlink" title="适配器 &#x2F; LoRA-s"></a>适配器 &#x2F; LoRA-s</h2><p>开源扩散模型通常使用全微调或适配器（如 LoRA-s）进行微调。这需要更多的 GPU 内存、训练数据和时间来训练，但质量也更高。</p>
<p>如前所述，可以通过多种方式融合这些适配器或文本反演。这些 LoRA-s 可以通过取它们的加权平均值或通过在从各个 LoRA-s 提取的特征上训练融合的 LoRA 来组合。<br>$W &#x3D; \sum\limits_{i&#x3D;1}^n w_i W_i$</p>
<p>$W &#x3D; argmin_W \sum\limits_{i&#x3D;1}^n ||(W_0 + \Delta W_i) X_i - W X_i||_F^2$</p>
<p><img src="/AI/NLP/NLP-TextImageModels/lora_fusion.png" alt="LoRA 融合结果"></p>
<h2 id="潜在一致性"><a href="#潜在一致性" class="headerlink" title="潜在一致性"></a>潜在一致性</h2><p>引入了 LDMs 的另一个加速方法，即潜在一致性模型。与在反向过程中预测 $\epsilon_\theta$ 不同，LCMs 直接训练 $f_\theta$ 估计器来预测 $x_0$。作为一种自蒸馏任务，我们可以运行长生成序列的反向过程，然后使用较低阶时间步长的 $x_0$ 预测作为目标。</p>
<div>
$
\mathcal{L}_{LCM} = \mathbb{E}_{x, y, w, i} [d(f_\theta(x_{\tau_{i+1}}, w, y, \tau_{i+1}), f_{\theta'}(x^{\psi,w}_{\tau_{i}}, w, y, \tau_{i}))]
$
</div>

<p>其中 $\theta’$ 是一组扩展参数，$\psi$ 是一个求解器，如 DDIM，$w$ 是引导权重，$d$ 是距离函数，$\tau$ 是一组解码时间步长。</p>
<p>DALL-E 3 使用这种方法实现 $2-4$ 步生成。</p>
<h3 id="潜在一致性结果"><a href="#潜在一致性结果" class="headerlink" title="潜在一致性结果"></a>潜在一致性结果</h3><p><img src="/AI/NLP/NLP-TextImageModels/lcm_results.png" alt="潜在一致性结果"></p>
<h3 id="潜在一致性-LoRA"><a href="#潜在一致性-LoRA" class="headerlink" title="潜在一致性 LoRA"></a>潜在一致性 LoRA</h3><p>上述训练目标也适用于 LoRA 训练。此 LoRA 模型可用作多个 LDM 的加速器，原始 LDM 的适配器也可与此 LCM-LoRA 一起使用。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/lcm_lora_results.png" alt="潜在一致性 LoRA"></p>
<h2 id="ControlNet"><a href="#ControlNet" class="headerlink" title="ControlNet"></a>ControlNet</h2><p>控制 LDM 的条件注入很难，添加额外的控制模态需要重新训练。ControlNet 系列创建了一组适应方法，可用于在不重新训练的情况下控制文本到图像模型。</p>
<p>原始 ControlNet 是 SD UNet 编码器的完整并行副本，经过训练以引导原始模型在额外的模态（如分割掩码、姿势、涂鸦等）上进行操作。</p>
<p>文本到图像适配器和 LoRA 也可用于在不重新训练的情况下实现控制。</p>
<p>这些方法可以在推理时相互混合使用。</p>
<p><img src="/AI/NLP/NLP-TextImageModels/controlnet.png" alt="ControlNet 单元"></p>
<h2 id="T2I-适配器"><a href="#T2I-适配器" class="headerlink" title="T2I 适配器"></a>T2I 适配器</h2><p><img src="/AI/NLP/NLP-TextImageModels/t2i_adapter.png" alt="T2I 适配器"></p>
<h2 id="ControlNet-结果"><a href="#ControlNet-结果" class="headerlink" title="ControlNet 结果"></a>ControlNet 结果</h2><p><img src="/AI/NLP/NLP-TextImageModels/controlnet_results.png" alt="ControlNet 结果"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP-TextImageModels</p><p><a href="https://aloen.to/AI/NLP/NLP-TextImageModels/">https://aloen.to/AI/NLP/NLP-TextImageModels/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-12-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-03-20</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI/NLP/NLP-VisionActionModels/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">NLP-VisionActionModels</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/NLP/NLP-VisualModels/"><span class="level-item">NLP-VisualModels</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">60</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">20</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">35</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/RL/"><span class="level-start"><span class="level-item">RL</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">34</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">49</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#介绍"><span class="level-left"><span class="level-item">1</span><span class="level-item">介绍</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#条件文本到图像生成"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">条件文本到图像生成</span></span></a></li><li><a class="level is-mobile" href="#序列到序列生成"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">序列到序列生成</span></span></a></li><li><a class="level is-mobile" href="#AlignDRAW"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">AlignDRAW</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#神经分布建模"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">神经分布建模</span></span></a></li><li><a class="level is-mobile" href="#读取和写入操作"><span class="level-left"><span class="level-item">1.3.2</span><span class="level-item">读取和写入操作</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#GAN"><span class="level-left"><span class="level-item">2</span><span class="level-item">GAN</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#生成对抗网络架构"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">生成对抗网络架构</span></span></a></li><li><a class="level is-mobile" href="#生成对抗网络回顾"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">生成对抗网络回顾</span></span></a></li><li><a class="level is-mobile" href="#文本条件GAN架构"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">文本条件GAN架构</span></span></a></li><li><a class="level is-mobile" href="#文本条件GAN训练"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">文本条件GAN训练</span></span></a></li><li><a class="level is-mobile" href="#文本嵌入插值"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">文本嵌入插值</span></span></a></li><li><a class="level is-mobile" href="#文本条件GAN结果"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">文本条件GAN结果</span></span></a></li><li><a class="level is-mobile" href="#风格编码"><span class="level-left"><span class="level-item">2.7</span><span class="level-item">风格编码</span></span></a></li><li><a class="level is-mobile" href="#风格编码结果"><span class="level-left"><span class="level-item">2.8</span><span class="level-item">风格编码结果</span></span></a></li><li><a class="level is-mobile" href="#ControlGAN"><span class="level-left"><span class="level-item">2.9</span><span class="level-item">ControlGAN</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#架构"><span class="level-left"><span class="level-item">2.9.1</span><span class="level-item">架构</span></span></a></li><li><a class="level is-mobile" href="#局部性"><span class="level-left"><span class="level-item">2.9.2</span><span class="level-item">局部性</span></span></a></li></ul></li><li><a class="level is-mobile" href="#感知损失"><span class="level-left"><span class="level-item">2.10</span><span class="level-item">感知损失</span></span></a></li><li><a class="level is-mobile" href="#词级特征"><span class="level-left"><span class="level-item">2.11</span><span class="level-item">词级特征</span></span></a></li></ul></li><li><a class="level is-mobile" href="#自回归方法"><span class="level-left"><span class="level-item">3</span><span class="level-item">自回归方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#DALL-E-架构"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">DALL-E 架构</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-训练"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">DALL-E 训练</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-dVAE-重建"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">DALL-E dVAE 重建</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-自回归先验"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">DALL-E 自回归先验</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-注意力机制"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">DALL-E 注意力机制</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-生成的图像"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">DALL-E 生成的图像</span></span></a></li></ul></li><li><a class="level is-mobile" href="#扩散方法"><span class="level-left"><span class="level-item">4</span><span class="level-item">扩散方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#扩散模型"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">扩散模型</span></span></a></li><li><a class="level is-mobile" href="#DDPM"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">DDPM</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#去噪扩散概率模型"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">去噪扩散概率模型</span></span></a></li><li><a class="level is-mobile" href="#学习反向过程"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">学习反向过程</span></span></a></li><li><a class="level is-mobile" href="#简化一切"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">简化一切</span></span></a></li><li><a class="level is-mobile" href="#噪声预测"><span class="level-left"><span class="level-item">4.2.4</span><span class="level-item">噪声预测</span></span></a></li><li><a class="level is-mobile" href="#训练和推理"><span class="level-left"><span class="level-item">4.2.5</span><span class="level-item">训练和推理</span></span></a></li><li><a class="level is-mobile" href="#总结"><span class="level-left"><span class="level-item">4.2.6</span><span class="level-item">总结</span></span></a></li><li><a class="level is-mobile" href="#特性"><span class="level-left"><span class="level-item">4.2.7</span><span class="level-item">特性</span></span></a></li><li><a class="level is-mobile" href="#潜在插值"><span class="level-left"><span class="level-item">4.2.8</span><span class="level-item">潜在插值</span></span></a></li></ul></li><li><a class="level is-mobile" href="#DDIM"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">DDIM</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#去噪扩散隐式模型"><span class="level-left"><span class="level-item">4.3.1</span><span class="level-item">去噪扩散隐式模型</span></span></a></li><li><a class="level is-mobile" href="#重新定义为非马尔可夫过程"><span class="level-left"><span class="level-item">4.3.2</span><span class="level-item">重新定义为非马尔可夫过程</span></span></a></li><li><a class="level is-mobile" href="#反向过程"><span class="level-left"><span class="level-item">4.3.3</span><span class="level-item">反向过程</span></span></a></li><li><a class="level is-mobile" href="#加速生成"><span class="level-left"><span class="level-item">4.3.4</span><span class="level-item">加速生成</span></span></a></li><li><a class="level is-mobile" href="#结果"><span class="level-left"><span class="level-item">4.3.5</span><span class="level-item">结果</span></span></a></li><li><a class="level is-mobile" href="#超越-DDIM"><span class="level-left"><span class="level-item">4.3.6</span><span class="level-item">超越 DDIM</span></span></a></li><li><a class="level is-mobile" href="#DDIM-总结"><span class="level-left"><span class="level-item">4.3.7</span><span class="level-item">DDIM 总结</span></span></a></li></ul></li><li><a class="level is-mobile" href="#引导扩散"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">引导扩散</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#分类器"><span class="level-left"><span class="level-item">4.4.1</span><span class="level-item">分类器</span></span></a></li><li><a class="level is-mobile" href="#无分类器"><span class="level-left"><span class="level-item">4.4.2</span><span class="level-item">无分类器</span></span></a></li><li><a class="level is-mobile" href="#CLIP引导扩散-GLIDE"><span class="level-left"><span class="level-item">4.4.3</span><span class="level-item">CLIP引导扩散: GLIDE</span></span></a></li><li><a class="level is-mobile" href="#GLIDE结果"><span class="level-left"><span class="level-item">4.4.4</span><span class="level-item">GLIDE结果</span></span></a></li></ul></li><li><a class="level is-mobile" href="#潜在扩散"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">潜在扩散</span></span></a></li><li><a class="level is-mobile" href="#DALL-E-2"><span class="level-left"><span class="level-item">4.6</span><span class="level-item">DALL-E 2</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#解码器"><span class="level-left"><span class="level-item">4.6.1</span><span class="level-item">解码器</span></span></a></li><li><a class="level is-mobile" href="#先验"><span class="level-left"><span class="level-item">4.6.2</span><span class="level-item">先验</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Stable-Diffusion"><span class="level-left"><span class="level-item">4.7</span><span class="level-item">Stable Diffusion</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#SD-架构"><span class="level-left"><span class="level-item">4.7.1</span><span class="level-item">SD 架构</span></span></a></li><li><a class="level is-mobile" href="#VQ-GAN"><span class="level-left"><span class="level-item">4.7.2</span><span class="level-item">VQ-GAN</span></span></a></li><li><a class="level is-mobile" href="#SD-先验"><span class="level-left"><span class="level-item">4.7.3</span><span class="level-item">SD 先验</span></span></a></li><li><a class="level is-mobile" href="#零样本"><span class="level-left"><span class="level-item">4.7.4</span><span class="level-item">零样本</span></span></a></li><li><a class="level is-mobile" href="#放大"><span class="level-left"><span class="level-item">4.7.5</span><span class="level-item">放大</span></span></a></li><li><a class="level is-mobile" href="#分割掩码合成"><span class="level-left"><span class="level-item">4.7.6</span><span class="level-item">分割掩码合成</span></span></a></li><li><a class="level is-mobile" href="#布局合成"><span class="level-left"><span class="level-item">4.7.7</span><span class="level-item">布局合成</span></span></a></li></ul></li><li><a class="level is-mobile" href="#DiT"><span class="level-left"><span class="level-item">4.8</span><span class="level-item">DiT</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#条件"><span class="level-left"><span class="level-item">4.8.1</span><span class="level-item">条件</span></span></a></li><li><a class="level is-mobile" href="#DiT-架构"><span class="level-left"><span class="level-item">4.8.2</span><span class="level-item">DiT 架构</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#扩散模型的扩展"><span class="level-left"><span class="level-item">5</span><span class="level-item">扩散模型的扩展</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#多阶段网络"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">多阶段网络</span></span></a></li><li><a class="level is-mobile" href="#修复"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">修复</span></span></a></li><li><a class="level is-mobile" href="#文本反演"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">文本反演</span></span></a></li><li><a class="level is-mobile" href="#适配器-LoRA-s"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">适配器 / LoRA-s</span></span></a></li><li><a class="level is-mobile" href="#潜在一致性"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">潜在一致性</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#潜在一致性结果"><span class="level-left"><span class="level-item">5.5.1</span><span class="level-item">潜在一致性结果</span></span></a></li><li><a class="level is-mobile" href="#潜在一致性-LoRA"><span class="level-left"><span class="level-item">5.5.2</span><span class="level-item">潜在一致性 LoRA</span></span></a></li></ul></li><li><a class="level is-mobile" href="#ControlNet"><span class="level-left"><span class="level-item">5.6</span><span class="level-item">ControlNet</span></span></a></li><li><a class="level is-mobile" href="#T2I-适配器"><span class="level-left"><span class="level-item">5.7</span><span class="level-item">T2I 适配器</span></span></a></li><li><a class="level is-mobile" href="#ControlNet-结果"><span class="level-left"><span class="level-item">5.8</span><span class="level-item">ControlNet 结果</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-19T08:25:53.000Z">2025-03-19</time></p><p class="title"><a href="/AI/RL/RL-MonteCarlo/">RL-MonteCarlo</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/RL/">RL</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-19T03:14:01.000Z">2025-03-19</time></p><p class="title"><a href="/Algorithm/DAA-Midterm-Revision/">DAA Midterm Revision</a></p><p class="categories"><a href="/categories/Algorithm/">Algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-18T03:55:21.000Z">2025-03-18</time></p><p class="title"><a href="/Data-Science/ITDS-%E8%81%9A%E7%B1%BB/">ITDS-聚类</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-12T06:55:06.000Z">2025-03-12</time></p><p class="title"><a href="/Data-Science/ITDS-%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/">ITDS-聚类分析</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-12T02:30:00.000Z">2025-03-12</time></p><p class="title"><a href="/Data-Science/ITDS-%E7%AE%80%E4%BB%8B/">ITDS-简介</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2025 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>