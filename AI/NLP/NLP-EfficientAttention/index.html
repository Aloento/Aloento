<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>NLP-EfficientAttention - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="高效注意力"><meta property="og:type" content="blog"><meta property="og:title" content="NLP-EfficientAttention"><meta property="og:url" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="高效注意力"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/bird_attention.jpg"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/sparse_attention.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/big_bird.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/alibi_comparison.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/alibi_speed.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/alibias.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/rope_interpolation.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/rope_interpolation2.png"><meta property="og:image" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/flash_attention.png"><meta property="article:published_time" content="2024-11-15T10:21:49.000Z"><meta property="article:modified_time" content="2025-05-14T17:58:02.479Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="笔记"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/AI/NLP/NLP-EfficientAttention/bird_attention.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/NLP/NLP-EfficientAttention/"},"headline":"NLP-EfficientAttention","image":["https://aloen.to/AI/NLP/NLP-EfficientAttention/bird_attention.jpg","https://aloen.to/AI/NLP/NLP-EfficientAttention/sparse_attention.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/big_bird.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/alibi_comparison.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/alibi_speed.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/alibias.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/rope_interpolation.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/rope_interpolation2.png","https://aloen.to/AI/NLP/NLP-EfficientAttention/flash_attention.png"],"datePublished":"2024-11-15T10:21:49.000Z","dateModified":"2025-05-14T17:58:02.479Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/NLP/NLP-EfficientAttention/"}},"description":"高效注意力"}</script><link rel="canonical" href="https://aloen.to/AI/NLP/NLP-EfficientAttention/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-15T10:21:49.000Z" title="11/15/2024, 10:21:49 AM">2024-11-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-14T17:58:02.479Z" title="5/14/2025, 5:58:02 PM">2025-05-15</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/NLP/">NLP</a></span><span class="level-item">15 minutes read (About 2180 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP-EfficientAttention</h1><div class="content"><p>高效注意力</p>
<span id="more"></span>

<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>Transformer 非常强大，主要得益于 <strong>注意力机制</strong>，它避免了传统 seq2seq 模型中的瓶颈。然而，注意力机制也有其自身的成本。</p>
<p>回想一下在注意力层中，输入序列中的每个向量 $\mathbf{I} &#x3D; \langle\mathbf{i}_1, …, \mathbf{i}_n\rangle$ 都会与另一个序列中的每个向量 $\mathbf{X} &#x3D; \langle\mathbf{x}_1, …, \mathbf{x}_m\rangle$ 进行比较，通过计算（例如）相似度</p>
<p>$$<br>s(\mathbf{i}_i, \mathbf{x}_j) &#x3D; \frac{\mathbf{i}^\intercal_i \mathbf{x}_j}{\sqrt{d}}<br>$$</p>
<p>大多数 Transformer 模型使用 <strong>全局注意力</strong>，其中（在自注意力层中）$\mathbf{X}&#x3D;\mathbf{I}$。这意味着</p>
<p>$$<br>\mathbf{S} &#x3D; \frac{\mathbf{Q^\intercal}\mathbf{K}}{\sqrt{d}}<br>$$</p>
<p>矩阵的计算和存储复杂度将是二次的：$\mathcal{O}(n^2)$。这限制了</p>
<ul>
<li>Transformer 可以使用的 <em>上下文</em></li>
<li>适合 GPU 内存的模型 <em>大小</em></li>
<li>模型的 <em>吞吐量</em>，因此增加了其碳足迹</li>
</ul>
<p>在下文中，我们将看到试图解决这些问题的技术。</p>
<h1 id="稀疏-Sparse-注意力"><a href="#稀疏-Sparse-注意力" class="headerlink" title="稀疏 Sparse 注意力"></a>稀疏 Sparse 注意力</h1><h2 id="稠密-Dense-层中的稀疏性"><a href="#稠密-Dense-层中的稀疏性" class="headerlink" title="稠密 Dense 层中的稀疏性"></a>稠密 Dense 层中的稀疏性</h2><p>在 CIFAR-10 数据集上训练了一个 128 层的图像 Transformer，并观察到在许多层中，注意力模式是稀疏的：</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/bird_attention.jpg" alt="稀疏模式：局部（左），行&#x2F;列（中），数据依赖&#x2F;全局（非稀疏）（右）"></p>
<h2 id="因式分解自注意力"><a href="#因式分解自注意力" class="headerlink" title="因式分解自注意力"></a>因式分解自注意力</h2><p>注意力层可以通过 <strong>连接模式</strong> （connectivity pattern） $S &#x3D; {S_1, …, S_n}$ 来表征，其中 $S_i$ 是第 $i$ 个输出向量关注的输入索引集。对于常规自注意力，这是 $S_i &#x3D; {j: j \leq i}$。</p>
<p><strong>Factorized 自注意力</strong></p>
<ul>
<li>有 $p$ 个独立的头，而不是常规注意力的 1 个头（或多头注意力的 $\times p$）</li>
<li>对于第 $m$ 个头，$S_i &#x3D; A^{(m)}_i \subset {j: j \leq i}$，是稠密注意力的一个 <strong>子集</strong></li>
<li>这些是连续应用的：$A_i &#x3D; A^{(1)}_i \cdots A^{(p)}_i$</li>
</ul>
<p>如果 $|A^{(m)}_i| \propto \sqrt[p]{n}$，则因式分解自注意力的复杂度为 $\mathcal{O}(n\sqrt[p]{n})$。从现在起，假设 $p&#x3D;2$。</p>
<h2 id="因式分解-patterns"><a href="#因式分解-patterns" class="headerlink" title="因式分解 patterns"></a>因式分解 patterns</h2><p>如果 $A$ 可以连接所有输入和输出位置，则因式分解是 <strong>有效的</strong>。两个例子：</p>
<p><strong>跨步 Strided 注意力</strong></p>
<ul>
<li>给定一个 <em>步长</em> $l \approx \sqrt{n}$</li>
<li>$A^{(1)}_i &#x3D; {i-l, i-l+1, …, i}$（前 $l$ 个位置）</li>
<li>$A^{(2)}_i &#x3D; {j: (i - j)\mod l &#x3D; 0}$（每 $l$ 个）</li>
</ul>
<p><strong>固定注意力</strong></p>
<ul>
<li>$A^{(1)}_i &#x3D; {j: (\lfloor j&#x2F;l \rfloor &#x3D; \lfloor i&#x2F;l \rfloor)}$（每个输出向量关注其块）</li>
<li>$A^{(2)}_i &#x3D; {j: j\mod l \in {l-c,l-c+1,…,l} }$（未来的输出关注块中的最后 $c$ 个项目）</li>
</ul>
<p>固定注意力更适合文本，跨步注意力更适合图像。</p>
<p>两种稀疏注意力类型的示意图：</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/sparse_attention.png" alt="常规注意力（左），稀疏跨步（中），稀疏固定（右）"></p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>$p$ 个头可以通过三种方式集成（$W_p$ 是 FF）：</p>
<ol>
<li>每层一个头：<br> $\textrm{attention}(X) &#x3D; W_p \cdot \textrm{attend}(X, A^{(r\mod p)})$</li>
<li><em>合并头</em>：<br> $\textrm{attention}(X) &#x3D; W_p \cdot \textrm{attend}(X, \bigcup^p_{m&#x3D;1}A^m)$</li>
<li>多头（$n_h$）注意力：<br> $\textrm{attention}(X) &#x3D; W_p\bigl(\textrm{attend}(X, A)^{(i)}\bigr)_{i \in {1, …, n_h}}$</li>
</ol>
<p>通过这些变化，可以训练具有数百层和&#x2F;或非常长上下文（文本为 12,160，音乐为 $2^{20}$）的 Transformer。</p>
<h2 id="其他稀疏注意力变体"><a href="#其他稀疏注意力变体" class="headerlink" title="其他稀疏注意力变体"></a>其他稀疏注意力变体</h2><p>多头注意力选项也适用于其他稀疏注意力模式：</p>
<ul>
<li><strong>全局注意力</strong>：一些全局 token 关注整个序列；<br>（注意：这种稀疏全局注意力不同于密集全局注意力，因为只有少数 token 关注所有内容）</li>
<li><strong>随机注意力</strong>：对于每个查询，计算一组 $r$ 个随机键，查询关注这些键</li>
<li><strong>窗口注意力</strong>：仅关注固定半径内的局部邻居</li>
</ul>
<h2 id="Big-Bird"><a href="#Big-Bird" class="headerlink" title="Big Bird"></a>Big Bird</h2><p>Big Bird 模型结合了所有这些线性注意力类型，以显著增加输入 token 的数量，而不会显著改变内存需求：</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/big_bird.png" alt="BigBird 稀疏注意力掩码"></p>
<h1 id="长序列"><a href="#长序列" class="headerlink" title="长序列"></a>长序列</h1><h2 id="长序列的外推"><a href="#长序列的外推" class="headerlink" title="长序列的外推"></a>长序列的外推</h2><p>RNN 可以在短序列上训练，但在推理期间可以在更长的序列上运行。Transformer 解码器可以像这样 <strong>extrapolate</strong>，但效果不佳：</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/alibi_comparison.png" alt="各种位置方法的外推性能"></p>
<h2 id="位置方法"><a href="#位置方法" class="headerlink" title="位置方法"></a>位置方法</h2><p>测试了以下位置方法：</p>
<ol>
<li><strong>正弦位置嵌入</strong>：默认的 Transformer 嵌入。性能在增加 $5-10%$ 额外 token 后下降</li>
<li><strong>旋转位置嵌入 (RoPE)</strong>：例如在 GPT-3 中使用。将Sinusoidal嵌入应用于每个注意力层中的 $\mathcal{K}$ 和 $\mathcal{Q}$（但不应用于 $\mathcal{V}$ 和嵌入）。可以外推到 $+10-40%$ 的 token，但速度较慢</li>
<li><strong>T5 偏置</strong>：一种相对位置方法，根据每层中键和值对之间的距离向 $\mathcal{V}$ 添加一个 <em>学习</em> 偏置。嵌入未被修改。可以外推到 $+80-120%$，但速度非常慢</li>
</ol>
<p><img src="/AI/NLP/NLP-EfficientAttention/alibi_speed.png" alt="alibi_speed"></p>
<h2 id="ALiBi"><a href="#ALiBi" class="headerlink" title="ALiBi"></a>ALiBi</h2><p>ALiBi (Attention with Linear Biases) 是一种简单的方法，它根据查询-键距离添加一个<em>静态</em>（非学习）偏置：</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/alibias.png" alt="alibias"></p>
<p>$$ \textrm{softmax}(\mathbf{q}_i\mathbf{K}^\intercal + m\cdot[-(i-1),…,-1,0]) $$</p>
<p>对于 $n$ 个头，斜率形成一个几何序列，范围在 $\bigl(1, \frac{1}{2^8}\bigr]$ 之间；例如，对于 8 个头，斜率为 $\frac{1}{2^1}, …, \frac{1}{2^8}$。</p>
<p>具有 ALiBi 的模型可以轻松地将其训练上下文（$L$）外推到 $2-10$ 倍，通常在 $2L$ 时表现最佳！</p>
<h2 id="RoPE-的分析"><a href="#RoPE-的分析" class="headerlink" title="RoPE 的分析"></a>RoPE 的分析</h2><p>为什么 RoPE 的<em>直接外推</em>不起作用？</p>
<p><img src="/AI/NLP/NLP-EfficientAttention/rope_interpolation.png" alt="rope_interpolation"></p>
<p>虽然在 RoPE 中自注意力得分应该只取决于两个位置之间的相对距离，但在训练上下文 $L$ 之外（中间），它会变得任意大。</p>
<p>三角函数族是一个<strong>通用 approximator</strong>，可以拟合任意函数。由于我们只在 $[0, L]$ 范围内训练 RoPE，我们不知道函数在 $L$ 以上的表现。</p>
<h2 id="位置插值"><a href="#位置插值" class="headerlink" title="位置插值"></a>位置插值</h2><p>位置插值（Position Interpolation，PI）通过将更长的上下文窗口 $L’$ 映射到 $L$ 来解决这个问题。</p>
<ul>
<li>每个位置 $m’ \in L’$ 被转换为 $m &#x3D; m’\frac{L}{L’}$</li>
<li>然后对模型进行 1000 步的微调</li>
</ul>
<p><img src="/AI/NLP/NLP-EfficientAttention/rope_interpolation2.png" alt="rope_interpolation2"></p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>实验表明</p>
<ul>
<li>Llama 的 2k 上下文可以扩展到 32k；</li>
<li>针对 $L’&#x3D;8k$ 微调的模型在原始 $L$ 范围内表现出最小的退化（$2%$）。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/discussions/1965">其他</a> <a target="_blank" rel="noopener" href="https://kaiokendev.github.io/context">实现</a>类似策略的<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">报告</a>表明，即使没有微调，也可能实现 $2-4\times$ 的扩展。</p>
<h1 id="Flash-注意力"><a href="#Flash-注意力" class="headerlink" title="Flash 注意力"></a>Flash 注意力</h1><h2 id="理论与实践"><a href="#理论与实践" class="headerlink" title="理论与实践"></a>理论与实践</h2><p>许多近似方法（例如稀疏注意力）设法减少了 FLOPs，但没有减少<strong>实际时间</strong>。这是因为忽略了内存访问速度（IO）。</p>
<p>常规注意力通过物化从 $Q, K, V \in \mathbb{R}^{N\times d}$ 计算输出 $\mathbf{O} \in \mathbb{R}^{N\times d}$</p>
<p>$\mathbf{S} &#x3D; \mathbf{QK}^\intercal \in \mathbb{R}^{N\times N}$</p>
<p>$\mathbf{P} &#x3D; \textrm{softmax}(\mathbf{S}) \in \mathbb{R}^{N\times N}$</p>
<p>$\mathbf{O} &#x3D; \mathbf{PV} \in \mathbb{R}^{N\times d}$</p>
<p>在 GPU 的<em>高带宽内存</em>（<em>HBM</em>）中。这是因为</p>
<ul>
<li>这些是 PyTorch &#x2F; TF 中的单独指令</li>
<li>这些矩阵在反向传播 backpropagation 中是必需的</li>
</ul>
<h2 id="GPU-层次结构"><a href="#GPU-层次结构" class="headerlink" title="GPU 层次结构"></a>GPU 层次结构</h2><p>主要问题是</p>
<ul>
<li>与计算和 SRAM 相比，HBM 非常慢</li>
<li>softmax、dropout、norm 操作都是<strong>内存受限</strong>的，需要 $\mathcal{O}(N^2)$ 的 HBM 访问</li>
</ul>
<p><img src="/AI/NLP/NLP-EfficientAttention/flash_attention.png" alt="flash_attention"></p>
<p><strong>FlashAttention</strong> 通过优化内存访问解决了这些问题。</p>
<h2 id="FlashAttention"><a href="#FlashAttention" class="headerlink" title="FlashAttention"></a>FlashAttention</h2><p>优化：</p>
<ul>
<li>$\mathbf{S}$ 和 $\mathbf{P}$ 使用<em>平铺 tiling</em>逐块计算</li>
<li>它们从未在 HBM 上物化，而是在反向传播时重新计算</li>
<li>单个块的所有操作在单个<em>内核</em>中同时执行</li>
</ul>
<p>尽管重新计算会导致更多的指令（FLOPs），但 HBM 访问的总次数减少了</p>
<ul>
<li>从 $\Theta(Nd+N^2)$（标准注意力）</li>
<li>到 $\Theta(N^2d^2M^{-1})$（FlashAttention）</li>
</ul>
<p>其中 SRAM 大小 $M &gt; d^2$ “多次”。</p>
<p>FlashAttention</p>
<ul>
<li>相对于标准注意力，实现了 $15%$（与 BERT 速度记录相比）到 $3\times$（GPT-2）的加速</li>
<li>内存随 $N$ 线性扩展</li>
<li>允许更长的上下文（GPT-2 为 4k）</li>
</ul>
<p><strong>稀疏 FlashAttention</strong></p>
<ul>
<li>块稀疏注意力可以通过 FlashAttention 加速</li>
<li>更快（取决于稀疏性，$2-4\times$），上下文可达 64k</li>
</ul>
<p><strong>FlashAttention2</strong> 通过优化 GPU 线程之间的作业分配，再次实现 $2\times$ 的加速</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP-EfficientAttention</p><p><a href="https://aloen.to/AI/NLP/NLP-EfficientAttention/">https://aloen.to/AI/NLP/NLP-EfficientAttention/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-11-15</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-05-15</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI/NLP/NLP-ReducedComplexity/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">NLP-ReducedComplexity</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/NLP/NLP-MixtureModels/"><span class="level-item">NLP-MixtureModels</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">67</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">36</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/CogSci/"><span class="level-start"><span class="level-item">CogSci</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/RL/"><span class="level-start"><span class="level-item">RL</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">37</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">53</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6/"><span class="tag">认知科学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#动机"><span class="level-left"><span class="level-item">1</span><span class="level-item">动机</span></span></a></li><li><a class="level is-mobile" href="#稀疏-Sparse-注意力"><span class="level-left"><span class="level-item">2</span><span class="level-item">稀疏 Sparse 注意力</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#稠密-Dense-层中的稀疏性"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">稠密 Dense 层中的稀疏性</span></span></a></li><li><a class="level is-mobile" href="#因式分解自注意力"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">因式分解自注意力</span></span></a></li><li><a class="level is-mobile" href="#因式分解-patterns"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">因式分解 patterns</span></span></a></li><li><a class="level is-mobile" href="#架构"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">架构</span></span></a></li><li><a class="level is-mobile" href="#其他稀疏注意力变体"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">其他稀疏注意力变体</span></span></a></li><li><a class="level is-mobile" href="#Big-Bird"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">Big Bird</span></span></a></li></ul></li><li><a class="level is-mobile" href="#长序列"><span class="level-left"><span class="level-item">3</span><span class="level-item">长序列</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#长序列的外推"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">长序列的外推</span></span></a></li><li><a class="level is-mobile" href="#位置方法"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">位置方法</span></span></a></li><li><a class="level is-mobile" href="#ALiBi"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">ALiBi</span></span></a></li><li><a class="level is-mobile" href="#RoPE-的分析"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">RoPE 的分析</span></span></a></li><li><a class="level is-mobile" href="#位置插值"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">位置插值</span></span></a></li><li><a class="level is-mobile" href="#结果"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">结果</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Flash-注意力"><span class="level-left"><span class="level-item">4</span><span class="level-item">Flash 注意力</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#理论与实践"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">理论与实践</span></span></a></li><li><a class="level is-mobile" href="#GPU-层次结构"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">GPU 层次结构</span></span></a></li><li><a class="level is-mobile" href="#FlashAttention"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">FlashAttention</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-07T02:15:33.000Z">2025-05-07</time></p><p class="title"><a href="/Algorithm/DAA-Endterm/">DAA Endterm</a></p><p class="categories"><a href="/categories/Algorithm/">Algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-05T09:19:25.000Z">2025-05-05</time></p><p class="title"><a href="/Data-Science/ITDS-%E5%88%86%E7%B1%BB/">ITDS-分类</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-05T06:13:32.000Z">2025-05-05</time></p><p class="title"><a href="/Data-Science/ITDS-Final/">ITDS Final</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-21T04:22:15.000Z">2025-04-21</time></p><p class="title"><a href="/AI/CogSci/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6/">认知科学</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/CogSci/">CogSci</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-29T16:00:00.000Z">2025-03-30</time></p><p class="title"><a href="/Data-Science/ITDS-Midterm/">ITDS Midterm</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2025 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>