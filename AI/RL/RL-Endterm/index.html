<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>RL-Endterm - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="Quizes 和 Oral"><meta property="og:type" content="blog"><meta property="og:title" content="RL-Endterm"><meta property="og:url" content="https://aloen.to/AI/RL/RL-Endterm/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="Quizes 和 Oral"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/AI/RL/RL-Endterm/9.png"><meta property="og:image" content="https://aloen.to/AI/RL/RL-Endterm/12.jpeg"><meta property="article:published_time" content="2025-05-24T08:50:53.000Z"><meta property="article:modified_time" content="2025-06-14T17:43:34.598Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="考试"><meta property="article:tag" content="AI"><meta property="article:tag" content="RL"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/AI/RL/RL-Endterm/9.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/RL/RL-Endterm/"},"headline":"RL-Endterm","image":["https://aloen.to/AI/RL/RL-Endterm/9.png"],"datePublished":"2025-05-24T08:50:53.000Z","dateModified":"2025-06-14T17:43:34.598Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/RL/RL-Endterm/"}},"description":"Quizes 和 Oral"}</script><link rel="canonical" href="https://aloen.to/AI/RL/RL-Endterm/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-05-24T08:50:53.000Z" title="5/24/2025, 8:50:53 AM">2025-05-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-14T17:43:34.598Z" title="6/14/2025, 5:43:34 PM">2025-06-15</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/RL/">RL</a></span><span class="level-item">36 minutes read (About 5402 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">RL-Endterm</h1><div class="content"><p>Quizes 和 Oral</p>
<span id="more"></span>

<h1 id="Quizes"><a href="#Quizes" class="headerlink" title="Quizes"></a>Quizes</h1><ol>
<li><p>强化学习智能体的目标是什么？</p>
<ul>
<li>泛化知识</li>
<li>学习数据集的隐藏结构</li>
<li><strong>最大化长期奖励 ✅</strong></li>
<li>从经验中学习</li>
</ul>
<p>强化学习的核心思想是：智能体通过与环境的交互，从经验中学习策略，以最大化未来获得的总奖励。虽然“从经验中 学习”也是过程的一部分，但“最大化长期奖励”才是最终目标。</p>
</li>
<li><p>马尔可夫性质表达的是什么？</p>
<ul>
<li>转移到某个特定状态不依赖于当前状态</li>
<li><strong>转移到某个特定状态只依赖于当前状态 ✅</strong></li>
<li>转移到某个特定状态依赖于当前和之前的状态</li>
<li>转移到某个特定状态只依赖于之前的状态</li>
</ul>
</li>
<li><p>马尔可夫决策过程（MDP）的主要元素是什么？</p>
<ul>
<li>环境、智能体、价值函数、策略、状态、动作、奖励</li>
<li>模型、智能体、状态、奖励、动作、观察、策略</li>
<li><strong>环境、智能体、状态、动作、奖励、模型、策略 ✅</strong></li>
<li>模型、环境动态、策略</li>
</ul>
</li>
<li><p>如果一个任务是“关联型”的（Associative），这意味着什么？</p>
<ul>
<li>不涉及在多个情境中学习行动</li>
<li><strong>在多个情境中采取行动 ✅</strong></li>
<li>与采取的动作无关</li>
<li>使用对动作进行评价的训练信息</li>
</ul>
<p>在下棋中，面对不同的棋局状态，玩家要做出不同的下法，这就是关联型任务。不是单纯学一个固定的动作，而是“观察状态 → 做出合适的动作”。</p>
</li>
<li><p>$q_* (a)$ 表示什么？</p>
<ul>
<li>动作 a 的估计动作价值</li>
<li>动作 a 的计算动作价值</li>
<li>动作 a 的期望动作价值</li>
<li><strong>动作 a 的真实动作价值 ✅</strong></li>
</ul>
</li>
<li><p>大数法则说明了什么？</p>
<ul>
<li>大量试验结果的平均值应接近均值</li>
<li>大量试验结果的总和应接近累加总和</li>
<li>大量试验结果的平均值应接近真实值</li>
<li><strong>大量试验结果的平均值应接近期望值 ✅</strong></li>
</ul>
</li>
<li><p>以下哪句话是正确的？</p>
<ul>
<li>任意时刻只有一个贪婪动作</li>
<li><strong>任意时刻至少有一个贪婪动作</strong> ✅</li>
<li>任意时刻总有多个贪婪动作</li>
<li>任意时刻有时没有贪婪动作</li>
</ul>
<p>贪婪动作指的是在当前已知的信息中，具有最高价值的动作。<br>在一个时刻，有可能多个动作的价值一样高，但至少总能选出一个或多个是“最优”的，这些都属于贪婪动作。</p>
</li>
<li><p>以下哪句话是正确的？</p>
<ul>
<li><strong>利用时总是选择贪婪动作</strong> ✅</li>
<li>探索时总是选择贪婪动作</li>
<li>利用时总是选择非贪婪动作</li>
<li>探索时总是选择非贪婪动作</li>
</ul>
<p>探索不一定总是非贪婪动作，它只是有概率尝试不同的动作。</p>
</li>
<li><p>在这个随机网格世界中，以下哪一个值不可能是 p(14, 0 | 10, up)</p>
<p><img src="/AI/RL/RL-Endterm/9.png" alt="stochastic"></p>
<ul>
<li><strong>1</strong>✅</li>
<li>0</li>
<li>0.8</li>
<li>0.1</li>
</ul>
<p>从状态 10 出发，执行 “up” 动作，到达状态 14 并获得奖励 0 的概率是多少？这是随机网格，所以一切都有可能，所以概率不可能是 100%。</p>
</li>
<li><p>策略评估指的是</p>
<ul>
<li>根据当前价值函数，使策略变成贪婪的</li>
<li><strong>让价值函数与当前策略保持一致</strong> ✅</li>
</ul>
<p>固定策略不动，然后去计算这个策略带来的价值函数，让它们一致。使策略变贪婪是策略改进</p>
</li>
<li><p>关于动态规划，哪一项是正确的？</p>
<ul>
<li><strong>使用 bootstrap 技术，且需要环境模型</strong>✅</li>
<li>不使用 bootstrap，但需要环境模型</li>
<li>不使用 bootstrap，也不需要模型</li>
<li>使用 bootstrap，但不需要模型</li>
</ul>
</li>
<li><p>图中的备份图表示的是哪种方法？</p>
<p><img src="/AI/RL/RL-Endterm/12.jpeg" alt="backup"></p>
<ul>
<li>DP</li>
<li><strong>MC</strong>✅</li>
<li>TD</li>
<li>Sarsa</li>
</ul>
</li>
<li><p>与 off-policy 预测相关的术语有哪些？</p>
<ul>
<li><strong>目标策略 与 行为策略</strong>✅</li>
<li>更新策略 与 行为策略</li>
<li>目标策略 与 控制策略</li>
<li>更新策略 与 控制策略</li>
</ul>
<p>用一个策略学习另一个策略的价值</p>
</li>
<li><p>以下是哪个算法的值函数更新公式</p>
<p>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]<br>$$</p>
<ul>
<li>DP</li>
<li>MC</li>
<li><strong>TD</strong>✅</li>
<li>Sarsa</li>
</ul>
</li>
<li><p>图中的备份图表示的是哪种方法？</p>
<pre class="mermaid">    flowchart TD
n1["Filled Circle"] --> n2["Small Circle"]
n2 --> n3["Filled Circle"]

n1@{ shape: f-circ}
n2@{ shape: sm-circ}
n3@{ shape: f-circ}</pre>

<ul>
<li>DP</li>
<li>MC</li>
<li>TD</li>
<li><strong>Sarsa</strong>✅</li>
</ul>
</li>
<li><p>缺失的部分是什么？</p>
<p>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \text{?}<br>$$</p>
<ul>
<li>$V_{t+n-1}(S_{t+n})$ ✅</li>
<li>$V_{t+n}(S_{t+n})$</li>
<li>$V_{t+n}(S_{t+n-1})$</li>
<li>$V_{t+n-1}(S_{t+n-1})$</li>
</ul>
<p>有时间下标的值函数，n-step TD 的回报估计公式</p>
</li>
<li><p>深度优先搜索算法在如下图中，从状态 S 出发，到达目标状态 G，会给出哪个解？</p>
<pre class="mermaid">    flowchart LR
    n1(["a"]) --> n2(["G"]) & n3(["b"])
    n3 --> n1 & n2
    n4(["S"]) --> n1 & n3</pre>

<ul>
<li>SaG</li>
<li>SbG</li>
<li>SabG</li>
<li><strong>No solution</strong> ✅</li>
</ul>
<p>图中有死循环，S -&gt; b -&gt; a -&gt; b…</p>
</li>
<li><p>在使用函数逼近的情况下，权重 $w$ 的维度（即参数数量 d）与状态数量 $S$ 之间的关系是什么？</p>
<ul>
<li>$S \gg d$ ✅</li>
<li>$S &gt; d$</li>
<li>$S &lt; d$</li>
<li>$S \ll d$</li>
</ul>
<p>在函数逼近中，我们的目标是用一个相对低维的参数向量 $w$ 来逼近一个高维的状态空间。</p>
</li>
<li><p>当 TD(λ) 中的 λ 参数设为 1 时，等价于哪种算法？</p>
<ul>
<li>TD(0)</li>
<li>TD(1)</li>
<li><strong>MC</strong> ✅</li>
<li>DP</li>
</ul>
</li>
<li><p>λ 是什么？</p>
<ul>
<li>折扣因子 → 通常是 γ</li>
<li>步长&#x2F;学习率 → 通常是 α</li>
<li>迹衰减参数 ✅</li>
<li>权重向量 → 通常是 w</li>
</ul>
</li>
</ol>
<h1 id="Oral"><a href="#Oral" class="headerlink" title="Oral"></a>Oral</h1><h2 id="强化学习的关键概念、要素与定义"><a href="#强化学习的关键概念、要素与定义" class="headerlink" title="强化学习的关键概念、要素与定义"></a>强化学习的关键概念、要素与定义</h2><p><a href="https://aloen.to/AI/RL/RL-%E8%A7%84%E5%88%92%E4%B8%8E%E5%AD%A6%E4%B9%A0/#RL-%E6%A6%82%E5%BF%B5">概念</a></p>
<p>智能体与环境交互，通过试错学习最优策略，以最大化长期累计奖励的过程。</p>
<p>状态，动作，奖励，策略，价值函数，环境模型 是核心要素</p>
<p>监督学习有标签，强化学习只有奖励反馈，没有标准答案</p>
<ul>
<li><p>基于模型：有环境模型，如 DP</p>
</li>
<li><p>无模型：如 MC TD</p>
</li>
<li><p>基于值：如 Q-Learning</p>
</li>
<li><p>混合型：同时学习策略和价值函数，如 Dyna</p>
</li>
<li><p>回合式任务：任务有明确的开始和结束</p>
</li>
<li><p>持续任务：无限进行</p>
</li>
</ul>
<h2 id="马尔可夫决策过程的概念、要素与定义"><a href="#马尔可夫决策过程的概念、要素与定义" class="headerlink" title="马尔可夫决策过程的概念、要素与定义"></a>马尔可夫决策过程的概念、要素与定义</h2><p><a href="https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5">MDP</a></p>
<p>在某个状态选择一个动作，得到奖励，并进入下一个状态。<br>用来描述代理和环境之间的交互过程，如何决策并最大化长期回报。</p>
<p>状态，动作，奖励，转移概率，折扣因子（0 - 1） 是 MDP 的核心要素<br>未来只和当前状态有关，和过去无关。</p>
<h2 id="探索-利用困境，贪婪方法"><a href="#探索-利用困境，贪婪方法" class="headerlink" title="探索-利用困境，贪婪方法"></a>探索-利用困境，贪婪方法</h2><p>exploration 与 exploitation 无法兼得。</p>
<p>贪婪方法在每一步都选择当前估计价值最大的动作，可能陷入局部最优。<br>而 $\epsilon$-贪婪方法增加了一点随机性：</p>
<ul>
<li>以 $\epsilon$ 的概率随机选择一个动作（探索，包括最大价值的动作）</li>
<li>以 $1 - \epsilon$ 的概率选择当前估计价值最大的动作（利用）</li>
</ul>
<p><a href="https://aloen.to/AI/RL/RL-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8">greedy</a></p>
<h2 id="K-臂赌博机问题、非平稳问题与增量实现"><a href="#K-臂赌博机问题、非平稳问题与增量实现" class="headerlink" title="K 臂赌博机问题、非平稳问题与增量实现"></a>K 臂赌博机问题、非平稳问题与增量实现</h2><p><a href="https://aloen.to/AI/RL/RL-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/#%E8%80%81%E8%99%8E%E6%9C%BA">老虎机</a></p>
<p>最普通的情况，有 K 台老虎机，每台中奖概率不同，目标是在有限次数内最大化总奖励，特点是只有一个状态，每次只选择一个动作，只能观察结果，其他台的状态未知。</p>
<p>Non-stationary 的情况是老虎机的中奖概率会变化，我们使用学习率来更新估值，让新数据更重要。</p>
<p><a href="https://aloen.to/AI/RL/RL-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/#%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0">增量更新</a></p>
<p>Incremental implementation 是因为我们不想储存所有历史奖励，我们使用增量公式，基于之前的估计和当前的奖励再乘上学习率来更新当前的估计值，可以应对非平稳问题。</p>
<p>$Q_{n + 1}(a) &#x3D; Q_n(a) + \alpha_n \left[ R_{n + 1} - Q_n(a) \right]$</p>
<h2 id="状态价值函数、状态-动作价值函数、贝尔曼方程"><a href="#状态价值函数、状态-动作价值函数、贝尔曼方程" class="headerlink" title="状态价值函数、状态-动作价值函数、贝尔曼方程"></a>状态价值函数、状态-动作价值函数、贝尔曼方程</h2><p><a href="https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/#%E7%AD%96%E7%95%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0">函数</a></p>
<p>State Value Function：在策略 π 下，从状态 $s$ 出发，未来累计奖励的期望值</p>
<p>$$<br>v_\pi(s) &#x3D; \mathbb{E}_\pi [G_t | S_t &#x3D; s]<br>$$</p>
<blockquote>
<p>当前在状态 $s$，按照策略 π 走下去，能拿多少总奖励？</p>
</blockquote>
<p>Action-Value Function：在策略 π 下，从状态 $s$ 执行动作 $a$ 后，未来累计奖励的期望值</p>
<p>$$<br>q_\pi(s, a) &#x3D; \mathbb{E}_\pi [G_t | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>
<blockquote>
<p>在状态 $s$ 做了动作 $a$，然后按策略 π 走下去，总共能拿多少奖励？</p>
</blockquote>
<p>Bellman Equation 是对上面两个价值函数的递归定义：</p>
<p>状态价值函数的贝尔曼方程：</p>
<p>$$<br>v_\pi(s) &#x3D; \sum_a \pi(a|s) \sum_{s’,r} p(s’, r|s, a) [r + \gamma v_\pi(s’)]<br>$$</p>
<p>状态-动作价值函数的贝尔曼方程：</p>
<p>$$<br>q_\pi(s, a) &#x3D; \sum_{s’,r} p(s’, r|s, a) [r + \gamma \sum_{a’} \pi(a’|s’) q_\pi(s’, a’)]<br>$$</p>
<blockquote>
<p>当前价值 &#x3D; 当前奖励 + 折扣 × 下个状态的价值<br>是“现在”和“未来”的平衡表达</p>
</blockquote>
<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p><a href="https://aloen.to/AI/RL/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%9F%BA%E7%A1%80">DP</a></p>
<p>利用递归和记忆的方式，在已知环境模型的情况下，计算最优策略和价值函数。<br>空间必须是有限可枚举的，通过 Bellman 方程来迭代更新价值函数。</p>
<ul>
<li>策略评估：迭代计算每个状态的价值</li>
<li>策略改进：对每个状态选择价值最高的动作，得到新的策略</li>
<li>策略迭代：交替进行评估和改进</li>
<li>价值迭代：评估和改进合成一步，每次只更新一次价值就立刻选择最优动作</li>
<li>异步更新，所有状态不一起更新，而是逐个状态更新</li>
</ul>
<h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p><a href="https://aloen.to/AI/RL/RL-MonteCarlo/#MC-%E7%AD%96%E7%95%A5">MC</a></p>
<p>不需要环境模型的强化学习方法，通过采样回合数据，直接估计价值函数。</p>
<ul>
<li>模型未知</li>
<li>状态空间大</li>
<li>Episodic</li>
</ul>
<p>核心思想：玩很多次，算平均数</p>
<p>$$<br>V(s) \leftarrow V(s) + \alpha (G_t - V(s))<br>$$</p>
<p>其中 $G_t$ 是从状态 $s$ 开始到回合结束的累计奖励。</p>
<ol>
<li><p>First-Visit MC<br>只更新状态首次出现时的回报，稳定，适合大多数情况</p>
</li>
<li><p>Every-Visit MC<br>每次状态出现都更新，收敛更快，但更新次数多</p>
</li>
</ol>
<p>为了解决永远不访问某些状态-动作对的问题，MC 采用 Stochastic Policy<br>如 $\epsilon$-greedy，确保每个动作都有非零概率被选中</p>
<h2 id="时序差分方法"><a href="#时序差分方法" class="headerlink" title="时序差分方法"></a>时序差分方法</h2><p><a href="https://aloen.to/AI/RL/RL-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/#%E5%85%A5%E9%97%A8">TD</a></p>
<p>TD 方法结合了：</p>
<ul>
<li>蒙特卡洛的采样真实经验</li>
<li>动态规划的自举更新</li>
<li>不用等到回合结束，也不用环境模型，就能边学边更新</li>
</ul>
<p>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]<br>$$</p>
<ul>
<li><p>$R_{t+1}$：即时奖励</p>
</li>
<li><p>$V(S_{t+1})$：下一个状态的估值</p>
</li>
<li><p>整体括号：预测误差</p>
</li>
<li><p>Sarsa：On-policy TD，用自己策略估值更新</p>
</li>
<li><p>Q-Learning：Off-policy TD，用最大动作估值更新</p>
</li>
<li><p>Expected Sarsa：期望下一个动作的加权平均，折中</p>
</li>
</ul>
<p><a href="https://aloen.to/AI/RL/RL-Midterm/#%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94">算法对比</a></p>
<h2 id="策略改进与广义策略改进"><a href="#策略改进与广义策略改进" class="headerlink" title="策略改进与广义策略改进"></a>策略改进与广义策略改进</h2><p><a href="https://aloen.to/AI/RL/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/#Policy-Improvement">改进</a></p>
<p>Policy Improvement：在已知策略 π 的状态价值函数 $v_\pi(s)$ 的基础上，找到更好的策略 π′ 来获取更高回报</p>
<p>改进原则：在每个状态选择当前最优动作，也就是：</p>
<p>$$<br>\pi’(s) &#x3D; \arg\max_a \sum_{s’, r} p(s’, r|s, a) [r + \gamma v_\pi(s’)]<br>$$</p>
<p>Policy Iteration &#x3D; (策略评估 + 策略改进) 交替进行：</p>
<ol>
<li>策略评估：计算 $v_\pi(s)$</li>
<li>策略改进：根据 $v_\pi(s)$ 贪婪选最优动作，得到新策略</li>
</ol>
<p>重复进行，直到策略不再变化。</p>
<p>Generalized Policy Iteration：现实中不需要等到完全评估完再改进策略，可以边评估边改进，只评估几步就开始改。</p>
<h2 id="On-policy-与-Off-policy，重要性采样"><a href="#On-policy-与-Off-policy，重要性采样" class="headerlink" title="On-policy 与 Off-policy，重要性采样"></a>On-policy 与 Off-policy，重要性采样</h2><p><a href="https://aloen.to/AI/RL/RL-MonteCarlo/#On-Off-Policy">策略</a></p>
<table>
<thead>
<tr>
<th>类型</th>
<th>核心区别</th>
<th>示例算法</th>
</tr>
</thead>
<tbody><tr>
<td>On-policy</td>
<td>采样 &amp; 学习用同一个策略</td>
<td>Sarsa</td>
</tr>
<tr>
<td>Off-policy</td>
<td>采样用一个，学习用另一个策略</td>
<td>Q-Learning</td>
</tr>
</tbody></table>
<p>On-policy：使用当前策略 π 来采样和学习</p>
<blockquote>
<p>用 ε-greedy 策略采样，并用它来更新价值函数</p>
</blockquote>
<p>优点：实现简单<br>缺点：探索效率可能低，收敛较慢</p>
<p>Off-policy：使用行为策略 b 来采样，但学习的是目标策略 π（通常是最优的贪婪策略）</p>
<p>优点：数据利用率高<br>缺点：要解决两个策略分布不同的问题</p>
<p>Importance Sampling 是指 Off-policy 要从行为策略 $b$ 学习目标策略 $\pi$，需要做“校正”，用重要性采样比率来重新加权采样数据：</p>
<p>一步重要性采样比率：</p>
<p>$$<br>\rho &#x3D; \frac{\pi(a|s)}{b(a|s)}<br>$$</p>
<blockquote>
<p>原本这个动作在目标策略 π 下的概率 &#x2F; 行为策略 b 下的概率</p>
</blockquote>
<p>多步回合重要性采样：</p>
<p>$$<br>\rho_t &#x3D; \prod_{k&#x3D;t}^{T} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}<br>$$</p>
<table>
<thead>
<tr>
<th>方式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>Ordinary Sampling</td>
<td>无偏，但方差大</td>
</tr>
<tr>
<td>Weighted Sampling</td>
<td>有偏，但稳定性好、收敛快</td>
</tr>
</tbody></table>
<h2 id="SARSA、Q-Learning-及其改进方法"><a href="#SARSA、Q-Learning-及其改进方法" class="headerlink" title="SARSA、Q-Learning 及其改进方法"></a>SARSA、Q-Learning 及其改进方法</h2><p><a href="https://aloen.to/AI/RL/RL-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/#Sarsa">算法</a></p>
<p>SARSA 是 On-policy TD 控制算法，名字来自它更新用到的五元组：</p>
<p>$$<br>(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})<br>$$</p>
<p>更新公式：</p>
<p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]<br>$$</p>
<p>特点：使用自己当前策略采样并学习，更新比较稳定，但探索能力较弱，可能保守，不一定找到最优路径</p>
<p>Q-Learning 是一种 Off-policy TD 控制算法，学习的是最优策略，即使行为策略是 ε-greedy</p>
<p>更新公式：</p>
<p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]<br>$$</p>
<p>特点：学习目标是<strong>最优动作值函数</strong>，收敛快，适合找最短路径。缺点：容易高估（偏差）、不够稳定</p>
<p>Expected SARSA 是 SARSA 与 Q-Learning 的折中版本：</p>
<p>更新公式：</p>
<p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right]<br>$$</p>
<ul>
<li>比 SARSA 更高效</li>
<li>比 Q-Learning 更稳定</li>
<li>用策略的动作概率做加权平均</li>
</ul>
<p>Q-Learning 有个问题叫 Maximization Bias</p>
<p>Double Q-Learning 的做法：维护两个 Q 表，一个选动作，一个估值，防止互相抬价</p>
<p>$$<br>Q_1(s,a) \leftarrow Q_1(s,a) + \alpha \left[ r + \gamma Q_2(s’, \arg\max_a Q_1(s’,a)) - Q_1(s,a) \right]<br>$$</p>
<p>好处：更稳更准，偏差小</p>
<h2 id="自举（Bootstrapping）"><a href="#自举（Bootstrapping）" class="headerlink" title="自举（Bootstrapping）"></a>自举（Bootstrapping）</h2><p><a href="https://aloen.to/AI/RL/RL-MonteCarlo/#Bootstrap">自举</a></p>
<p>用已有估计更新当前估计，也就是说，当前状态的价值，用下一个状态的估计值来更新</p>
<p>TD 和 DP 都是自举，MC 不是。</p>
<h2 id="规划与学习智能体，Dyna-架构"><a href="#规划与学习智能体，Dyna-架构" class="headerlink" title="规划与学习智能体，Dyna 架构"></a>规划与学习智能体，Dyna 架构</h2><ul>
<li>学习：与真实环境交互</li>
<li>规划：用模拟环境学习</li>
</ul>
<p>智能体可以有两种行为：</p>
<ul>
<li>反射型：只看当前状态做决定，不考虑长期后果，可以没有模型</li>
<li>规划型：根据模型预测未来，做出更聪明的决策，需要环境模型</li>
</ul>
<p><a href="https://aloen.to/AI/RL/RL-%E8%A7%84%E5%88%92%E4%B8%8E%E5%AD%A6%E4%B9%A0/#Dyna-Q">Dyna</a></p>
<p>Dyna &#x3D; 规划 + 学习 的融合方法</p>
<ol>
<li>与真实环境交互</li>
<li>更新 Q 值</li>
<li>把数据存入模型</li>
<li>用模型生成虚拟经验</li>
</ol>
<p>比较纯粹的 Q-Learning 更高效。</p>
<h2 id="强化学习中的函数逼近"><a href="#强化学习中的函数逼近" class="headerlink" title="强化学习中的函数逼近"></a>强化学习中的函数逼近</h2><p><a href="https://aloen.to/AI/RL/RL-%E8%A7%84%E5%88%92%E4%B8%8E%E5%AD%A6%E4%B9%A0/#%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC">近似</a></p>
<p>当状态空间太大，不能再用表格存 Q 和 V 值时，就需要用函数来近似。</p>
<p>典型的方法是线性逼近，可以估计没见过的状态，参数少，可用于在线学习，但是不够精确</p>
<ul>
<li>Coarse Coding：用多个圆覆盖状态空间，状态属于哪些圆，就激活哪些特征</li>
<li>Tile Coding：用多个矩形网格编码状态，每个 tile 是一个特征，适合连续状态空间</li>
<li>神经网络：表达能力更强，但需要更多数据和计算资源</li>
</ul>
<h2 id="资格迹与-λ-回报"><a href="#资格迹与-λ-回报" class="headerlink" title="资格迹与 λ-回报"></a>资格迹与 λ-回报</h2><p><a href="https://aloen.to/AI/RL/RL-%E8%B5%84%E6%A0%BC%E8%BF%B9/">迹</a></p>
<p>TD 更新快但信息少，MC 更新慢但信息多，我们需要一个折中方法，这就是 TD(λ)。<br>资格迹记录谁参与过，该被奖励，是一种短期记忆机制，随着时间衰减。<br>谁资格迹高，谁就更新的更频繁。</p>
<h2 id="深度强化学习——价值与策略学习，SOTA-算法"><a href="#深度强化学习——价值与策略学习，SOTA-算法" class="headerlink" title="深度强化学习——价值与策略学习，SOTA 算法"></a>深度强化学习——价值与策略学习，SOTA 算法</h2><p>深度强化学习 &#x3D; 强化学习 + 深度神经网络</p>
<p>用神经网络代替表格或线性函数，逼近价值函数或策略函数</p>
<p>适合：</p>
<ul>
<li>状态空间大或连续</li>
<li>高维输入（如图像、视频）</li>
</ul>
<p>价值学习（Value-based）典型算法：DQN（Deep Q-Network）</p>
<p>思路：</p>
<ul>
<li>用神经网络估计 Q 值：$Q(s, a)$</li>
<li>策略通过 $\arg\max Q(s, a)$ 决定动作（贪婪）</li>
</ul>
<p>关键技术：</p>
<ol>
<li>经验回放（Replay Buffer）：打乱相关性，提升样本利用率</li>
<li>目标网络（Target Network）：固定一段时间不更新，防止发散</li>
</ol>
<p>策略学习（Policy-based）典型算法：Policy Gradient、A2C、PPO 等</p>
<p>思路：</p>
<ul>
<li>直接用神经网络输出策略 $\pi(a|s)$</li>
<li>用梯度上升优化目标函数 $J(\theta)$</li>
<li>可学出随机策略，适合连续动作空间</li>
</ul>
<p>Actor-Critic 架构结合策略和值函数：</p>
<ul>
<li>Actor：负责输出策略 π（选动作）</li>
<li>Critic：估计值函数（判断动作好坏）</li>
</ul>
<blockquote>
<p>互相促进，提升性能和稳定性</p>
</blockquote>
<p>SOTA（先进） 算法</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>类型</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>DQN</td>
<td>价值型</td>
<td>离散动作，Q-learning + CNN</td>
</tr>
<tr>
<td>A2C</td>
<td>Actor-Critic</td>
<td>同步多线程训练</td>
</tr>
<tr>
<td>A3C</td>
<td>Actor-Critic</td>
<td>异步多智能体，更快收敛</td>
</tr>
<tr>
<td>PPO</td>
<td>Policy-based</td>
<td>策略剪切，训练更稳定</td>
</tr>
<tr>
<td>TRPO</td>
<td>Policy-based</td>
<td>保守更新，防止退步</td>
</tr>
<tr>
<td>DDPG</td>
<td>连续控制</td>
<td>Actor-Critic + 目标网络</td>
</tr>
<tr>
<td>SAC</td>
<td>高效探索</td>
<td>最大熵策略，探索更强</td>
</tr>
</tbody></table>
<h2 id="事后经验回放、模仿学习"><a href="#事后经验回放、模仿学习" class="headerlink" title="事后经验回放、模仿学习"></a>事后经验回放、模仿学习</h2><p>事后经验回放（Experience Replay）是什么？</p>
<p>把过去的交互经验存下来，在训练时重复利用旧数据，提高效率</p>
<p>适用于：如 DQN、DDPG、SAC 等深度强化学习算法</p>
<p>关键作用：</p>
<ol>
<li>打破数据相关性（防止连续样本导致训练不稳定）</li>
<li>提高数据利用率（旧经验还能用）</li>
</ol>
<p>重要变种：Hindsight Experience Replay（HER）</p>
<ul>
<li>适用于稀疏奖励任务（如机器人抓东西）</li>
<li>把失败经验“重新解释成成功”，提高学习效率</li>
</ul>
<p>思路：把没完成目标的经验，换个目标当作“完成了另一个任务”</p>
<p>模仿学习（Imitation Learning）是什么？</p>
<p>不是自己试错，而是模仿专家的行为来学习策略</p>
<p>适用于：专家容易演示，但奖励难以设计的任务<br>如自动驾驶、机械臂操作等</p>
<p>行为克隆（Behaviour Cloning）</p>
<ul>
<li>把专家演示当成监督学习数据</li>
<li>输入状态 → 输出动作</li>
<li>学习一个策略网络模仿专家</li>
<li>简单高效</li>
<li>会积累错误，偏离专家轨迹</li>
<li>模仿学习 ≠ 监督学习，状态分布是非独立同分布（non-iid）的</li>
<li>要想提升鲁棒性，可结合数据增强、多任务训练</li>
</ul>
<script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';	mermaid.initialize({startOnLoad: true, flowchart: {curve: 'linear'}}); </script></div><div class="article-licensing box"><div class="licensing-title"><p>RL-Endterm</p><p><a href="https://aloen.to/AI/RL/RL-Endterm/">https://aloen.to/AI/RL/RL-Endterm/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-05-24</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-06-15</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E8%80%83%E8%AF%95/">考试</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/RL/">RL</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Memo/%E9%87%8F%E5%8C%96%E6%97%A5%E8%AE%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">量化日记</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/RL/RL-%E8%B5%84%E6%A0%BC%E8%BF%B9/"><span class="level-item">RL-资格迹</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">71</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">37</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/CogSci/"><span class="level-start"><span class="level-item">CogSci</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/RL/"><span class="level-start"><span class="level-item">RL</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">39</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A0%E5%AF%86%E8%B4%A7%E5%B8%81/"><span class="tag">加密货币</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">54</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6/"><span class="tag">认知科学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Quizes"><span class="level-left"><span class="level-item">1</span><span class="level-item">Quizes</span></span></a></li><li><a class="level is-mobile" href="#Oral"><span class="level-left"><span class="level-item">2</span><span class="level-item">Oral</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#强化学习的关键概念、要素与定义"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">强化学习的关键概念、要素与定义</span></span></a></li><li><a class="level is-mobile" href="#马尔可夫决策过程的概念、要素与定义"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">马尔可夫决策过程的概念、要素与定义</span></span></a></li><li><a class="level is-mobile" href="#探索-利用困境，贪婪方法"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">探索-利用困境，贪婪方法</span></span></a></li><li><a class="level is-mobile" href="#K-臂赌博机问题、非平稳问题与增量实现"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">K 臂赌博机问题、非平稳问题与增量实现</span></span></a></li><li><a class="level is-mobile" href="#状态价值函数、状态-动作价值函数、贝尔曼方程"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">状态价值函数、状态-动作价值函数、贝尔曼方程</span></span></a></li><li><a class="level is-mobile" href="#动态规划"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">动态规划</span></span></a></li><li><a class="level is-mobile" href="#蒙特卡洛方法"><span class="level-left"><span class="level-item">2.7</span><span class="level-item">蒙特卡洛方法</span></span></a></li><li><a class="level is-mobile" href="#时序差分方法"><span class="level-left"><span class="level-item">2.8</span><span class="level-item">时序差分方法</span></span></a></li><li><a class="level is-mobile" href="#策略改进与广义策略改进"><span class="level-left"><span class="level-item">2.9</span><span class="level-item">策略改进与广义策略改进</span></span></a></li><li><a class="level is-mobile" href="#On-policy-与-Off-policy，重要性采样"><span class="level-left"><span class="level-item">2.10</span><span class="level-item">On-policy 与 Off-policy，重要性采样</span></span></a></li><li><a class="level is-mobile" href="#SARSA、Q-Learning-及其改进方法"><span class="level-left"><span class="level-item">2.11</span><span class="level-item">SARSA、Q-Learning 及其改进方法</span></span></a></li><li><a class="level is-mobile" href="#自举（Bootstrapping）"><span class="level-left"><span class="level-item">2.12</span><span class="level-item">自举（Bootstrapping）</span></span></a></li><li><a class="level is-mobile" href="#规划与学习智能体，Dyna-架构"><span class="level-left"><span class="level-item">2.13</span><span class="level-item">规划与学习智能体，Dyna 架构</span></span></a></li><li><a class="level is-mobile" href="#强化学习中的函数逼近"><span class="level-left"><span class="level-item">2.14</span><span class="level-item">强化学习中的函数逼近</span></span></a></li><li><a class="level is-mobile" href="#资格迹与-λ-回报"><span class="level-left"><span class="level-item">2.15</span><span class="level-item">资格迹与 λ-回报</span></span></a></li><li><a class="level is-mobile" href="#深度强化学习——价值与策略学习，SOTA-算法"><span class="level-left"><span class="level-item">2.16</span><span class="level-item">深度强化学习——价值与策略学习，SOTA 算法</span></span></a></li><li><a class="level is-mobile" href="#事后经验回放、模仿学习"><span class="level-left"><span class="level-item">2.17</span><span class="level-item">事后经验回放、模仿学习</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-06-03T07:57:14.000Z">2025-06-03</time></p><p class="title"><a href="/Memo/K%E7%BA%BF%E5%BD%A2%E6%80%81/">K线形态</a></p><p class="categories"><a href="/categories/Memo/">Memo</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-06-01T09:58:41.000Z">2025-06-01</time></p><p class="title"><a href="/Memo/%E9%87%8F%E5%8C%96%E6%97%A5%E8%AE%B0/">量化日记</a></p><p class="categories"><a href="/categories/Memo/">Memo</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-24T08:50:53.000Z">2025-05-24</time></p><p class="title"><a href="/AI/RL/RL-Endterm/">RL-Endterm</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/RL/">RL</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-24T04:19:25.000Z">2025-05-24</time></p><p class="title"><a href="/AI/RL/RL-%E8%B5%84%E6%A0%BC%E8%BF%B9/">RL-资格迹</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/RL/">RL</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-07T02:15:33.000Z">2025-05-07</time></p><p class="title"><a href="/Algorithm/DAA-Endterm/">DAA Endterm</a></p><p class="categories"><a href="/categories/Algorithm/">Algorithm</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/06/"><span class="level-start"><span class="level-item">June 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2025 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>