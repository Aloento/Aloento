<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>RL-马尔可夫决策过程 - Aloento</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f0f0f0"><meta name="application-name" content="Aloento"><meta name="msapplication-TileImage" content="/img/Aloento.png"><meta name="msapplication-TileColor" content="#f0f0f0"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Aloento"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="96x96" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="128x128" href="/img/Aloento.png"><link rel="apple-touch-icon" sizes="256x256" href="/img/Aloento.png"><meta name="description" content="Markov Decision Processes流感让我断更一周"><meta property="og:type" content="blog"><meta property="og:title" content="RL-马尔可夫决策过程"><meta property="og:url" content="https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><meta property="og:site_name" content="Aloento"><meta property="og:description" content="Markov Decision Processes流感让我断更一周"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://aloen.to/img/og_image.png"><meta property="article:published_time" content="2025-03-05T09:05:17.000Z"><meta property="article:modified_time" content="2025-05-12T19:40:07.940Z"><meta property="article:author" content="Aloento"><meta property="article:tag" content="笔记"><meta property="article:tag" content="AI"><meta property="article:tag" content="RL"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://aloen.to/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"},"headline":"RL-马尔可夫决策过程","image":["https://aloen.to/img/og_image.png"],"datePublished":"2025-03-05T09:05:17.000Z","dateModified":"2025-05-12T19:40:07.940Z","author":{"@type":"Person","name":"Aloento"},"publisher":{"@type":"Organization","name":"Aloento","logo":{"@type":"ImageObject","url":"https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"}},"description":"Markov Decision Processes流感让我断更一周"}</script><link rel="canonical" href="https://aloen.to/AI/RL/RL-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><link rel="icon" href="/img/Aloento.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Aloento</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://Q-Audio.org/Aloento">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-05T09:05:17.000Z" title="3/5/2025, 9:05:17 AM">2025-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-12T19:40:07.940Z" title="5/12/2025, 7:40:07 PM">2025-05-13</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/RL/">RL</a></span><span class="level-item">24 minutes read (About 3659 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">RL-马尔可夫决策过程</h1><div class="content"><p>Markov Decision Processes<br><del>流感让我断更一周</del></p>
<span id="more"></span>

<h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><p>马尔可夫决策过程有点像自动机。简而言之，它：在某个状态下，选择一个动作，获得某种奖励，进入下一个状态。</p>
<ul>
<li>状态（State）：$S$，环境状态，如棋盘上的位置，角色血量</li>
<li>动作（Action）：$A$，代理可选的动作，如走一步，攻击</li>
<li>奖励（Reward）：$R$，反馈，如吃到食物，受到伤害</li>
<li>转移概率（Transition Probability）：$P$，状态转移概率，如可能的下一步</li>
</ul>
<p>决策是一个不断重复的过程，每次做出选择都会影响未来的状态和奖励。其本质是代理（Agent）与环境（Environment）之间的交互：</p>
<ol>
<li>Agent 在状态 $S_t$ 选择动作 $A_t$</li>
<li>Environment 返回奖励 $R_{t+1}$ 和新状态 $S_{t+1}$</li>
<li>Agent 根据奖励和新状态更新策略</li>
<li>重复</li>
</ol>
<p>整个过程形成一个轨迹（Trajectory）$S_0, A_0, R_1, S_1, A_1, R_2, S_2, \cdots$，我们可以发现它的核心是“状态如何变化”，我们用</p>
<h1 id="转移概率"><a href="#转移概率" class="headerlink" title="转移概率"></a>转移概率</h1><p>（Transition Probability）来描述这个过程：</p>
<div>
$$
p(s', r | s, a) \doteq Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
$$
</div>

<p>其中：</p>
<ul>
<li>$s, s’$ 是状态</li>
<li>$a$ 是动作</li>
<li>$r$ 是奖励</li>
</ul>
<p>意思是在状态 $s$ 下，选择动作 $a$，进入状态 $s’$，获得奖励 $r$ 的概率</p>
<p>且 $\sum_{s’} \sum_r P(s’, r | s, a) &#x3D; 1$ 也就是说一定会转移到某个状态并获得某个奖励，不会出现转移到虚空，或者没有奖励的情况，且所有可能的状态和奖励的概率和为 1。</p>
<p>如果状态，动作，奖励是有限个数的，就叫 Finite MDP，我们能够精确计算出最优策略。</p>
<p>此公式还有其他形式：</p>
<h2 id="状态转移概率"><a href="#状态转移概率" class="headerlink" title="状态转移概率"></a>状态转移概率</h2><div>
$$
p(s' | s, a) \doteq Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_r p(s', r | s, a)
$$
</div>

<p>它表示在状态 $s$ 下，选择动作 $a$，进入状态 $s’$ 的概率。由于状态转移可能伴随不同奖励，所以需要对所有可能的奖励求和。</p>
<h2 id="期望即时奖励"><a href="#期望即时奖励" class="headerlink" title="期望即时奖励"></a>期望即时奖励</h2><p>$$<br>r(s, a) \doteq E[R_t | S_{t-1} &#x3D; s, A_{t-1} &#x3D; a] &#x3D; \sum_r r \sum_{s’} p(s’, r | s, a)<br>$$</p>
<p>它表示在状态 $s$ 下，选择动作 $a$，期望获得的奖励 $R_t$。每种情况都有不同的概率和奖励，所以需要对所有可能的状态和奖励求和，以计算平均期望奖励。</p>
<h2 id="期望奖励"><a href="#期望奖励" class="headerlink" title="期望奖励"></a>期望奖励</h2><p>$$<br>r(s, a, s’) \doteq E[R_t | S_{t-1} &#x3D; s, A_{t-1} &#x3D; a, S_t &#x3D; s’] &#x3D; \sum_r r \frac{p(s’, r | s, a)}{p(s’ | s, a)}<br>$$</p>
<p>这里是在已经知道从 $s$ 执行 $a$ 后，转移到 $s’$ 的情况下，计算期望奖励。通过对所有可能的奖励加权求和，归一化到特定的状态转移概率 $s \to s’$。</p>
<h2 id="确定与随机"><a href="#确定与随机" class="headerlink" title="确定与随机"></a>确定与随机</h2><p>如果一个系统是 Deterministic 的，那么在给定 $s$ 和 $a$ 后，$s’$ 是确定的，即 $p(s’ | s, a) &#x3D; 1$，每次执行相同的动作，结果都会一样，如棋类游戏。</p>
<p>如果是 Stochastic 的，那么 $p(s’ | s, a)$ 是一个概率分布，且 $\sum_{s’} p(s’ | s, a) &#x3D; 1$，表示执行后可能转移到不同状态，如掷骰子，环境有随机变化等。</p>
<p>任何代理无法改变的事物都是环境的一部分，边界代表着代理能够影响的事物的范围极限。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>假设有环境，你当前处于 C：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>10, A</th>
<th>x</th>
</tr>
</thead>
<tbody><tr>
<td>0, B</td>
<td>0, C</td>
<td>0, D</td>
</tr>
<tr>
<td>x</td>
<td>0, E</td>
<td>x</td>
</tr>
</tbody></table>
<ul>
<li>State Space: $S &#x3D; [A, B, C, D, E]$</li>
<li>Action Space: $A &#x3D; [Up, Down, Left, Right]$</li>
</ul>
<h3 id="确定性转移"><a href="#确定性转移" class="headerlink" title="确定性转移"></a>确定性转移</h3><p>代理在 C 选择 Up，进入 A 的概率是 1，奖励是 10，公式表示为</p>
<p>$$<br>p(A | C, Up) &#x3D; 1<br>$$</p>
<p>奖励表述为</p>
<p>$$<br>r(C, Up) &#x3D; 10 \times p(A, 10 | C, Up) &#x3D; 10<br>$$</p>
<h3 id="随机转移"><a href="#随机转移" class="headerlink" title="随机转移"></a>随机转移</h3><p>设代理有 0.8 的概率向上移动成功，0.1 的概率被环境带到其他位置。</p>
<p>$$<br>p(A | C, Up) &#x3D; 0.8, p(B | C, Up) &#x3D; 0.1, p(C | C, Up) &#x3D; 0.1<br>$$</p>
<p>奖励计算为：</p>
<p>$$<br>r(C, Up) &#x3D; 10 \times p(A, 10 | C, Up) + 0 \times p(B, 0 | C, Up) + 0 \times p(C, 0 | C, Up) &#x3D; 8<br>$$</p>
<h1 id="Exercise-Transition-Graph"><a href="#Exercise-Transition-Graph" class="headerlink" title="Exercise: Transition Graph"></a>Exercise: Transition Graph</h1><p>你正在玩一个回合制战斗游戏：</p>
<ul>
<li>你的初始 HP &#x3D; 2</li>
<li>敌人的初始 HP &#x3D; 1</li>
<li>你的动作有 Attack 和 Heal</li>
<li>攻击：有 0.2 的几率击杀敌人，获得 5 奖励，其余情况无事发生</li>
<li>治疗：有 0.6 的几率回复 1 HP（最大 HP 2），获得 1 奖励，其余情况无事发生</li>
<li>你执行动作后，敌人有 0.5 的几率攻击你，造成 1 伤害</li>
<li>如果你 HP &#x3D; 0，游戏结束，获得 -5 奖励</li>
<li>如果敌人 HP &#x3D; 0，游戏结束，获得 5 奖励</li>
</ul>
<p>我们能够列出所有的可能状态：</p>
<pre class="mermaid">flowchart TD
    B(["Strong<br>2/1"]) --> n1["H"] & n2["A"]
    n3(["Week<br>1/1"]) --> n4["H"] & n5["A"]
    n6(["Win<br>2/0 or 1/0"])
    n7(["Die 0/1"])</pre>

<p>你现在在 Strong，假设你选择了攻击，那么你有 0.2 的概率杀死敌人获得奖励。但还有 0.8 的概率无事发生，这就轮到敌人的回合，它有 0.5 的概率让你受到伤害，则：</p>
<pre class="mermaid">flowchart LR
    B(["Strong<br>2/1"]) --> n1["H"] & n2["A"]
    n3(["Week<br>1/1"]) --> n4["H"] & n5["A"]
    n2 -- "0.2 | 5" --> n6(["Win<br>2/0 or 1/0"])
    n2 -- "0.4 | 0" --> B & n3
    n7(["Die 0/1"])</pre>

<p>你现在在 Strong，假设你选择了治疗，但是你满血，所以无论回血是否发生，都无事发生，轮到敌人的回合，它有 0.5 的概率让你受到伤害，则：</p>
<pre class="mermaid">flowchart LR
    B(["Strong<br>2/1"]) --> n1["H"] & n2["A"]
    n3(["Week<br>1/1"]) --> n4["H"] & n5["A"]
    n2 -- "0.2 | 5" --> n6(["Win<br>2/0 or 1/0"])
    n2 -- "0.4 | 0" --> B & n3
    n1 -- "0.5 | 0" --> B & n3
    n7(["Die 0/1"])</pre>

<p>其他情况以此类推，最终得到：</p>
<pre class="mermaid">flowchart LR
    B(["Strong<br>2/1"]) --> n1["H"] & n2["A"]
    n3(["Week<br>1/1"]) --> n4["H"] & n5["A"]
    n2 -- "0.2 | 5" --> n6(["Win<br>2/0 or 1/0"])
    n2 -- "0.4 | 0" --> B & n3
    n1 -- "0.5 | 0" --> B & n3
    n5 -- "0.2 | 5" --> n6
    n5 -- "0.4 | 0" --> n3
    n5 -- "0.4 | -5" --> n7(["Die 0/1"])
    n4 -- "0.3 | 1" --> B
    n4 -- "0.2 | 0" --> n3
    n4 -- "0.2 | -5" --> n7
    n4 -- "0.3 | 0" --> n3</pre>

<p>在 Week 时治疗，有 0.6 概率成功，随后有 0.5 的概率再被攻击，所以是 0.3 的概率回到 Strong。</p>
<h1 id="目标与奖励"><a href="#目标与奖励" class="headerlink" title="目标与奖励"></a>目标与奖励</h1><p>代理的目标是最大化累积奖励的期望值。</p>
<p>$$<br>G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T &#x3D; \sum_{k&#x3D;0}^{\infty} \gamma^k R_{t+k+1}<br>$$</p>
<p>其中：</p>
<ul>
<li>$G_t$ 是从 $t$ 时刻开始的累积奖励</li>
<li>$R_t$ 是 $t$ 时刻的奖励</li>
</ul>
<p>任务类型</p>
<ul>
<li>Episodic Task：上面的 t 是有限的，任务有明确的终止状态，如游戏</li>
<li>Continuing Task：t 是无限的，任务没有明确的终止状态，如生存，股票</li>
</ul>
<p>折扣回报（Discounted Return）：</p>
<p>由于在持续任务中，回报可能是无限的，难以计算，且在现实世界中，未来的奖励通常不如当前奖励重要，因此要给未来奖励打折，所以引入折扣因子 $0 \leq \gamma \leq 1$，来减少未来奖励的影响。</p>
<ul>
<li>$\gamma \rightarrow 0$：关注短期的奖励</li>
<li>$\gamma \rightarrow 1$：关注长期的奖励</li>
</ul>
<p>由于折扣回报涉及多个时间步，所以可以使用递归形式：</p>
<p>$$<br>G_t \doteq R_{t+1} + \gamma G_{t+1}<br>$$</p>
<p>表明当前的回报由 当前奖励 和 未来回报 组成。我们约定 $R_{T+1}$ 代表在 $t$ 时采取动作的奖励。</p>
<p>当 $R_t &#x3D; 1$ 且 $\gamma &lt; 1$ 时，有：</p>
<p>$$<br>G_t &#x3D; \frac{1}{1 - \gamma}<br>$$</p>
<h1 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h1><p>Markov Property 就一句话，下一个状态只依赖于当前状态，与之前的状态无关。</p>
<p>练习：给定 $\gamma &#x3D; 0.5$，奖励序列如下：</p>
<ul>
<li>$R_1 &#x3D; -1$</li>
<li>$R_2 &#x3D; 2$</li>
<li>$R_3 &#x3D; 6$</li>
<li>$R_4 &#x3D; 3$</li>
<li>$R_5 &#x3D; 2$</li>
</ul>
<p>目标是计算从每个时间步 $t$ 开始的折扣回报 $G_t$。</p>
<p>解：</p>
<p>在 MDP 中，终止状态的回报通常为 0，因为不会再有新的状态了，所以有</p>
<p>$$<br>G_5 &#x3D; 0<br>$$</p>
<p>我们从最后一步开始，使用折扣回报公式，逐步向前计算：</p>
<p>$$<br>G_t &#x3D; R_{t+1} + \gamma G_{t+1}<br>$$</p>
<table>
<thead>
<tr>
<th>t</th>
<th>$R_t$</th>
<th>$G_t$</th>
</tr>
</thead>
<tbody><tr>
<td>5</td>
<td>-</td>
<td>$G_5 &#x3D; 0$</td>
</tr>
<tr>
<td>4</td>
<td>$R_5 &#x3D; 2$</td>
<td>$G_4 &#x3D; 2 + 0.5 \times 0 &#x3D; 2$</td>
</tr>
<tr>
<td>3</td>
<td>$R_4 &#x3D; 3$</td>
<td>$G_3 &#x3D; 3 + 0.5 \times 2 &#x3D; 4$</td>
</tr>
<tr>
<td>2</td>
<td>$R_3 &#x3D; 6$</td>
<td>$G_2 &#x3D; 6 + 0.5 \times 4 &#x3D; 8$</td>
</tr>
<tr>
<td>1</td>
<td>$R_2 &#x3D; 2$</td>
<td>$G_1 &#x3D; 2 + 0.5 \times 8 &#x3D; 6$</td>
</tr>
<tr>
<td>0</td>
<td>$R_1 &#x3D; -1$</td>
<td>$G_0 &#x3D; -1 + 0.5 \times 6 &#x3D; 2$</td>
</tr>
</tbody></table>
<h1 id="吸收状态"><a href="#吸收状态" class="headerlink" title="吸收状态"></a>吸收状态</h1><p>有限与无限任务需要一个共同的表示方式，所以我们为 Episodic Task 引入了吸收状态（Absorbing State）。当任务到达 $T$ 时，不再设为终止状态，而是吸收状态，它不会再产生奖励。我们可以重新定义折扣回报：</p>
<p>$$<br>G_t \doteq \sum_{k&#x3D;t+1}^{T} \gamma^{k-t-1} R_k<br>$$</p>
<ul>
<li>$G_t$ 是从 $t$ 时刻开始的累积奖励</li>
<li>$R_k$ 是 $k$ 时刻的奖励</li>
<li>$T$ 是任务结束的时间步，也可以是无限的</li>
<li>$T &#x3D; \infty$ 与 $\gamma &#x3D; 1$ 不能同时存在，否则回报无限</li>
</ul>
<h1 id="策略与价值函数"><a href="#策略与价值函数" class="headerlink" title="策略与价值函数"></a>策略与价值函数</h1><p>代理用于在环境中做出决定的规则成为策略（Policy）$\pi$，将状态映射到动作：</p>
<p>$$<br>\pi(a|s) &#x3D; P(A_t &#x3D; a | S_t &#x3D; s)<br>$$</p>
<p>这表示代理在状态 $s$ 下选择动作 $a$ 的概率。策略可以是确定性（Deterministic）的，即 $\pi(s) &#x3D; a$，在某个状态下始终采取同一个动作。也可以是随机性（Stochastic）的，即上述公式。在 RL 中，我们的目标是找到最优策略 $\pi^*$，使得在任何状态下，都能获得最大的累积奖励。</p>
<p>Value Function 用于衡量 State-Action Pair 的好坏，即代理在该状态下应期望得到多少奖励。</p>
<h2 id="Action-value-函数"><a href="#Action-value-函数" class="headerlink" title="Action-value 函数"></a>Action-value 函数</h2><p>动作价值函数表示在策略 $\pi$ 下，从状态 $s$ 开始，选择动作 $a$ 后，未来能够获得的期望折扣回报：</p>
<p>$$<br>q_{\pi}(s, a) \doteq E_{\pi}[G_t | S_t &#x3D; s, A_t &#x3D; a] &#x3D; E_{\pi}[\sum_{k&#x3D;0}^{\infty} \gamma^k R_{t+k+1} | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>
<p>这个函数对于动作选择很重要，它直接告诉我们某个动作的价值。人类难以直观估计 Q 值，因为其涉及长期回报计算。</p>
<p>我们可以用递归（Bellman）的方式表示：</p>
<p>$$<br>q_{\pi}(s, a) &#x3D; E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>
<h2 id="State-value-函数"><a href="#State-value-函数" class="headerlink" title="State-value 函数"></a>State-value 函数</h2><p>状态价值函数表示在策略 $\pi$ 下，从状态 $s$ 开始，未来能够获得的期望折扣回报：</p>
<p>$$<br>v_{\pi}(s) \doteq E_{\pi}[G_t | S_t &#x3D; s] &#x3D; E_{\pi}[\sum_{k&#x3D;0}^{\infty} \gamma^k R_{t+k+1} | S_t &#x3D; s]<br>$$</p>
<blockquote>
<p>折扣是折扣因子导致的</p>
</blockquote>
<p>我们可以用递归（Bellman）的方式表示：</p>
<p>$$<br>v_{\pi}(s) &#x3D; E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t &#x3D; s]<br>$$</p>
<p>$V_{\pi}(s)$ 是在该状态下所有可能的动作 $Q(s, a)$ 的加权和：</p>
<p>$$<br>v_{\pi}(s) &#x3D; \sum_{a} \pi(a|s) q_{\pi}(s, a)<br>$$</p>
<p>这意味着某个状态的价值，是在该状态下按策略选择不同动作的期望值。它衡量的是在该状态下，按照策略平均能得到多少奖励。</p>
<h2 id="例：值迭代"><a href="#例：值迭代" class="headerlink" title="例：值迭代"></a>例：值迭代</h2><table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
</tr>
<tr>
<td>8</td>
<td>9</td>
<td>10</td>
<td>11</td>
</tr>
<tr>
<td>12</td>
<td>13</td>
<td>14</td>
<td>x</td>
</tr>
</tbody></table>
<ul>
<li>Action Space: $A &#x3D; [Up, Down, Left, Right]$</li>
<li>策略：每个动作选择概率相等 $\pi(a|s) &#x3D; 0.25$</li>
<li>每走一步都会有 $R &#x3D; -1$ 的奖励，终点设置为 0。</li>
<li>环境为确定性环境，即 $p(s’ | s, a) &#x3D; 1$</li>
<li>$\gamma &#x3D; 1$</li>
</ul>
<p>请计算每个状态的价值函数 $V(s)$，进行策略评估。</p>
<p>解：</p>
<p>值迭代使用贝尔曼方程来迭代：</p>
<p>$$<br>v_{\pi}(s) &#x3D; \sum_{a} \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_{\pi}(s’)]<br>$$</p>
<p>$k &#x3D; 0, V_0(s) &#x3D; 0$</p>
<table>
<thead>
<tr>
<th>0</th>
<th>0</th>
<th>0</th>
<th>0</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<h3 id="k-1"><a href="#k-1" class="headerlink" title="$k &#x3D; 1$"></a>$k &#x3D; 1$</h3><p>用 $s &#x3D; 1$ 为例，分析概率：</p>
<ul>
<li>Up: $p(1, -1 | 1, Up) &#x3D; 1$</li>
<li>Down: $p(5, -1 | 1, Down) &#x3D; 1$</li>
<li>Left: $p(0, -1 | 1, Left) &#x3D; 1$</li>
<li>Right: $p(2, -1 | 1, Right) &#x3D; 1$</li>
<li>其余情况为 0，如 $p(6, -1 | 1, Down) &#x3D; 0$ 因为不可能直接从 1 走到 6</li>
</ul>
<p>所以</p>
<p>$v_{\pi}(1) &#x3D; \sum_{4} 0.25 \sum_{s’, r} p(s’, r | 1, a) [r + \gamma v_{\pi}(s’)] &#x3D;$<br>$0.25 \times 1 \times [-1 + 1 \times 0] +$<br>$0.25 \times 1 \times [-1 + 1 \times 0] +$<br>$0.25 \times 1 \times [-1 + 1 \times 0] +$<br>$0.25 \times 1 \times [-1 + 1 \times 0] &#x3D;$<br>$4 \times 0.25 \times -1 &#x3D; -1$</p>
<p>其他点也是同样的情况，有</p>
<table>
<thead>
<tr>
<th>0</th>
<th>-1</th>
<th>-1</th>
<th>-1</th>
</tr>
</thead>
<tbody><tr>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>0</td>
</tr>
</tbody></table>
<h3 id="k-2"><a href="#k-2" class="headerlink" title="$k &#x3D; 2$"></a>$k &#x3D; 2$</h3><p>用 $s &#x3D; 1$ 为例：</p>
<p>$v_{\pi}(1) &#x3D; \sum_{4} 0.25 \sum_{s’, r} p(s’, r | 1, a) [r + \gamma v_{\pi}(s’)] &#x3D;$<br>$0.25 \times 1 \times [-1 + 1 \times 0] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] &#x3D;$<br>$3 \times 0.25 \times -2 + 1 \times 0.25 \times -1 &#x3D; -1.75$</p>
<p>其他点也是同样的算法：</p>
<p>$v_{\pi}(6) &#x3D; \sum_{4} 0.25 \sum_{s’, r} p(s’, r | 6, a) [r + \gamma v_{\pi}(s’)] &#x3D;$<br>$0.25 \times 1 \times [-1 + 1 \times -1] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] +$<br>$0.25 \times 1 \times [-1 + 1 \times -1] &#x3D;$<br>$4 \times 0.25 \times -2 &#x3D; -2$</p>
<p>得到</p>
<table>
<thead>
<tr>
<th>0</th>
<th>-1.75</th>
<th>-2</th>
<th>-2</th>
</tr>
</thead>
<tbody><tr>
<td>-1.75</td>
<td>-2</td>
<td>-2</td>
<td>-2</td>
</tr>
<tr>
<td>-2</td>
<td>-2</td>
<td>-2</td>
<td>-1.75</td>
</tr>
<tr>
<td>-2</td>
<td>-2</td>
<td>-1.75</td>
<td>0</td>
</tr>
</tbody></table>
<p>然后可以无限向后迭代，直到达到某个收敛值。</p>
<h2 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h2><p>最优策略是能够最大化长期期望回报的策略。在 MDP 中，至少存在一个最优策略。使用 $v_*(s)$ 和 $q_*(s, a)$ 表示最优状态价值函数和最优动作价值函数（其实也就是那俩方程求 max）。</p>
<p>但这基本上是理论上可行，实践困难。在低维度，状态少的时候可以用动态规划或者值迭代，但一旦高维数据会导致计算量爆炸。Curse of Dimensionality 维度灾难就是说明状态空间维度高时，数据稀疏，计算困难的，这时候我们就需要 Function Approximation 函数逼近，如神经网络等。</p>
<h1 id="备份图"><a href="#备份图" class="headerlink" title="备份图"></a>备份图</h1><p>Backup Diagram 用于可视化值更新过程，每个空心圆代表一个状态，而实心圆代表一个 State-Action Pair。</p>
<p>考虑如下 MDP：</p>
<ul>
<li>State Space: 只有一个决策状态</li>
<li>Action Space: Left, Right</li>
<li>Reward: $R_{Left} &#x3D; 1, R_{Right} &#x3D; 2$</li>
<li>$\gamma &#x3D; 0 &#x2F; 0.9 &#x2F; 0.5$</li>
</ul>
<pre class="mermaid">flowchart TD
    n1["Small Circle"] -- left --> n2["Filled Circle"]
    n1 -- right --> n3["Filled Circle"]
    n3 -- 0 --> n4["Small Circle"]
    n2 -- +1 --> n5["Small Circle"]
    n5 --> n6["Filled Circle"]
    n4 --> n7["Filled Circle"]
    n6 -- 0 --> n1
    n7 -- +2 --> n1

    n1@{ shape: sm-circ}
    n2@{ shape: f-circ}
    n3@{ shape: f-circ}
    n4@{ shape: sm-circ}
    n5@{ shape: sm-circ}
    n6@{ shape: f-circ}
    n7@{ shape: f-circ}</pre>

<p>求不同 $\gamma$ 下的最优策略。</p>
<p>解：</p>
<p>首先拿出函数：</p>
<p>$$<br>v_{\pi}(s) &#x3D; E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t &#x3D; s]<br>$$</p>
<p>$\gamma &#x3D; 0$ 时，只考虑当前奖励，所以</p>
<p>$$<br>v_{\pi &#x3D; Left}(s) &#x3D; E_{\pi}[1 + 0 \times G_{t+1}] &#x3D; 1<br>$$</p>
<p>$$<br>v_{\pi &#x3D; Right}(s) &#x3D; E_{\pi}[0 + 0 \times G_{t+1}] &#x3D; 0<br>$$</p>
<p>完全不考虑未来回报，选择 left 直接获得 1，所以最优策略是 left。</p>
<p>$\gamma &#x3D; 0.5$ 时，考虑部分未来回报，所以</p>
<p>$$<br>v_{\pi &#x3D; Left}(s) &#x3D; E_{\pi}[1 + 0.5 \times 0] &#x3D; 1<br>$$</p>
<p>$$<br>v_{\pi &#x3D; Right}(s) &#x3D; E_{\pi}[0 + 0.5 \times 2] &#x3D; 1<br>$$</p>
<p>选择 left 仍然是 1，后续奖励为 0 所以折扣对其无影响。而选择 right 则后续奖励以 0.5 折扣。结果相同，此时策略无偏好。</p>
<p>$\gamma &#x3D; 0.9$ 时，考虑更多未来回报，所以</p>
<p>$$<br>v_{\pi &#x3D; Left}(s) &#x3D; E_{\pi}[1 + 0.9 \times 0] &#x3D; 1<br>$$</p>
<p>$$<br>v_{\pi &#x3D; Right}(s) &#x3D; E_{\pi}[0 + 0.9 \times 2] &#x3D; 1.8<br>$$</p>
<p>选择 left 仍然是 1，选择 right 后续奖励以 0.9 折扣。right 为最优策略。</p>
<blockquote>
<p>由于右侧状态在下一步后无额外奖励，所以第一轮计算就已经能决定最终值函数。而计算未来回报 $G_{t+1}$ 时，需要递归 $v_{\pi}(s)$，但由于终止状态的奖励为 0，未来回报不影响计算，所以第一轮计算就已经足够。</p>
</blockquote>
<script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';	mermaid.initialize({startOnLoad: true, flowchart: {curve: 'linear'}}); </script></div><div class="article-licensing box"><div class="licensing-title"><p>RL-马尔可夫决策过程</p><p><a href="https://aloen.to/AI/RL/RL-马尔可夫决策过程/">https://aloen.to/AI/RL/RL-马尔可夫决策过程/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Aloento</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-03-05</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-05-13</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/RL/">RL</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI/RL/RL-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">RL-动态规划</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/AI/RL/RL-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/"><span class="level-item">RL-多臂老虎机</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/Aloento.png" alt="Aloento"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Aloento</p><p class="is-size-6 is-block">Reindeer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Foot of Sacred Mountain</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">67</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">36</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/CogSci/"><span class="level-start"><span class="level-item">CogSci</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/RL/"><span class="level-start"><span class="level-item">RL</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/TM/"><span class="level-start"><span class="level-item">TM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Cloud/"><span class="level-start"><span class="level-item">Cloud</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Cloud/OpenStack/"><span class="level-start"><span class="level-item">OpenStack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Science/"><span class="level-start"><span class="level-item">Data Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/MSSQL/"><span class="level-start"><span class="level-item">MSSQL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Theory/"><span class="level-start"><span class="level-item">Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math/Logic/"><span class="level-start"><span class="level-item">Logic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/Matlab/"><span class="level-start"><span class="level-item">Matlab</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Memo/"><span class="level-start"><span class="level-item">Memo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/"><span class="level-start"><span class="level-item">Program</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Program/C/CLI/"><span class="level-start"><span class="level-item">CLI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Program/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Program/WebCodecs/"><span class="level-start"><span class="level-item">WebCodecs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">37</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JS/"><span class="tag">JS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LINQ/"><span class="tag">LINQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenStack/"><span class="tag">OpenStack</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQLServer/"><span class="tag">SQLServer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebCodecs/"><span class="tag">WebCodecs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A0%E9%A2%98/"><span class="tag">习题</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%91/"><span class="tag">云</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%88%E7%89%99%E5%88%A9/"><span class="tag">匈牙利</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E7%81%B5%E6%9C%BA/"><span class="tag">图灵机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BB%E7%95%A5/"><span class="tag">攻略</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%80%BC%E6%96%B9%E6%B3%95/"><span class="tag">数值方法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"><span class="tag">数据科学</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%95%99%E5%AD%A6/"><span class="tag">留学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">53</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%BB%E8%AF%91/"><span class="tag">翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E8%AF%95/"><span class="tag">考试</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6/"><span class="tag">认知科学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%BB%E8%BE%91/"><span class="tag">逻辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"><span class="tag">音视频</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://Q-Audio.org" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Q-Audio</span></span><span class="level-right"><span class="level-item tag">q-audio.org</span></span></a></li><li><a class="level is-mobile" href="https://Musi.Land" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MusiLand</span></span><span class="level-right"><span class="level-item tag">musi.land</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#基础概念"><span class="level-left"><span class="level-item">1</span><span class="level-item">基础概念</span></span></a></li><li><a class="level is-mobile" href="#转移概率"><span class="level-left"><span class="level-item">2</span><span class="level-item">转移概率</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#状态转移概率"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">状态转移概率</span></span></a></li><li><a class="level is-mobile" href="#期望即时奖励"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">期望即时奖励</span></span></a></li><li><a class="level is-mobile" href="#期望奖励"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">期望奖励</span></span></a></li><li><a class="level is-mobile" href="#确定与随机"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">确定与随机</span></span></a></li><li><a class="level is-mobile" href="#例子"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">例子</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#确定性转移"><span class="level-left"><span class="level-item">2.5.1</span><span class="level-item">确定性转移</span></span></a></li><li><a class="level is-mobile" href="#随机转移"><span class="level-left"><span class="level-item">2.5.2</span><span class="level-item">随机转移</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Exercise-Transition-Graph"><span class="level-left"><span class="level-item">3</span><span class="level-item">Exercise: Transition Graph</span></span></a></li><li><a class="level is-mobile" href="#目标与奖励"><span class="level-left"><span class="level-item">4</span><span class="level-item">目标与奖励</span></span></a></li><li><a class="level is-mobile" href="#马尔可夫性质"><span class="level-left"><span class="level-item">5</span><span class="level-item">马尔可夫性质</span></span></a></li><li><a class="level is-mobile" href="#吸收状态"><span class="level-left"><span class="level-item">6</span><span class="level-item">吸收状态</span></span></a></li><li><a class="level is-mobile" href="#策略与价值函数"><span class="level-left"><span class="level-item">7</span><span class="level-item">策略与价值函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Action-value-函数"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Action-value 函数</span></span></a></li><li><a class="level is-mobile" href="#State-value-函数"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">State-value 函数</span></span></a></li><li><a class="level is-mobile" href="#例：值迭代"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">例：值迭代</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#k-1"><span class="level-left"><span class="level-item">7.3.1</span><span class="level-item">$k = 1$</span></span></a></li><li><a class="level is-mobile" href="#k-2"><span class="level-left"><span class="level-item">7.3.2</span><span class="level-item">$k = 2$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#最优策略"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">最优策略</span></span></a></li></ul></li><li><a class="level is-mobile" href="#备份图"><span class="level-left"><span class="level-item">8</span><span class="level-item">备份图</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-07T02:15:33.000Z">2025-05-07</time></p><p class="title"><a href="/Algorithm/DAA-Endterm/">DAA Endterm</a></p><p class="categories"><a href="/categories/Algorithm/">Algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-05T09:19:25.000Z">2025-05-05</time></p><p class="title"><a href="/Data-Science/ITDS-%E5%88%86%E7%B1%BB/">ITDS-分类</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-05T06:13:32.000Z">2025-05-05</time></p><p class="title"><a href="/Data-Science/ITDS-Final/">ITDS Final</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-21T04:22:15.000Z">2025-04-21</time></p><p class="title"><a href="/AI/CogSci/%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6/">认知科学</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/CogSci/">CogSci</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-29T16:00:00.000Z">2025-03-30</time></p><p class="title"><a href="/Data-Science/ITDS-Midterm/">ITDS Midterm</a></p><p class="categories"><a href="/categories/Data-Science/">Data Science</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2025 Aloento</span><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Q-Audio" href="https://Q-Audio.org"><i class="fas fa-compact-disc"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="MusiLand" href="https://Musi.Land/"><i class="fab fa-dashcube"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Aloento"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>